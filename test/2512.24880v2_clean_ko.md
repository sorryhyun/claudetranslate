

**2.2. 거시적 설계**

거시적 설계(Macro-design)는 네트워크의 전역 토폴로지를 관장한다 (Srivastava et al., 2015). ResNet (He et al., 2016a)을 따라, DenseNet (Huang et al., 2017) 및 FractalNet (Larsson et al., 2016)과 같은 아키텍처들은 각각 조밀한 연결성과 다중 경로 구조를 통해 토폴로지 복잡도를 증가시킴으로써 성능 향상을 목표로 하였다. Deep Layer Aggregation (DLA) (Yu et al., 2018)은 다양한 깊이와 해상도에 걸쳐 특징을 재귀적으로 집계함으로써 이 패러다임을 더욱 확장하였다.

보다 최근에는 거시적 설계의 초점이 잔차 스트림(residual stream)의 폭을 확장하는 방향으로 이동하였다 (Chai et al., 2020; Fang et al., 2023; Heddes et al., 2025; Mak and Flanigan, 2025; Menghani et al., 2025; Pagliardini et al., 2024; Xiao et al., 2025; Xie et al., 2023; Zhu et al., 2024). 하이퍼 연결(Hyper-Connections, HC) (Zhu et al., 2024)은 다양한 깊이의 특징들 간의 연결 강도를 조절하기 위해 학습 가능한 행렬을 도입하였으며, Residual Matrix Transformer (RMT) (Mak and Flanigan, 2025)는 특징 저장을 용이하게 하기 위해 표준 잔차 스트림을 외적 메모리 행렬로 대체하였다. 유사하게, MUDDFormer (Xiao et al., 2025)는 계층 간 정보 흐름을 최적화하기 위해 다방향 동적 밀집 연결을 사용한다. 이러한 접근법들의 잠재력에도 불구하고, 이들은 잔차 연결의 고유한 항등 사상(identity mapping) 특성을 훼손하여 불안정성을 야기하고 확장성을 저해한다. 더욱이, 이들은 확장된 특징 폭으로 인해 상당한 메모리 접근 오버헤드를 발생시킨다. HC를 기반으로 구축된 제안된 _m_HC는 잔차 연결 공간을 특정 매니폴드 위로 제한하여 항등 사상 특성을 복원하는 동시에, 효율성을 보장하기 위한 엄격한 인프라 최적화를 통합한다. 이 접근법은 확장된 연결의 토폴로지적 이점을 유지하면서 안정성과 확장성을 향상시킨다.

#### **3. 예비 지식**

먼저 본 연구에서 사용되는 표기법을 확립한다. HC 공식화에서, _𝑙_-번째 층의 입력 **x**_𝑙_ ∈ **R**[1][×]_[𝐶]_는 _𝑛_배로 확장되어 은닉 행렬 **x**_𝑙_ = (**x**[⊤]_𝑙_,0[,] _[. . .]_ [,] **[x]**[⊤]_𝑙_, _𝑛_−1[)][⊤] [∈] **[R]**_[𝑛]_ [×] _[𝐶]_를 구성한다.

이는 _𝑛_-스트림 잔차로 볼 수 있다. 이 연산은 잔차 스트림의 폭을 효과적으로 확장한다. 이 스트림의 읽기, 쓰기 및 업데이트 과정을 관장하기 위해, HC는 세 개의 학습 가능한 선형 사상—H_𝑙_[pre], H_𝑙_[post] ∈ **R**[1][×]_[𝑛]_, 그리고 H_𝑙_[res] ∈ **R**_[𝑛]_ [×] _[𝑛]_—을 도입한다. 이러한 사상들은 식 (1)에 표시된 표준 잔차 연결을 수정하여 식 (3)에 주어진 공식을 생성한다.

HC 공식화에서, 학습 가능한 사상들은 두 부분의 계수로 구성된다: 입력 의존적 계수와 전역 계수이며, 이들은 각각 동적 사상(dynamic mappings)과 정적 사상(static mappings)으로 지칭된다. 형식적으로, HC는 계수를 다음과 같이 계산한다:




**x**˜_𝑙_ = RMSNorm(**x**_𝑙_)
H_𝑙_[pre] = _𝛼_[pre]_𝑙_ - tanh(_𝜃𝑙_[pre] **x**˜[⊤]_𝑙_ [) +] **[b]**[pre]_𝑙_
H_𝑙_[post] = _𝛼_[post]_𝑙_ - tanh(_𝜃𝑙_[post] **x**˜[⊤]_𝑙_ [) +] **[b]**[post]_𝑙_
H_𝑙_[res] = _𝛼_[res]_𝑙_ - tanh(_𝜃𝑙_[res] **x**˜[⊤]_𝑙_ [) +] **[b]**[res]_𝑙_[,]

(5)

여기서 RMSNorm(·) (Zhang and Sennrich, 2019)은 마지막 차원에 적용되며, 스칼라

_𝛼_[pre]_𝑙_, _𝛼_[post]_𝑙_ 그리고 _𝛼_[res]_𝑙_ ∈ **R**는 작은 값으로 초기화되는 학습 가능한 게이팅 인자이다. 동적

5

사상들은 _𝜃𝑙_[pre], _𝜃𝑙_[post] ∈ **R**[1][×]_[𝐶]_ 및 _𝜃𝑙_[res] ∈ **R**_[𝑛]_ [×] _[𝐶]_로 매개변수화된 선형 투영을 통해 유도되며, 정적 사상들은 학습 가능한 편향 **b**[pre]_𝑙_, **b**[post]_𝑙_ ∈ **R**[1][×]_[𝑛]_ 및 **b**[res]_𝑙_ ∈ **R**_[𝑛]_ [×] _[𝑛]_으로 표현된다.

이러한 사상들—H_𝑙_[pre], H_𝑙_[post], 그리고 H_𝑙_[res]—의 도입이 무시할 만한 계산 오버헤드만을 발생시킨다는 점은 주목할 가치가 있다. 전형적인 확장 비율 _𝑛_, 예를 들어 4는, 입력 차원 _𝐶_보다 훨씬 작기 때문이다. 이러한 설계로 인해, HC는 잔차 스트림의 정보 용량을 층의 입력 차원으로부터 효과적으로 분리하는데, 이는 모델의 계산 복잡도(FLOPs)와 강하게 상관되어 있다. 결과적으로, HC는 사전 학습 스케일링 법칙(Hoffmann et al., 2022)에서 논의된 모델 FLOPs 및 학습 데이터 크기의 전통적인 스케일링 차원을 보완하며, 잔차 스트림 폭을 조정함으로써 스케일링을 위한 새로운 방향을 제공한다.

HC는 잔차 스트림과 층 입력 간의 차원 불일치를 관리하기 위해 세 가지 사상을 필요로 하지만, 표 1에 제시된 예비 실험은 잔차 사상 H_𝑙_[res]가 가장 중요한 성능 향상을 제공한다는 것을 나타낸다. 이 발견은 잔차 스트림 내의 효과적인 정보 교환의 중요성을 강조한다.

표 1 | **HC 구성요소의 절제 연구.** 특정 사상(H_𝑙_[pre], H_𝑙_[post], 또는 H_𝑙_[res])이 비활성화될 때, 차원 일관성을 유지하기 위해 고정된 사상을 사용한다: H_𝑙_[pre]에는 1/_𝑛_의 균일 가중치, H_𝑙_[post]에는 1의 균일 가중치, H_𝑙_[res]에는 항등 행렬.

H_𝑙_[res] H_𝑙_[pre] H_𝑙_[post] 절대 손실 차이

0.0
✓         - 0.022
✓ ✓         - 0.025
✓ ✓ ✓         - 0.027

**3.1. 수치적 불안정성**

잔차 사상 H_𝑙_[res]는 성능에 중요하지만, 그것의 순차적 적용은 수치적 안정성에 심각한 위험을 초래한다. 식 (4)에 상세히 설명된 바와 같이, HC가 여러 층에 걸쳐 확장될 때, 층 _𝑙_에서 _𝐿_까지의 효과적인 신호 전파는 합성 사상 [∏]_𝑖_ _[𝐿]_ = [−] 1_[𝑙]_ [H]_𝐿_[res] - _𝑖_에 의해 관장된다. 학습 가능한 사상 H_𝑙_[res]가 제약되지 않기 때문에, 이 합성 사상은 필연적으로 항등 사상에서 벗어난다. 결과적으로, 신호 크기는 순전파와 역전파 모두에서 폭발 또는 소실되기 쉽다. 이 현상은 방해받지 않는 신호 흐름에 의존하는 잔차 학습의 기본 전제를 훼손하여, 더 깊거나 대규모 모델에서 학습 과정을 불안정하게 만든다.

실증적 증거가 이 분석을 뒷받침한다. 우리는 그림 2에 도시된 바와 같이 대규모 실험에서 불안정한 손실 행동을 관찰한다. _m_HC를 기준선으로 하여, HC는 약 12,000 스텝 부근에서 예상치 못한 손실 급등을 보이는데, 이는 기울기 노름의 불안정성과 높은 상관관계가 있다. 더욱이, H_𝑙_[res]에 대한 분석은 이 불안정성의 메커니즘을 검증한다. 합성 사상 [∏]_𝑖_ _[𝐿]_ = [−] 1_[𝑙]_ [H]_𝐿_[res] - _𝑖_가 잔차 스트림을 따라 신호를 증폭하는 방식을 정량화하기 위해, 우리는 두 가지 메트릭을 활용한다. 첫 번째는 합성 사상의 행 합의 최대 절댓값을 기반으로 하여 순전파에서의 최악의 경우 확장을 포착한다. 두 번째는 최대 절대 열 합을 기반으로 하며, 이는 역전파에 해당한다. 우리는 이러한 메트릭을 합성 사상의 _최대 이득 크기(Amax Gain Magnitude)_로 지칭한다. 그림 3 (b)에 도시된 바와 같이, 최대 이득 크기는 3000의 피크를 보이는 극단적인 값을 산출하는데, 이는 1로부터의 현저한 이탈로서 잔차 스트림 폭발의 존재를 확인한다.

6

그림 2 | **하이퍼 연결(HC)의 학습 불안정성.** 이 그림은 (a) _m_HC에 대한 HC의 절대 손실 차이, 그리고 (b) 기울기 노름의 비교를 보여준다. 모든 결과는 27B 모델을 기반으로 한다.

|Hr es Forward Signal Gain<br>l<br>Hr es Backward Gradient<br>l|Gain|
|---|---|
|||
|||
|0<br>10<br>20<br><br>Laye|0<br>10<br>20<br><br>Laye|

|5|Col2|Col3|
|---|---|---|
||Y<br>l<br>i = 1H<br>Y<br>~~61 −l~~<br>i = 1|res<br>l + 1 −i Forward Signal Gain<br>~~res~~<br>61 i~~ Backward Gradient Gain~~|
||Y<br>l<br>i = 1H<br>Y<br>~~61 −l~~<br>i = 1|res<br>l + 1 −i Forward Signal Gain<br>~~res~~<br>61 i~~ Backward Gradient Gain~~|
||<br>|−|
||||
||||
||||
|0<br>10<br>20<br>30<br>Layer In|0<br>10<br>20<br>30<br>Layer In|40<br>50<br>60<br> dexl|

그림 3 | **하이퍼 연결(HC)의 전파 불안정성.** 이 그림은 (a) 단일 층 사상 H_𝑙_[res]과 (b) 합성 사상

- _𝐿_ - _𝑙_
_𝑖_ =1 [H]_𝐿_[res]   - _𝑖_의 전파 동역학을 27B 모델 내에서 보여준다. 층 인덱스 _𝑙_(x-축)은 각 표준 트랜스포머 블록을 두 개의 독립적인 층(어텐션과 FFN)으로 전개한다. 최대 이득 크기(y-축)는 최대 절대 행 합(순전파 신호의 경우)과 열 합(역전파 기울기의 경우)으로 계산되며, 선택된 시퀀스의 모든 토큰에 걸쳐 평균화된다.

**3.2. 시스템 오버헤드**

HC의 계산 복잡도는 추가 사상들의 선형성으로 인해 관리 가능하지만, 시스템 수준의 오버헤드는 무시할 수 없는 문제를 제기한다. 특히, 메모리 접근(I/O) 비용은 흔히 현대 모델 아키텍처에서 주요 병목 중 하나를 구성하며, 이는 널리 "메모리 월(memory wall)"로 지칭된다 (Dao et al., 2022). 이 병목은 아키텍처 설계에서 자주 간과되지만, 런타임 효율성에 결정적인 영향을 미친다.

널리 채택된 사전 정규화 트랜스포머(pre-norm Transformer) (Vaswani et al., 2017) 아키텍처에 초점을 맞춰, 우리는 HC에 내재된 I/O 패턴을 분석한다. 표 2는 _𝑛_-스트림 잔차 설계에 의해 도입된 단일 잔차 층에서의 토큰 당 메모리 접근 오버헤드를 요약한다. 분석 결과, HC는 메모리 접근 비용을 대략 _𝑛_에 비례하는 배수만큼 증가시키는 것으로 나타난다. 이러한 과도한 I/O 요구는 융합된 커널의 완화 없이는 학습 처리량을 크게 저하시킨다. 더욱이, H_𝑙_[pre], H_𝑙_[post], 그리고 H_𝑙_[res]는 학습 가능한 매개변수를 포함하기 때문에, 그들의 중간 활성화가 역전파에 필요하다. 이는 GPU 메모리 사용량의 상당한 증가를 초래하며, 종종 실행 가능한 메모리 사용량을 유지하기 위해 기울기 체크포인팅을 필요로 한다. 게다가, HC는 파이프라인 병렬화(Qi et al., 2024)에서 _𝑛_배 더 많은 통신 비용을 요구하여, 더 큰 버블을 발생시키고 학습 처리량을 감소시킨다.

7

표 2 | **토큰 당 메모리 접근 비용 비교.** 이 분석은 순전파에서 잔차 스트림 유지에 의해 도입된 오버헤드를 설명하며, 층 함수 F의 내부 I/O는 제외한다.

방법 연산 읽기 (요소 수) 쓰기 (요소 수)

잔차
연결

하이퍼연결

#### **4. 방법론**

잔차 병합 2_𝐶_ _𝐶_

**총 I/O** **2C** **C**

H_𝑙_[pre], H_𝑙_[post], H_𝑙_[res] 계산 _𝑛𝐶_ _𝑛_[2] + 2_𝑛_
H_𝑙_[pre] _𝑛𝐶_ + _𝑛_ _𝐶_
H_𝑙_[post] _𝐶_ + _𝑛_ _𝑛𝐶_
H_𝑙_[res] _𝑛𝐶_ + _𝑛_[2] _𝑛𝐶_
잔차 병합 2_𝑛𝐶_ _𝑛𝐶_

**총 I/O** ( **5n** + **1** ) **C** + **n****[2]** + **2n** ( **3n** + **1** ) **C** + **n****[2]** + **2n**

**4.1. 매니폴드 제약 하이퍼 연결**

항등 사상(He et al., 2016b) 원리에서 영감을 받아, _m_HC의 핵심 전제는 잔차 사상 H_𝑙_[res]를 특정 매니폴드 위로 제약하는 것이다. 원래의 항등 사상은 H_𝑙_[res] = **I**를 강제함으로써 안정성을 보장하지만, 이는 근본적으로 잔차 스트림 내의 정보 교환을 배제하며, 이는 다중 스트림 아키텍처의 잠재력을 극대화하는 데 필수적이다. 따라서, 우리는 잔차 사상을 층 간 신호 전파의 안정성을 동시에 유지하고 잔차 스트림들 간의 상호 작용을 촉진하여 모델의 표현력을 보존하는 매니폴드 위로 투영할 것을 제안한다. 이를 위해, 우리는 H_𝑙_[res]를 이중 확률 행렬(doubly stochastic matrix)로 제한하는데, 이는 행과 열이 모두 1로 합산되는 비음수 항목을 갖는다. 형식적으로, M[res]를 이중 확률 행렬의 매니폴드(버코프 폴리토프로도 알려진)라고 하면, 우리는 H_𝑙_[res]를 PMres(H_𝑙_[res])로 제약하며, 다음과 같이 정의된다:

PMres(H_𝑙_[res]) ≔ �H_𝑙_[res] ∈ **R**_[𝑛]_ [×] _[𝑛]_ | H_𝑙_[res] **1**_𝑛_ = **1**_𝑛_, **1**[⊤]_𝑛_ [H]_𝑙_[res] = **1**[⊤]_𝑛_ [,][ H]_𝑙_[res] ⩾ 0�, (6)

여기서 **1**_𝑛_은 _𝑛_-차원의 모든 요소가 1인 벡터를 나타낸다.

_𝑛_ = 1일 때, 이중 확률 조건은 스칼라 1로 퇴화되어 원래의 항등 사상을 복원한다는 점은 주목할 가치가 있다. 이중 확률성의 선택은 대규모 모델 학습에 유익한 몇 가지 엄격한 이론적 특성을 부여한다:

1. **노름 보존:** 이중 확률 행렬의 스펙트럼 노름은 1로 제한된다 (즉, ∥H_𝑙_[res]∥2 ≤ 1). 이는 학습 가능한 사상이 비확장적임을 의미하여, 기울기 폭발 문제를 효과적으로 완화한다.
2. **합성 폐쇄성:** 이중 확률 행렬의 집합은 행렬 곱셈에 대해 폐쇄되어 있다. 이는 여러 층에 걸친 합성 잔차 사상,

- _𝐿_  - _𝑙_
_𝑖_ =1 [H]_𝐿_[res]          - _𝑖_[,][ 가 이중 확률적으로 유지되어, 모델의 전체 깊이에 걸쳐 안정성을 보존함을 보장한다.]
3. **버코프 폴리토프를 통한 기하학적 해석:** 집합 M[res]는 버코프 폴리토프를 형성하며, 이는 순열 행렬 집합의 볼록 껍질이다. 이는 명확한 기하학적 해석을 제공한다: 잔차 사상은 순열의 볼록 조합으로 작용한다. 수학적으로, 이러한 행렬의 반복적 적용은 증가하는 경향이 있다

8

스트림 간 정보 혼합을 단조적으로 유지하여 강건한 특징 융합 메커니즘으로 효과적으로 기능합니다.

또한, 입력 매핑 H _𝑙_ [pre]와 출력 매핑 H _𝑙_ [post]에 비음수 제약을 부과합니다. 이 제약은 양수 계수와 음수 계수의 합성으로 발생하는 신호 상쇄를 방지하며, 이는 특수한 매니폴드 투영으로도 간주될 수 있습니다.

**4.2. 매개변수화와 매니폴드 투영**

이 절에서는 _m_ HC에서 H _𝑙_ [pre], H _𝑙_ [post], H _𝑙_ [res]의 계산 과정을 자세히 설명합니다. _𝑙_ 번째 계층의 입력 은닉 행렬 **x** _𝑙_ ∈ **R** _[𝑛]_ [×] _[𝐶]_가 주어지면, 먼저 이를 벡터 � **x** _𝑙_ = vec( **x** _𝑙_ ) ∈ **R** [1][×] _[𝑛𝐶]_로

평탄화하여 전체 맥락 정보를 보존합니다. 그런 다음 원래의 HC 공식을 따라 동적 매핑과 정적 매핑을 다음과 같이 얻습니다:

**x**  - [′] _𝑙_ [=][ RMSNorm][(�] **[x]** _[𝑙]_ [)]

H˜ _𝑙_ [pre] = _𝛼_ [pre] _𝑙_ - (� **x** [′] _𝑙_ _[𝜑]_ [pre] _𝑙_ ) + **b** [pre] _𝑙_

H˜ _𝑙_ [post] = _𝛼_ [post] _𝑙_  - (� **x** [′] _𝑙_ _[𝜑]_ [post] _𝑙_ ) + **b** [post] _𝑙_
H˜ _𝑙_ [res] = _𝛼_ [res] _𝑙_  - mat(� **x** [′] _𝑙_ _[𝜑]_ [res] _𝑙_ [) +] **[ b]** [res] _𝑙_ [,]



(7)

여기서 _𝜑_ [pre] _𝑙_, _𝜑_ [post] _𝑙_ ∈ **R** _[𝑛𝐶]_ [×] _[𝑛]_와 _𝜑_ [res] _𝑙_ ∈ **R** _[𝑛𝐶]_ [×] _[𝑛]_ [2]는 동적 매핑을 위한 선형 투영이며, mat(·)는 **R** [1][×] _[𝑛]_ [2]에서 **R** _[𝑛]_ [×] _[𝑛]_으로의 재구성 함수입니다.

그런 다음, 최종 제약 매핑은 다음과 같이 얻어집니다:

H _𝑙_ [pre] = _𝜎_ (H [˜] _𝑙_ [pre] )



H _𝑙_ [post] = 2 _𝜎_ (H [˜] _𝑙_ [post] )
H _𝑙_ [res] = Sinkhorn-Knopp(H [˜] _𝑙_ [res] ),



(8)

여기서 _𝜎_ (·)는 시그모이드 함수를 나타냅니다. 싱크혼-크놉 연산자(Sinkhorn-Knopp(·))는 먼저 지수 연산자를 통해 모든 원소를 양수로 만든 다음, 행과 열을 번갈아 재조정하여 합이 1이 되도록 하는 반복 정규화 과정을 수행합니다. 구체적으로, 시작점으로 양수 행렬 **M** [(][0][)] = exp(H [˜] _𝑙_ [res] )가 주어지면, 정규화 반복은 다음과 같이 진행됩니다:

-                             **M** [(] _[𝑡]_ [)] = T _𝑟_ T _𝑐_ ( **M** [(] _[𝑡]_ [−][1][)] ), (9)

여기서 T _𝑟_과 T _𝑐_는 각각 행 정규화와 열 정규화를 나타냅니다. 이 과정은 _𝑡_ max →∞일 때 이중 확률 행렬 H _𝑙_ [res] = **M** [(] _[𝑡]_ [max] [)]로 수렴합니다. 실험에서는 _𝑡_ max = 20을 실용적인 값으로 선택합니다.

**4.3. 효율적인 인프라 설계**

이 절에서는 _m_ HC를 위해 맞춤화된 인프라 설계를 자세히 설명합니다. 엄격한 최적화를 통해, 대규모 모델에서 _m_ HC (_𝑛_ = 4)를 단 6.7%의 미미한 학습 오버헤드로 구현합니다.

_**4.3.1. 커널 융합**_

_m_ HC의 RMSNorm이 고차원 은닉 상태 � **x** _𝑙_ ∈ **R** [1][×] _[𝑛𝐶]_에서 작동할 때 상당한 지연을 발생시킨다는 것을 관찰하여, 노름으로 나누는 연산을 행렬 곱셈 이후로 재배치합니다.

9

이 최적화는 효율성을 향상시키면서 수학적 동등성을 유지합니다. 또한, 속도를 저해하지 않으면서 수치 정확도를 최대화하기 위해 혼합 정밀도 전략을 사용하고, 공유 메모리 접근을 가진 여러 연산을 통합된 계산 커널로 융합하여 메모리 대역폭 병목을 줄입니다. 식 (10)~(13)에 자세히 설명된 입력과 매개변수를 기반으로, H _𝑙_ [pre], H _𝑙_ [post], H _𝑙_ [res]를 계산하기 위한 세 가지 특수화된 _m_ HC 커널을 구현합니다. 이러한 커널에서 편향과 선형 투영은 **b** _𝑙_과 _𝜑𝑙_로 통합되며, RMSNorm 가중치도 _𝜑𝑙_에 흡수됩니다.

- 식 (14)~(15): � **x** _𝑙_에 대한 두 번의 스캔을 융합하는 통합 커널을 개발하여, 행렬 곱셈 유닛을 활용하여 메모리 대역폭 활용을 최대화합니다. 역전파 과정—두 번의 행렬 곱셈으로 구성됨—도 유사하게 단일 커널로 통합되어 � **x** _𝑙_의 중복 재로딩을 제거합니다. 두 커널 모두 혼합 정밀도 처리를 효율적으로 다루기 위해 세밀하게 조정된 파이프라인(로드, 캐스트, 계산, 저장)을 특징으로 합니다.

- 식 (16)~(18): 작은 계수에 대한 이러한 경량 연산은 단일 커널로 기회주의적으로 융합되어 커널 실행 오버헤드를 크게 줄입니다.

- 식 (19): 싱크혼-크놉 반복을 단일 커널 내에서 구현합니다. 역전파 과정에서는 중간 결과를 온칩에서 재계산하고 전체 반복을 순회하는 맞춤형 역전파 커널을 유도합니다.

_𝜑𝑙_ : tfloat32 [ _𝑛𝐶_, _𝑛_ [2] + 2 _𝑛_ ] (10)
**x**            - _𝑙_ : bfloat16 [1, _𝑛𝐶_ ] (11)

_𝛼_ [pre] _𝑙_, _𝛼_ [post] _𝑙_, _𝛼_ [res] _𝑙_ : float32 스칼라 (12)

**b** _𝑙_ : float32 [1, _𝑛_ [2] + 2 _𝑛_ ] (13)

- pre      H˜˜ _𝑙_, H [˜˜] _𝑙_ [post], H [˜˜] _𝑙_ [res] : float32 = � **x** _𝑙_ _𝜑𝑙_ (14)

~~√~~
_𝑟_ : float32 = �� **x**                - _𝑙_ ��2 [/] _𝑛𝐶_ (15)

�H˜ _𝑙_ [pre], H [˜] _𝑙_ [post], H [˜] _𝑙_ [res]     - : float32 = 1/ _𝑟_     - _𝛼_ [pre] _𝑙_ H˜˜ _𝑙_ [pre], _𝛼_ [post] _𝑙_ H˜˜ _𝑙_ [post], _𝛼_ [res] _𝑙_ H˜˜ _𝑙_ res� + **b** _𝑙_ (16)

-                               H _𝑙_ [pre] : float32 = _𝜎_ H˜ _𝑙_ [pre] (17)

-                                H _𝑙_ [post] : float32 = 2 _𝜎_ H˜ _𝑙_ [post] (18)

H _𝑙_ [res] : float32 = Sinkhorn-Knopp H _𝑙_ [res]      - (19)

[�] [˜]

앞서 언급한 커널에서 유도된 계수를 사용하여, 이러한 매핑을 적용하기 위한 두 개의 추가 커널을 도입합니다: 하나는 Fpre ≔ H _𝑙_ [pre] **x** _𝑙_를 위한 것이고, 다른 하나는 Fpost,res ≔
H _𝑙_ [res] **x** _𝑙_ + H _𝑙_ [post][ ⊤] F (·, ·)를 위한 것입니다. H _𝑙_ [post]와 H _𝑙_ [res]의 적용을 잔차 병합과 융합함으로써, 이 커널에서 읽는 원소의 수를 (3 _𝑛_ + 1) _𝐶_에서 ( _𝑛_ + 1) _𝐶_로, 쓰는 원소의 수를 3 _𝑛𝐶_에서 _𝑛𝐶_로 줄입니다. 대부분의 커널(식 (14)~(15) 제외)을 TileLang (Wang et al., 2025)을 사용하여 효율적으로 구현합니다. 이 프레임워크는 복잡한 계산 과정을 가진 커널의 구현을 간소화하고 최소한의 엔지니어링 노력으로 메모리 대역폭을 완전히 활용할 수 있게 합니다.

_**4.3.2. 재계산**_

_𝑛_ 개의 스트림 잔차 설계는 학습 중에 상당한 메모리 오버헤드를 도입합니다. 이를 완화하기 위해, 순전파 이후 _m_ HC 커널의 중간 활성화를 폐기하고 역전파 과정에서 즉석에서 재계산합니다. 이는 무거운 계층 함수 F 없이 _m_ HC 커널을 재실행함으로써 이루어집니다.

10

결과적으로, _𝐿𝑟_개의 연속 계층 블록에 대해, 첫 번째 계층에 대한 입력 **x** _𝑙_ 0만 저장하면 됩니다. F 내의 사전 정규화를 고려하여 경량 계수를 제외하면, 표 3은 역전파 과정을 위해 보존되는 중간 활성화를 요약합니다.

표 3 | **저장 및 재계산되는 중간 활성화** _𝐿𝑟_개의 연속 계층에서 역전파를 위해 보존된 토큰당 활성화와 재계산되는 임시 활성화를 나열합니다.
계층 _𝑙_ 0은 _𝐿𝑟_ 계층의 첫 번째 계층을 나타내며 계층 _𝑙_은 [ _𝑙_ 0, _𝑙_ 0 + _𝐿𝑟_ - 1]에 있습니다.

활성화 **x** _𝑙_ 0 F (H _𝑙_ [pre] **x** _𝑙_, W _𝑙_ ) **x** _𝑙_ H _𝑙_ [pre] **x** _𝑙_ RMSNorm(H _𝑙_ [pre] **x** _𝑙_ )

크기 (원소) _𝑛𝐶_ _𝐶_ _𝑛𝐶_ _𝐶_ _𝐶_
저장 방법 _𝐿𝑟_ 계층마다 모든 계층 _𝐿𝑟_ 계층 내 임시

_m_ HC 커널 재계산은 _𝐿𝑟_개의 연속 계층 블록에 대해 수행되므로, 총 _𝐿_개의 계층이 주어지면, 역전파 과정을 위해 모든 ⌈ _𝐿_ _[𝐿]_ _𝑟_ [⌉] [블록에 대한 첫 번째 계층 입력] **[x]** _𝑙_ 0을 지속적으로 저장해야 합니다.

이러한 상주 메모리 외에도, 재계산 과정은 활성 블록에 대해 ( _𝑛_ + 2) _𝐶_ × _𝐿𝑟_ 원소의 일시적 메모리 오버헤드를 도입하며, 이는 역전파 중 최대 메모리 사용량을 결정합니다. 결과적으로, 다음과 같이 _𝐿_ [𝑟][에 대응하는 총 메모리 공간을 최소화함으로써 최적 블록 크기] _𝐿_ [∗] _𝑟_[을 결정합니다:]

_𝑛𝐿_ (20)
_𝑛_ + 2 [.]

~~√~~

≈

- _𝐿_
_𝑛𝐶_ ×

_𝐿𝑟_

+ ( _𝑛_ + 2) _𝐶_ × _𝐿𝑟_

_𝐿_ [∗] _𝑟_ [=][ arg min]
_𝐿𝑟_

또한, 대규모 학습에서 파이프라인 병렬화는 재계산 블록이 파이프라인 스테이지 경계를 넘지 않아야 한다는 제약을 부과합니다. 이론적 최적값 _𝐿_ [∗] _𝑟_이 일반적으로 파이프라인 스테이지당 계층 수와 일치한다는 것을 관찰하여, 재계산 경계를 파이프라인 스테이지와 동기화하도록 선택합니다.

_**4.3.3. 듀얼파이프에서 통신 중첩**_

대규모 학습에서 파이프라인 병렬화는 매개변수와 기울기 메모리 공간을 완화하기 위한 표준 관행입니다. 구체적으로, 우리는 전문가 및 파이프라인 병렬화와 같은 스케일아웃 상호 연결 통신 트래픽을 효과적으로 중첩시키는 듀얼파이프 스케줄 (Liu et al., 2024b)을 채택합니다. 그러나 단일 스트림 설계에 비해, _m_ HC에서 제안된
_𝑛_ 개의 스트림 잔차는 파이프라인 스테이지 간 상당한 통신 지연을 발생시킵니다.
또한, 스테이지 경계에서 모든 _𝐿𝑟_ 계층에 대한 _m_ HC 커널의 재계산은 무시할 수 없는 계산 오버헤드를 도입합니다. 이러한 병목을 해결하기 위해, 파이프라인 스테이지 경계에서 통신과 계산의 개선된 중첩을 용이하게 하도록 듀얼파이프 스케줄을 확장합니다(그림 4 참조).

특히, 통신 스트림의 차단을 방지하기 위해, MLP (즉, FFN) 계층의 Fpost,res 커널을 전용 고우선순위 계산 스트림에서 실행합니다. 또한 주목 계층에서 장시간 실행되는 연산에 대해 지속 커널을 사용하는 것을 자제하여 장시간 정지를 방지합니다. 이 설계는 중첩된 주목 계산의 선점을 가능하게 하여, 계산 장치의 처리 유닛의 높은 활용률을 유지하면서 유연한 스케줄링을 가능하게 합니다. 또한, 재계산 과정은 각 스테이지의 초기 활성화 **x** _𝑙_ 0이 이미 로컬에 캐시되어 있으므로 파이프라인 통신 종속성으로부터 분리됩니다.

11

정상 계산 스트림

통신 스트림

고우선순위 계산 스트림

!"#$, '(#) (F) ℱ!"#$, '(#)

 ℱ!"#$, '(#)

) (B)

그림 4 | **_m_ HC를 위한 통신-계산 중첩.** _m_ HC에 의해 도입된 오버헤드를 처리하기 위해 듀얼파이프
스케줄을 확장합니다. 각 블록의 길이는 설명 목적일 뿐이며 실제 지속 시간을 나타내지 않습니다. (F), (B), (W)는 각각 순전파, 역전파, 가중치 기울기 계산을 나타냅니다. F [A]와 F [M]은 각각 주목과 MLP에 대응하는 커널을 나타냅니다.

#### **5. 실험**

**5.1. 실험 설정**

제안된 방법을 언어 모델 사전 학습을 통해 검증하고, 기준선, HC, 그리고 제안된 _m_ HC 간의 비교 분석을 수행합니다. DeepSeek-V3 (Liu et al., 2024b)에서 영감을 받은 MoE 아키텍처를 활용하여 서로 다른 평가 체제를 포괄하는 네 가지 고유한 모델 변형을 학습합니다. 구체적으로, HC와 _m_ HC 모두의 확장 비율 _𝑛_은 4로 설정됩니다. 우리의 주된 초점은 매개변수에 비례하는 데이터셋 크기로 학습된 27B 모델이며, 이는 시스템 수준 주요 결과의 대상이 됩니다. 이를 확장하여, 비례 데이터로 학습된 더 작은 3B 및 9B 모델을 통합하여 연산 확장 동작을 분석하며, 이를 통해 다양한 연산에 걸친 성능 추세를 관찰할 수 있습니다. 또한, 토큰 확장 동작을 구체적으로 조사하기 위해, 1조 토큰의 고정 코퍼스로 학습된 별도의 3B 모델을 학습합니다. 자세한 모델 구성 및 학습 하이퍼파라미터는 부록 A.1에 제공됩니다.

**5.2. 주요 결과**

그림 5 | **매니폴드 제약 하이퍼 연결 (** _**m**_ **HC)의 학습 안정성.** 이 그림은
(a) 기준선에 대한 _m_ HC와 HC의 절대 학습 손실 차이, 그리고 (b) 세 가지 방법의 기울기 노름을 보여줍니다. 모든 실험은 27B 모델을 활용합니다. 결과는 _m_ HC가 손실과 기울기 노름 모두에서 개선된 안정성을 보인다는 것을 보여줍니다.

27B 모델의 학습 안정성과 수렴을 검토하는 것으로 시작합니다. 그림 5 (a)에서 보듯이, _m_ HC는 HC에서 관찰된 학습 불안정성을 효과적으로 완화하여 기준선에 비해 0.021의 최종 손실 감소를 달성합니다. 이러한 개선된 안정성은 그림 5 (b)의 기울기 노름 분석에서 더욱 확증되며, _m_ HC는 HC보다 현저히 나은 동작을 보이며, 기준선과 유사한 안정적인 프로파일을 유지합니다.

12

표 4 | **27B 모델에 대한 시스템 수준 벤치마크 결과.** 이 표는 8개의 다양한 다운스트림
벤치마크에 걸친 기준선, HC, _m_ HC의 제로샷 및 퓨샷 성능을 비교합니다. _m_ HC는 기준선을 일관되게 능가하고 대부분의 벤치마크에서 HC를 초과하여 대규모 사전 학습에서의 효과를 입증합니다.

**벤치마크** BBH DROP GSM8K HellaSwag MATH MMLU PIQA TriviaQA
**(지표)** (EM) (F1) (EM) (Acc.) (EM) (Acc.) (Acc.) (EM)

**# Shots** 3-shot 3-shot 8-shot 10-shot 4-shot 5-shot 0-shot 5-shot

표 4는 다양한 벤치마크 세트에서의 다운스트림 성능을 제시한다 (Bisk et al., 2020; Cobbe et al., 2021; Hendrycks et al., 2020, 2021; Joshi et al., 2017; Zellers et al., 2019). _m_ HC는 포괄적인 개선을 달성하여 일관되게 기준선을 능가하고 대부분의 작업에서 HC를 초과한다. 특히, HC와 비교하여 _m_ HC는 모델의 추론 능력을 더욱 향상시켜 BBH (Suzgun et al., 2022)에서 2.1%, DROP (Dua et al., 2019)에서 2.3%의 성능 향상을 제공한다.

**5.3. 확장성 실험**

그림 6 | **기준선과 비교한** _**m**_ **HC의 확장 특성. (a) 연산 확장성 곡선.** 실선은 다양한 연산 예산에 걸친 성능 차이를 나타낸다. 각 점은 3B와 9B에서 27B 파라미터까지 확장되는 모델 크기와 데이터셋 크기의 특정 연산 최적 구성을 나타낸다. **(b) 토큰 확장성 곡선.** 학습 중 3B 모델의 궤적. 각 점은 서로 다른 학습 토큰에서 모델의 성능을 나타낸다. 자세한 아키텍처 및 학습 구성은 부록 A.1에 제공되어 있다.

우리의 접근법의 확장성을 평가하기 위해, 서로 다른 규모에 걸쳐 기준선 대비 _m_ HC의 상대적 손실 개선을 보고한다. 그림 6 (a)에서는 3B, 9B, 27B 파라미터에 걸친 연산 확장성 곡선을 그린다. 궤적은 성능 이점이 더 높은 연산 예산에서도 견고하게 유지되며, 미미한 감쇠만 보인다는 것을 나타낸다. 또한, 그림 6 (b)에서 실행 내 동역학을 조사하며, 이는 3B 모델에 대한 토큰 확장성 곡선을 제시한다. 종합적으로, 이러한 발견은 대규모 시나리오에서 _m_ HC의 효과를 검증한다. 이 결론은 우리의 사내 대규모 학습 실험에 의해 더욱 입증된다.

13

그림 7 | **매니폴드 제약 하이퍼 연결 (** _**m**_ **HC)의 전파 안정성.** 이 그림은 (a) 단일 계층 매핑 PMres (H _𝑙_ [res] )와 (b) 27B 모델 내에서 복합 매핑 [�] _𝑖_ _[𝐿]_ = [−] 1 _[𝑙]_ [P][M][res] [(H] _𝐿_ [res] - _𝑖_ [)]의 전파 동역학을 보여준다. 결과는 _m_ HC가 HC와 비교하여 전파 안정성을 크게 향상시킨다는 것을 보여준다.

그림 8 | **학습 가능한 매핑의 시각화.** 이 그림은 HC (첫 번째 행)와 _m_ HC (두 번째 행)에 대한 대표적인 단일 계층 및 복합 매핑을 표시한다. 각 행렬은 선택된 시퀀스 내의 모든 토큰에 대해 평균을 구하여 계산된다. y축과 x축을 따라 표시된 레이블은 각각 순전파 신호 증폭 (행 합)과 역전파 기울기 증폭 (열 합)을 나타낸다.

**5.4. 안정성 분석**

그림 3과 유사하게, 그림 7은 _m_ HC의 전파 안정성을 보여준다. 이상적으로, 단일 계층 매핑은 이중 확률 제약을 만족하며, 이는 순전파 신호 증폭과 역전파 기울기 증폭이 모두 1이어야 함을 의미한다. 그러나 싱크혼-크놉 알고리즘을 활용하는 실제 구현에서는 연산 효율성을 달성하기 위해 반복 횟수를 제한해야 한다. 우리의 설정에서는 20회 반복을 사용하여 근사 솔루션을 얻는다. 결과적으로, 그림 7(a)에서 보여지듯이 역전파 기울기 증폭이 1에서 약간 벗어난다. 그림 7(b)에 표시된 복합 경우에서는 편차가 증가하지만 제한되어 있으며, 최대값이 약 1.6에 도달한다. 특히, HC의 거의 3000에 달하는 최대 이득 크기와 비교하여 _m_ HC는 이를 3차수 크기로 감소시킨다. 이러한 결과는 _m_ HC가 HC와 비교하여 전파 안정성을 크게 향상시켜 안정적인 순전파 신호 및 역전파 기울기 흐름을 보장한다는 것을 보여준다. 추가로, 그림 8은 대표적인 매핑을 표시한다. 우리는 HC의 경우 최대 이득이 클 때 다른 값들도 크게 나타나는 경향이 있으며, 이는 모든 전파 경로에 걸친 일반적인 불안정성을 나타낸다는 것을 관찰한다. 대조적으로, _m_ HC는 일관되게 안정적인 결과를 산출한다.

14

#### **6. 결론 및 전망**

본 논문에서는 하이퍼 연결 (HC)에서 제안된 바와 같이 잔차 스트림의 폭을 확장하고 연결을 다양화하는 것이 성능 향상을 가져오는 동시에, 이러한 연결의 무제약적 특성이 신호 발산을 유발한다는 것을 밝힌다. 이러한 교란은 계층 간 신호 에너지 보존을 손상시켜 학습 불안정성을 유도하고 심층 네트워크의 확장성을 저해한다. 이러한 과제를 해결하기 위해, 우리는 잔차 연결 공간을 특정 매니폴드에 투영하는 일반화된 프레임워크인 **매니폴드 제약 하이퍼 연결** ( _**m**_ **HC** )을 소개한다. 싱크혼-크놉 알고리즘을 사용하여 잔차 매핑에 이중 확률 제약을 적용함으로써, _m_ HC는 신호 전파를 특징의 볼록 조합으로 변환한다. 실험 결과는 _m_ HC가 효과적으로 항등 사상 특성을 복원하여 기존 HC와 비교하여 우수한 확장성으로 안정적인 대규모 학습을 가능하게 한다는 것을 확인한다. 중요하게도, 효율적인 인프라 수준 최적화를 통해 _m_ HC는 무시할 수 있는 연산 오버헤드로 이러한 개선을 제공한다.

HC 패러다임의 일반화된 확장으로서, _m_ HC는 향후 연구를 위한 여러 유망한 방향을 제시한다. 본 연구는 안정성을 보장하기 위해 이중 확률 행렬을 활용하지만, 프레임워크는 특정 학습 목표에 맞춤화된 다양한 매니폴드 제약의 탐색을 수용한다. 우리는 서로 다른 기하학적 제약에 대한 추가 조사가 가소성과 안정성 간의 균형을 더 잘 최적화하는 새로운 방법을 산출할 수 있을 것으로 기대한다. 또한, _m_ HC가 거시적 아키텍처 설계에 대한 커뮤니티의 관심을 다시 불러일으키기를 바란다. 토폴로지 구조가 최적화 및 표현 학습에 미치는 영향에 대한 이해를 심화함으로써, _m_ HC는 현재의 한계를 해결하고 차세대 기초 아키텍처의 진화를 위한 새로운 경로를 잠재적으로 조명할 것이다.

#### **참고문헌**

J. Ainslie, J. Lee-Thorp, M. De Jong, Y. Zemlyanskiy, F. Lebrón, and S. Sanghai. Gqa: Training generalized multi-query transformer models from multi-head checkpoints. arXiv preprint arXiv:2305.13245, 2023.

Y. Bisk, R. Zellers, R. L. Bras, J. Gao, and Y. Choi. PIQA: reasoning about physical commonsense in natural language. In The Thirty-Fourth AAAI Conference on Artifcial Intelligence, AAAI
2020, The Thirty-Second Innovative Applications of Artifcial Intelligence Conference, IAAI
2020, The Tenth AAAI Symposium on Educational Advances in Artifcial Intelligence, EAAI
2020, New York, NY, USA, February 7-12, 2020, pages 7432–7439. AAAI Press, 2020. doi:
10.1609/aaai.v34i05.6239. URL `https://doi.org/10.1609/aaai.v34i05.6239` .

T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020.

Y. Chai, S. Jin, and X. Hou. Highway transformer: Self-gating enhanced self-attentive networks.
In D. Jurafsky, J. Chai, N. Schluter, and J. Tetreault, editors, Proceedings of the 58th Annual
Meeting of the Association for Computational Linguistics, pages 6887–6900, Online, July
2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.616. URL
`https://aclanthology.org/2020.acl-main.616/` .

F. Chollet. Xception: Deep learning with depthwise separable convolutions. In Proceedings of

the IEEE conference on computer vision and pattern recognition, pages 1251–1258, 2017.

15

K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek, J. Hilton, R. Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.

T. Dao, D. Y. Fu, S. Ermon, A. Rudra, and C. Ré. FlashAttention: Fast and memory-efficient exact attention with IO-awareness. In Advances in Neural Information Processing Systems (NeurIPS), 2022.

D. Dua, Y. Wang, P. Dasigi, G. Stanovsky, S. Singh, and M. Gardner. DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs. In J. Burstein, C. Doran, and
T. Solorio, editors, Proceedings of the 2019 Conference of the North American Chapter of the
Association for Computational Linguistics: Human Language Technologies, NAACL-HLT
2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pages 2368–
2378. Association for Computational Linguistics, 2019. doi: 10.18653/V1/N19-1246. URL
`https://doi.org/10.18653/v1/n19-1246` .

Y. Fang, Y. CAI, J. Chen, J. Zhao, G. Tian, and G. Li. Cross-layer retrospective retrieving via layer attention. In The Eleventh International Conference on Learning Representations, 2023. URL
`https://openreview.net/forum?id=pvgEL1yS3Ql` .

W. Fedus, B. Zoph, and N. Shazeer. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. Journal of Machine Learning Research, 23(120):1–39, 2022.

K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proceedings

of the IEEE conference on computer vision and pattern recognition, pages 770–778, 2016a.

K. He, X. Zhang, S. Ren, and J. Sun. Identity mappings in deep residual networks. In European

conference on computer vision, pages 630–645. Springer, 2016b.

M. Heddes, A. Javanmard, K. Axiotis, G. Fu, M. Bateni, and V. Mirrokni. Deepcrossattention:
Supercharging transformer residual connections. In Forty-second International Conference on Machine Learning, 2025. URL `https://openreview.net/forum?id=j3JBfFnGYh` .

D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020.

D. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang, D. Song, and J. Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021.

J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. de Las Casas, L. A. Hendricks, J. Welbl, A. Clark, T. Hennigan, E. Noland, K. Millican, G. van den Driessche, B. Damoc, A. Guy, S. Osindero, K. Simonyan, E. Elsen, O. Vinyals, J. Rae, and L. Sifre.
An empirical analysis of compute-optimal large language model training. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural
Information Processing Systems, volume 35, pages 30016–30030. Curran Associates, Inc., 2022.
URL `https://proceedings.neurips.cc/paper_files/paper/2022/file/c1e2faff6f588870935f114ebe04a3e5-Paper-Conference.pdf` .

G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger. Densely connected convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4700–4708, 2017.

16

M. Joshi, E. Choi, D. Weld, and L. Zettlemoyer. TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension. In R. Barzilay and M.-Y. Kan, editors, Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long
Papers), pages 1601–1611, Vancouver, Canada, July 2017. Association for Computational
Linguistics. doi: 10.18653/v1/P17-1147. URL `https://aclanthology.org/P17-1147` .

G. Larsson, M. Maire, and G. Shakhnarovich. Fractalnet: Ultra-deep neural networks without residuals. arXiv preprint arXiv:1605.07648, 2016.

D. Lepikhin, H. Lee, Y. Xu, D. Chen, O. Firat, Y. Huang, M. Krikun, N. Shazeer, and Z. Chen.
Gshard: Scaling giant models with conditional computation and automatic sharding. arXiv preprint arXiv:2006.16668, 2020.

A. Liu, B. Feng, B. Wang, B. Wang, B. Liu, C. Zhao, C. Dengr, C. Ruan, D. Dai, D. Guo, et al.
Deepseek-v2: A strong, economical, and efficient mixture-of-experts language model. arXiv preprint arXiv:2405.04434, 2024a.

A. Liu, B. Feng, B. Xue, B. Wang, B. Wu, C. Lu, C. Zhao, C. Deng, C. Zhang, C. Ruan, et al.
Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024b.

I. Loshchilov and F. Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017.

B. Mak and J. Flanigan. Residual matrix transformers: Scaling the size of the residual stream.

arXiv preprint arXiv:2506.22696, 2025.

G. Menghani, R. Kumar, and S. Kumar. LAurel: Learned augmented residual layer. In
Forty-second International Conference on Machine Learning, 2025. URL `https://openreview.net/forum?id=rUDRWP9WvZ` .

M. Pagliardini, A. Mohtashami, F. Fleuret, and M. Jaggi. Denseformer: Enhancing information flow in transformers via depth weighted averaging. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. URL `https://openreview.net/forum?id=kMnoh7CXrq` .

P. Qi, X. Wan, G. Huang, and M. Lin. Zero bubble (almost) pipeline parallelism. In The Twelfth

International Conference on Learning Representations, 2024. URL `https://openreview.net/forum?id=tuzTN0eIO5` .

N. Shazeer. Fast transformer decoding: One write-head is all you need. arXiv preprint arXiv:1911.02150, 2019.

N. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. Le, G. Hinton, and J. Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538, 2017.

R. Sinkhorn and P. Knopp. Concerning nonnegative matrices and doubly stochastic matrices.

Pacifc Journal of Mathematics, 21(2):343–348, 1967.

R. K. Srivastava, K. Greff, and J. Schmidhuber. Training very deep networks. In C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information
Processing Systems, volume 28. Curran Associates, Inc., 2015. URL `https://proceedings.neurips.cc/paper_files/paper/2015/file/215a71a12769b056c3c32e7299f1c5ed-Paper.pdf` .

17

J. Su, M. Ahmed, Y. Lu, S. Pan, W. Bo, and Y. Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024.

M. Suzgun, N. Scales, N. Schärli, S. Gehrmann, Y. Tay, H. W. Chung, A. Chowdhery, Q. V. Le, E. H. Chi, D. Zhou, et al. Challenging big-bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261, 2022.

H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozière, N. Goyal, E. Hambro, F. Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.

A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.

L. Wang, H. Gao, C. Zhao, X. Sun, and D. Dai. Auxiliary-loss-free load balancing strategy for mixture-of-experts. arXiv preprint arXiv:2408.15664, 2024.

L. Wang, Y. Cheng, Y. Shi, Z. Tang, Z. Mo, W. Xie, L. Ma, Y. Xia, J. Xue, F. Yang, et al. Tilelang: A composable tiled programming model for ai systems. arXiv preprint arXiv:2504.17577, 2025.

D. Xiao, Q. Meng, S. Li, and X. Yuan. Muddformer: Breaking residual bottlenecks in transformers via multiway dynamic dense connections. arXiv preprint arXiv:2502.12170, 2025.

S. Xie, R. Girshick, P. Dollár, Z. Tu, and K. He. Aggregated residual transformations for deep neural networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1492–1500, 2017.

S. Xie, H. Zhang, J. Guo, X. Tan, J. Bian, H. H. Awadalla, A. Menezes, T. Qin, and R. Yan. Residual:
Transformer with dual residual connections, 2023. URL `[https://arxiv.org/abs/2304.1](https://arxiv.org/abs/2304.14802)`
`[4802](https://arxiv.org/abs/2304.14802)` .

F. Yu, D. Wang, E. Shelhamer, and T. Darrell. Deep layer aggregation. In Proceedings of the

IEEE conference on computer vision and pattern recognition, pages 2403–2412, 2018.

R. Zellers, A. Holtzman, Y. Bisk, A. Farhadi, and Y. Choi. HellaSwag: Can a machine really finish your sentence? In A. Korhonen, D. R. Traum, and L. Màrquez, editors, Proceedings of the 57th
Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July
28- August 2, 2019, Volume 1: Long Papers, pages 4791–4800. Association for Computational
Linguistics, 2019. doi: 10.18653/v1/p19-1472. URL `[https://doi.org/10.18653/v1/p1](https://doi.org/10.18653/v1/p19-1472)`
`[9-1472](https://doi.org/10.18653/v1/p19-1472)` .

B. Zhang and R. Sennrich. Root mean square layer normalization. Advances in neural information processing systems, 32, 2019.

D. Zhu, H. Huang, Z. Huang, Y. Zeng, Y. Mao, B. Wu, Q. Min, and X. Zhou. Hyper-connections.

arXiv preprint arXiv:2409.19606, 2024.

18

#### **A. 부록**

**A.1. 상세 모델 명세 및 하이퍼파라미터.**

표 5 | **상세 모델 명세 및 하이퍼파라미터.** 이 표는 DeepSeek-V3 (Liu et al., 2024b) 아키텍처를 기반으로 한 3B, 9B, 27B 모델의 아키텍처 구성을 제시한다. 이는 실험에서 사용된 최적화 및 학습 프로토콜과 함께, 잔차 스트림 확장 및 싱크혼-크놉 설정을 포함한 _m_ HC 및 HC의 구체적인 하이퍼파라미터를 설명한다.

**속성** | 3B | 9B | 27B | 3B 1T 토큰

어휘 파라미터 | 331M | 496M | 662M | 331M
활성 파라미터 | 612M | 1.66B | 4.14B | 612M
전체 파라미터 | 2.97B | 9.18B | 27.0B | 2.97B

계층 수 | 12 | 18 | 30 | 12
선행 밀집 계층 | 1 | 1 | 1 | 1
라우팅 전문가 | 64 | 64 | 72 | 64
활성 전문가 | 6 | 6 | 6 | 6
공유 전문가 | 2 | 2 | 2 | 2
차원 | 1280 | 1920 | 2560 | 1280
FFN 차원 | 896 | 1280 | 1536 | 896
부하 분산 방법 | Loss-Free (Wang et al., 2024) | Loss-Free (Wang et al., 2024) | Loss-Free (Wang et al., 2024) | Loss-Free
어텐션 헤드 | 16 | 24 | 32 | 16
어텐션 차원 | 128 | 128 | 128 | 128
어텐션 변형 | MLA (Liu et al., 2024a) | MLA (Liu et al., 2024a) | MLA (Liu et al., 2024a) | MLA
KV 순위 | 512 | 512 | 512 | 512
위치 임베딩 | RoPE (Su et al., 2024) | RoPE (Su et al., 2024) | RoPE (Su et al., 2024) | RoPE
RoPE 차원 | 64 | 64 | 64 | 64
RoPE _𝜃_ | 10000 | 10000 | 10000 | 10000
계층 정규화 유형 | RMSNorm (Zhang and Sennrich, 2019) | RMSNorm (Zhang and Sennrich, 2019) | RMSNorm (Zhang and Sennrich, 2019) | RMSNorm
계층 정규화 _𝜀_ | 1e-20 | 1e-20 | 1e-20 | 1e-20

_m_ HC/HC 확장 비율 _𝑛_ | 4 | 4 | 4 | 4
_m_ HC/HC 게이팅 인자 초기값 _𝛼_ | 0.01 | 0.01 | 0.01 | 0.01
_m_ HC 싱크혼-크놉 _𝑡_ max | 20 | 20 | 20 | 20

시퀀스 길이 | 4096 | 4096 | 4096 | 4096
어휘 크기 | 129280 | 129280 | 129280 | 129280
배치 크기 | 320 | 512 | 1280 | 2560
학습 스텝 | 30000 | 50000 | 50000 | 100000
학습 토큰 | 39.3B | 105B | 262B | 1.05T
워밍업 스텝 | 2000 | 2000 | 2000 | 2000
최적화기 | AdamW (Loshchilov and Hutter, 2017) | AdamW (Loshchilov and Hutter, 2017) | AdamW (Loshchilov and Hutter, 2017) | AdamW
AdamW 베타 | (0.9, 0.95) | (0.9, 0.95) | (0.9, 0.95) | (0.9, 0.95)
AdamW _𝜀_ | 1e-20 | 1e-20 | 1e-20 | 1e-20
기본 학습률 | 8.6e-4 | 5.9e-4 | 4.0e-4 | 9.0e-4
학습률 스케줄러 | Step | Step | Step | Step
학습률 감소 스텝 비율 | [0.8 ×, 0.9 ×] | [0.8 ×, 0.9 ×] | [0.8 ×, 0.9 ×] | [0.8 ×, 0.9 ×]
학습률 감소율 | [0.316, 0.1] | [0.316, 0.1] | [0.316, 0.1] | [0.316, 0.1]
가중치 감쇠 | 0.1 | 0.1 | 0.1 | 0.1

19

<!-- Extraction Warnings:
  - Detected potential broken math subscripts/superscripts. PDF may use custom fonts for mathematical notation.
  - Found 39 characters from Unicode Private Use Area. These may be custom glyphs that didn't convert properly.
  - Document appears to contain mathematical equations. Consider using OCR-based tools for better equation extraction.
-->