mHC: Manifold-Constrained Hyper-Connections
Zhenda Xie*â€ , Yixuan Wei*, Huanqi Cao*,
Chenggang Zhao, Chengqi Deng, Jiashi Li, Damai Dai, Huazuo Gao, Jiang Chang,
Kuai Yu, Liang Zhao, Shangyan Zhou, Zhean Xu, Zhengyan Zhang, Wangding Zeng,
Shengding Hu, Yuqing Wang, Jingyang Yuan, Lean Wang, Wenfeng Liang
DeepSeek-AI
Abstract
Recently, studies exemplified by Hyper-Connections (HC) have extended the ubiquitous resid-
ual connection paradigm established over the past decade by expanding the residual stream
width and diversifying connectivity patterns. While yielding substantial performance gains,
this diversification fundamentally compromises the identity mapping property intrinsic to
the residual connection, which causes severe training instability and restricted scalability, and
additionally incurs notable memory access overhead. To address these challenges, we pro-
pose Manifold-Constrained Hyper-Connections (mHC), a general framework that projects
the residual connection space of HC onto a specific manifold to restore the identity mapping
property, while incorporating rigorous infrastructure optimization to ensure efficiency. Em-
pirical experiments demonstrate that mHC is effective for training at scale, offering tangible
performance improvements and superior scalability. We anticipate that mHC, as a flexible and
practical extension of HC, will contribute to a deeper understanding of topological architecture
design and suggest promising directions for the evolution of foundational models.
(a) Residual Connection
(b) Hyper-Connections (HC)
(c) Manifold-Constrained HC (mHC)
Layer â„±
x!
x!"#
Res Mapping
â„‹!
$%&
Pre Mapping
â„‹!
'$%
Post Mapping
â„‹!
'(&)
Layer â„±
x!"#
h!
$%&
x!
h!
'(&)
h!
*+
h!
(,)
Res Mapping
ğ’«â„³!"#(â„‹!
$%&)
Pre Mapping
ğ’«â„³$!"(â„‹!
'$%)
Post Mapping
ğ’«â„³$%#&(â„‹!
'(&))
Layer â„±
x!"#
h!
$%&
x!
h!
'(&)
h!
*+
h!
(,)
Figure 1 | Illustrations of Residual Connection Paradigms. This figure compares the structural
design of (a) standard Residual Connection, (b) Hyper-Connections (HC), and (c) our proposed
Manifold-Constrained Hyper-Connections (mHC). Unlike the unconstrained HC, mHC focuses
on optimizing the residual connection space by projecting the matrices onto a constrained
manifold to ensure stability.
*Core contributors. â€ Corresponding author: xie.zhenda@deepseek.com
arXiv:2512.24880v2  [cs.CL]  5 Jan 2026


Contents
1
Introduction
3
2
Related Works
4
2.1
Micro Design
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4
2.2
Macro Design . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5
3
Preliminary
5
3.1
Numerical Instability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
6
3.2
System Overhead . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7
4
Method
8
4.1
Manifold-Constrained Hyper-Connections
. . . . . . . . . . . . . . . . . . . . . .
8
4.2
Parameterization and Manifold Projection . . . . . . . . . . . . . . . . . . . . . . .
9
4.3
Efficient Infrastructure Design . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
9
4.3.1
Kernel Fusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
9
4.3.2
Recomputing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
10
4.3.3
Overlapping Communication in DualPipe
. . . . . . . . . . . . . . . . . .
11
5
Experiments
12
5.1
Experimental Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
12
5.2
Main Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
12
5.3
Scaling Experiments
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
13
5.4
Stability Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
14
6
Conclusion and Outlook
15
A Appendix
19
A.1 Detailed Model Specifications and Hyper-parameters. . . . . . . . . . . . . . . . .
19
2


1. Introduction
Deep neural network architectures have undergone rapid evolution since the introduction of
ResNets (He et al., 2016a). As illustrated in Fig. 1(a), the structure of a single-layer can be
formulated as follows:
xğ‘™+1 = xğ‘™+ F (xğ‘™, Wğ‘™),
(1)
where xğ‘™and xğ‘™+1 denote the ğ¶-dimensional input and output of the ğ‘™-th layer, respectively,
and F represents the residual function. Although the residual function F has evolved over
the past decade to include various operations such as convolution, attention mechanisms, and
feed forward networks, the paradigm of the residual connection has maintained its original
form. Accompanying the progression of Transformer (Vaswani et al., 2017) architecture, this
paradigm has currently established itself as a fundamental design element in large language
models (LLMs) (Brown et al., 2020; Liu et al., 2024b; Touvron et al., 2023).
This success is primarily attributed to the concise form of the residual connection. More
importantly, early research (He et al., 2016b) revealed that the identity mapping property of the
residual connection maintains stability and efficiency during large-scale training. By recursively
extending the residual connection across multiple layers, Eq. (1) yields:
xğ¿= xğ‘™+
ğ¿âˆ’1
âˆ‘ï¸
ğ‘–=ğ‘™
F (xğ‘–, Wğ‘–),
(2)
where ğ¿and ğ‘™correspond to deeper and shallower layers, respectively. The term identity
mapping refers to the component xğ‘™itself, which emphasizes the property that the signal from
the shallower layer maps directly to the deeper layer without any modification.
Recently, studies exemplified by Hyper-Connections (HC) (Zhu et al., 2024) have introduced
a new dimension to the residual connection and empirically demonstrated its performance
potential. The single-layer architecture of HC is illustrated in Fig. 1(b). By expanding the width of
the residual stream and enhancing connection complexity, HC significantly increases topological
complexity without altering the computational overhead of individual units regarding FLOPs.
Formally, single-layer propagation in HC is defined as:
xğ‘™+1 = Hres
ğ‘™
xğ‘™+ Hpost âŠ¤
ğ‘™
F (Hpre
ğ‘™
xğ‘™, Wğ‘™),
(3)
where xğ‘™and xğ‘™+1 denote the input and output of the ğ‘™-th layer, respectively. Unlike the formu-
lation in Eq. (1), the feature dimension of xğ‘™and xğ‘™+1 is expanded from ğ¶to ğ‘›Ã— ğ¶, where ğ‘›is
the expansion rate. The term Hres
ğ‘™
âˆˆRğ‘›Ã—ğ‘›represents a learnable mapping that mixes features
within the residual stream. Also as a learnable mapping, Hpre
ğ‘™
âˆˆR1Ã—ğ‘›aggregates features from
the ğ‘›ğ¶-dim stream into a ğ¶-dim layer input, and conversely, Hpost
ğ‘™
âˆˆR1Ã—ğ‘›maps the layer output
back onto the stream.
However, as the training scale increases, HC introduces potential risks of instability. The
primary concern is that the unconstrained nature of HC compromises the identity mapping
property when the architecture extends across multiple layers. In architectures comprising
multiple parallel streams, an ideal identity mapping serves as a conservation mechanism. It
ensures that the average signal intensity across streams remains invariant during both forward
and backward propagation. Recursively extending HC to multiple layers via Eq. (3) yields:
xğ¿=
 ğ¿âˆ’ğ‘™
Ã–
ğ‘–=1
Hres
ğ¿âˆ’ğ‘–
!
xğ‘™+
ğ¿âˆ’1
âˆ‘ï¸
ğ‘–=ğ‘™
Â©Â­
Â«
ğ¿âˆ’1âˆ’ğ‘–
Ã–
ğ‘—=1
Hres
ğ¿âˆ’ğ‘—
ÂªÂ®
Â¬
Hpost âŠ¤
ğ‘–
F (Hpre
ğ‘–
xğ‘–, Wğ‘–),
(4)
3


where ğ¿and ğ‘™represent a deeper layer and a shallower layer, respectively. In contrast to Eq. (2),
the composite mapping Ãğ¿âˆ’ğ‘™
ğ‘–=1 Hres
ğ¿âˆ’ğ‘–in HC fails to preserve the global mean of the features. This
discrepancy leads to unbounded signal amplification or attenuation, resulting in instability
during large-scale training. A further consideration is that, while HC preserves computational
efficiency in terms of FLOPs, the hardware efficiency concerning memory access costs for the
widened residual stream remains unaddressed in the original design. These factors collectively
restrict the practical scalability of HC and hinder its application in large-scale training.
To address these challenges, we propose Manifold-Constrained Hyper-Connections (mHC),
as shown in Fig. 1(c), a general framework that projects the residual connection space of HC
onto a specific manifold to restore the identity mapping property, while incorporating rigorous
infrastructure optimization to ensure efficiency. Specifically, mHC utilizes the Sinkhorn-Knopp
algorithm (Sinkhorn and Knopp, 1967) to entropically project Hres
ğ‘™
onto the Birkhoff polytope.
This operation effectively constrains the residual connection matrices within the manifold
that is constituted by doubly stochastic matrices. Since the row and column sums of these
matrices equal to 1, the operation Hres
ğ‘™
xğ‘™functions as a convex combination of the input features.
This characteristic facilitates a well-conditioned signal propagation where the feature mean
is conserved, and the signal norm is strictly regularized, effectively mitigating the risk of
vanishing or exploding signals. Furthermore, due to the closure of matrix multiplication for
doubly stochastic matrices, the composite mapping Ãğ¿âˆ’ğ‘™
ğ‘–=1 Hres
ğ¿âˆ’ğ‘–retains this conservation property.
Consequently, mHC effectively maintains the stability of identity mappings between arbitrary
depths. To ensure efficiency, we employ kernel fusion and develop mixed precision kernels
utilizing TileLang (Wang et al., 2025). Furthermore, we mitigate the memory footprint through
selective recomputing and carefully overlap communication within the DualPipe schedule (Liu
et al., 2024b).
Extensive experiments on language model pretraining demonstrate that mHC exhibits
exceptional stability and scalability while maintaining the performance advantages of HC. In-
house large-scale training indicates that mHC supports training at scale and introduces only a
6.7% additional time overhead when expansion rate ğ‘›= 4.
2. Related Works
Architectural advancements in deep learning can be primarily classified into micro-design and
macro-design. Micro-design concerns the internal architecture of computational blocks, specifying
how features are processed across spatial, temporal, and channel dimensions. In contrast,
macro-design establishes the inter-block topological structure, thereby dictating how feature
representations are propagated, routed, and merged across distinct layers.
2.1. Micro Design
Driven by parameter sharing and translation invariance, convolution initially dominated the pro-
cessing of structured signals. While subsequent variations such as depthwise separable (Chollet,
2017) and grouped convolutions (Xie et al., 2017) optimized efficiency, the advent of Trans-
formers (Vaswani et al., 2017) established Attention and Feed-Forward Networks (FFNs) as
the fundamental building blocks of modern architecture. Attention mechanisms facilitate
global information propagation, while FFNs enhance the representational capacity of individual
features. To balance performance with the computational demands of LLMs, attention mecha-
nisms have evolved towards efficient variants such as Multi-Query Attention (MQA) (Shazeer,
2019), Grouped-Query Attention (GQA) (Ainslie et al., 2023), and Multi-Head Latent Attention
4


(MLA) (Liu et al., 2024a). Simultaneously, FFNs have been generalized into sparse computing
paradigms via Mixture-of-Experts (MoE) (Fedus et al., 2022; Lepikhin et al., 2020; Shazeer et al.,
2017), allowing for massive parameter scaling without proportional computational costs.
2.2. Macro Design
Macro-design governs the global topology of the network (Srivastava et al., 2015). Following
ResNet (He et al., 2016a), architectures such as DenseNet (Huang et al., 2017) and Fractal-
Net (Larsson et al., 2016) aimed to enhance performance by increasing topological complexity
through dense connectivity and multi-path structures, respectively. Deep Layer Aggregation
(DLA) (Yu et al., 2018) further extended this paradigm by recursively aggregating features across
various depths and resolutions.
More recently, the focus of macro-design has shifted toward expanding the width of the
residual stream (Chai et al., 2020; Fang et al., 2023; Heddes et al., 2025; Mak and Flanigan,
2025; Menghani et al., 2025; Pagliardini et al., 2024; Xiao et al., 2025; Xie et al., 2023; Zhu et al.,
2024). Hyper-Connections (HC) (Zhu et al., 2024) introduced learnable matrices to modulate
connection strengths among features at varying depths, while the Residual Matrix Transformer
(RMT) (Mak and Flanigan, 2025) replaced the standard residual stream with an outer-product
memory matrix to facilitate feature storage. Similarly, MUDDFormer (Xiao et al., 2025) employs
multiway dynamic dense connections to optimize cross-layer information flow. Despite their
potential, these approaches compromise the inherent identity mapping property of the residual
connection, thereby introducing instability and hindering scalability. Furthermore, they incur
significant memory access overhead due to expanded feature widths. Building upon HC,
the proposed mHC restricts the residual connection space onto a specific manifold to restore
the identity mapping property, while also incorporating rigorous infrastructure optimizations
to ensure efficiency. This approach enhances stability and scalability while maintaining the
topological benefits of expanded connections.
3. Preliminary
We first establish the notation used in this work. In the HC formulation, the input to the ğ‘™-th layer,
xğ‘™âˆˆR1Ã—ğ¶, is expanded by a factor of ğ‘›to construct a hidden matrix xğ‘™= (xâŠ¤
ğ‘™,0, . . . , xâŠ¤
ğ‘™,ğ‘›âˆ’1)âŠ¤âˆˆRğ‘›Ã—ğ¶
which can be viewed as ğ‘›-stream residual. This operation effectively broadens the width of
the residual stream. To govern the read-out, write-in, and updating processes of this stream,
HC introduces three learnable linear mappingsâ€”Hpre
ğ‘™
, Hpost
ğ‘™
âˆˆR1Ã—ğ‘›, and Hres
ğ‘™
âˆˆRğ‘›Ã—ğ‘›. These
mappings modify the standard residual connection shown in Eq. (1), resulting in the formulation
given in Eq. (3).
In the HC formulation, learnable mappings are composed of two parts of coefficients: the
input-dependent one and the global one, referred to as dynamic mappings and static mappings,
respectively. Formally, HC computes the coefficients as follows:
ï£±ï£´ï£´ï£´ï£´ï£´ï£´ï£²
ï£´ï£´ï£´ï£´ï£´ï£´ï£³
Ëœxğ‘™= RMSNorm(xğ‘™)
Hpre
ğ‘™
= ğ›¼pre
ğ‘™
Â· tanh(ğœƒpre
ğ‘™
ËœxâŠ¤
ğ‘™) + bpre
ğ‘™
Hpost
ğ‘™
= ğ›¼post
ğ‘™
Â· tanh(ğœƒpost
ğ‘™
ËœxâŠ¤
ğ‘™) + bpost
ğ‘™
Hres
ğ‘™
= ğ›¼res
ğ‘™
Â· tanh(ğœƒres
ğ‘™
ËœxâŠ¤
ğ‘™) + bres
ğ‘™,
(5)
where RMSNorm(Â·) (Zhang and Sennrich, 2019) is applied to the last dimension, and the scalars
ğ›¼pre
ğ‘™
, ğ›¼post
ğ‘™
and ğ›¼res
ğ‘™
âˆˆR are learnable gating factors initialized to small values. The dynamic
5


mappings are derived via linear projections parameterized by ğœƒpre
ğ‘™
, ğœƒpost
ğ‘™
âˆˆR1Ã—ğ¶and ğœƒres
ğ‘™
âˆˆRğ‘›Ã—ğ¶,
while the static mappings are represented by learnable biases bpre
ğ‘™
, bpost
ğ‘™
âˆˆR1Ã—ğ‘›and bres
ğ‘™
âˆˆRğ‘›Ã—ğ‘›.
It is worth noting that the introduction of these mappingsâ€”Hpre
ğ‘™
, Hpost
ğ‘™
, and Hres
ğ‘™
â€”incurs
negligible computational overhead, as the typical expansion rate ğ‘›, e.g. 4, is much smaller than
the input dimension ğ¶. With this design, HC effectively decouples the information capacity
of the residual stream from the layerâ€™s input dimension, which is strongly correlated with the
modelâ€™s computational complexity (FLOPs). Consequently, HC offers a new avenue for scaling
by adjusting the residual stream width, complementing the traditional scaling dimensions of
model FLOPs and training data size discussed in pre-training scaling laws (Hoffmann et al.,
2022).
Although HC necessitates three mappings to manage the dimensional mismatch between
the residual stream and the layer input, preliminary experiments presented in Tab. 1 indicate
that the residual mapping Hres
ğ‘™
yields the most significant performance gain. This finding
underscores the critical importance of effective information exchange within the residual stream.
Table 1 | Ablation Study of HC Components. When a specific mapping (Hpre
ğ‘™
, Hpost
ğ‘™
, or Hres
ğ‘™
) is
disabled, we employ a fixed mapping to maintain dimensional consistency: uniform weights of
1/ğ‘›for Hpre
ğ‘™
, uniform weights of ones for Hpost
ğ‘™
, and the identity matrix for Hres
ğ‘™
.
Hres
ğ‘™
Hpre
ğ‘™
Hpost
ğ‘™
Absolute Loss Gap
0.0
âœ“
âˆ’0.022
âœ“
âœ“
âˆ’0.025
âœ“
âœ“
âœ“
âˆ’0.027
3.1. Numerical Instability
While the residual mapping Hres
ğ‘™
is instrumental for performance, its sequential application
poses a significant risk to numerical stability. As detailed in Eq. (4), when HC is extended across
multiple layers, the effective signal propagation from layer ğ‘™to ğ¿is governed by the composite
mapping Ãğ¿âˆ’ğ‘™
ğ‘–=1 Hres
ğ¿âˆ’ğ‘–. Since the learnable mapping Hres
ğ‘™
is unconstrained, this composite mapping
inevitably deviates from the identity mapping. Consequently, the signal magnitude is prone to
explosion or vanishing during both the forward pass and backpropagation. This phenomenon
undermines the fundamental premise of residual learning, which relies on unimpeded signal
flow, thereby destabilizing the training process in deeper or larger-scale models.
Empirical evidence supports this analysis. We observe unstable loss behavior in large-scale
experiments, as illustrated in Fig. 2. Taking mHC as the baseline, HC exhibits an unexpected
loss surge around the 12k step, which is highly correlated with the instability in the gradient
norm. Furthermore, the analysis on Hres
ğ‘™
validates the mechanism of this instability. To quantify
how the composite mapping Ãğ¿âˆ’ğ‘™
ğ‘–=1 Hres
ğ¿âˆ’ğ‘–amplifies signals along the residual stream, we utilize
two metrics. The first, based on the maximum absolute value of the row sums of the composite
mapping, captures the worst-case expansion in the forward pass. The second, based on the
maximum absolute column sum, corresponds to the backward pass. We refer to these metrics
as the Amax Gain Magnitude of the composite mapping. As shown in Fig. 3 (b), the Amax Gain
Magnitude yields extreme values with peaks of 3000, a stark divergence from 1 that confirms
the presence of exploding residual streams.
6


0
10000
20000
30000
40000
50000
Steps
-0.002
0.000
0.002
0.004
0.006
0.008
0.010
0.012
Absolute Loss Gap
(a) Absolute Training Loss Gap vs.  Training Steps
mHC
HC
0
10000
20000
30000
40000
50000
Steps
0.00
0.05
0.10
0.15
0.20
0.25
Grad Norm
(b) Gradient Norm vs.  Training Steps
mHC
HC
Figure 2 | Training Instability of Hyper-Connections (HC). This figure illustrates (a) the absolute
loss gap of HC relative to mHC, and (b) the comparisons of gradient norms. All results are based
on 27B models.
0
10
20
30
40
50
60
Layer Index l
100
101
Amax Gain Magnitude
(a) Single-Layer Mapping
Hres
l  Forward Signal Gain
Hres
l  Backward Gradient Gain
0
10
20
30
40
50
60
Layer Index l
101
102
103
104
105
Amax Gain Magnitude
(b) Composite Mapping
Y
l
i = 1Hres
l + 1 âˆ’i Forward Signal Gain
Y
61 âˆ’l
i = 1 Hres
61 âˆ’i Backward Gradient Gain
Figure 3 | Propagation Instability of Hyper-Connections (HC). This figure illustrates the
propagation dynamics of (a) the single-layer mapping Hres
ğ‘™
and (b) the composite mapping
Ãğ¿âˆ’ğ‘™
ğ‘–=1 Hres
ğ¿âˆ’ğ‘–within the 27B model. The layer index ğ‘™(x-axis) unrolls each standard Transformer
block into two independent layers (Attention and FFN). The Amax Gain Magnitude (y-axis) is
calculated as the maximum absolute row sum (for the forward signal) and column sum (for the
backward gradient), averaged over all tokens in a selected sequence.
3.2. System Overhead
While the computational complexity of HC remains manageable due to the linearity of the
additional mappings, the system-level overhead prevents a non-negligible challenge. Specifically,
memory access (I/O) costs often constitute one of the primary bottlenecks in modern model
architectures, which is widely referred to as the â€œmemory wallâ€ (Dao et al., 2022). This bottleneck
is frequently overlooked in architectural design, yet it decisively impacts runtime efficiency.
Focusing on the widely adopted pre-norm Transformer (Vaswani et al., 2017) architecture,
we analyze the I/O patterns inherent to HC. Tab. 2 summarizes the per token memory access
overhead in a single residual layer introduced by the ğ‘›-stream residual design. The analysis
reveals that HC increases the memory access cost by a factor approximately proportional to ğ‘›.
This excessive I/O demand significantly degrades training throughput without the mitigation of
fused kernels. Besides, since Hpre
ğ‘™
, Hpost
ğ‘™
, and Hres
ğ‘™
involve learnable parameters, their interme-
diate activations are required for backpropagation. This results in a substantial increase in the
GPU memory footprint, often necessitating gradient checkpointing to maintain feasible memory
usage. Furthermore, HC requires ğ‘›-fold more communication cost in pipeline parallelism (Qi
et al., 2024), leading to larger bubbles and decreasing the training throughput.
7


Table 2 | Comparison of Memory Access Costs Per Token. This analysis accounts for the
overhead introduced by the residual stream maintenance in the forward pass, excluding the
internal I/O of the layer function F .
Method
Operation
Read (Elements)
Write (Elements)
Residual
Connection
Residual Merge
2ğ¶
ğ¶
Total I/O
2C
C
Hyper-
Connections
Calculate Hpre
ğ‘™
, Hpost
ğ‘™
, Hres
ğ‘™
ğ‘›ğ¶
ğ‘›2 + 2ğ‘›
Hpre
ğ‘™
ğ‘›ğ¶+ ğ‘›
ğ¶
Hpost
ğ‘™
ğ¶+ ğ‘›
ğ‘›ğ¶
Hres
ğ‘™
ğ‘›ğ¶+ ğ‘›2
ğ‘›ğ¶
Residual Merge
2ğ‘›ğ¶
ğ‘›ğ¶
Total I/O
(5n + 1)C + n2 + 2n
(3n + 1)C + n2 + 2n
4. Method
4.1. Manifold-Constrained Hyper-Connections
Drawing inspiration from the identity mapping principle (He et al., 2016b), the core premise
of mHC is to constrain the residual mapping Hres
ğ‘™
onto a specific manifold. While the original
identity mapping ensures stability by enforcing Hres
ğ‘™
= I, it fundamentally precludes information
exchange within the residual stream, which is critical for maximizing the potential of multi-
stream architectures. Therefore, we propose projecting the residual mapping onto a manifold
that simultaneously maintains the stability of signal propagation across layers and facilitates
mutual interaction among residual streams to preserve the modelâ€™s expressivity. To this end,
we restrict Hres
ğ‘™
to be a doubly stochastic matrix, which has non-negative entries where both
the rows and columns sum to 1. Formally, let Mres denote the manifold of doubly stochastic
matrices (also known as the Birkhoff polytope). We constrain Hres
ğ‘™
to PMres(Hres
ğ‘™
), defined as:
PMres(Hres
ğ‘™
) â‰”

Hres
ğ‘™
âˆˆRğ‘›Ã—ğ‘›| Hres
ğ‘™
1ğ‘›= 1ğ‘›, 1âŠ¤
ğ‘›Hres
ğ‘™
= 1âŠ¤
ğ‘›, Hres
ğ‘™
â©¾0
	
,
(6)
where 1ğ‘›represents the ğ‘›-dimensional vector of all ones.
It is worth noting that when ğ‘›= 1, the doubly stochastic condition degenerates to the scalar
1, thereby recovering the original identity mapping. The choice of double stochasticity confers
several rigorous theoretical properties beneficial for large-scale model training:
1. Norm Preservation: The spectral norm of a doubly stochastic matrix is bounded by 1
(i.e., âˆ¥Hres
ğ‘™
âˆ¥2 â‰¤1). This implies that the learnable mapping is non-expansive, effectively
mitigating the gradient explosion problem.
2. Compositional Closure: The set of doubly stochastic matrices is closed under matrix
multiplication. This ensures that the composite residual mapping across multiple layers,
Ãğ¿âˆ’ğ‘™
ğ‘–=1 Hres
ğ¿âˆ’ğ‘–, remains doubly stochastic, thereby preserving stability throughout the entire
depth of the model.
3. Geometric Interpretation via the Birkhoff Polytope: The set Mres forms the Birkhoff
polytope, which is the convex hull of the set of permutation matrices. This provides a
clear geometric interpretation: the residual mapping acts as a convex combination of
permutations. Mathematically, the repeated application of such matrices tends to increase
8


the mixing of information across streams monotonically, effectively functioning as a robust
feature fusion mechanism.
Additionally, we impose non-negativity constraints on the input mappings Hpre
ğ‘™
and output
mappings Hpost
ğ‘™
. This constrain prevents signal cancellation arising from the composition of
positive and negative coefficients, which can also be considered as a special manifold projection.
4.2. Parameterization and Manifold Projection
In this section, we detail the calculation process of Hpre
ğ‘™
, Hpost
ğ‘™
, and Hres
ğ‘™
in mHC. Given the
input hidden matrix xğ‘™âˆˆRğ‘›Ã—ğ¶at the ğ‘™-th layer, we first flatten it into a vector Â®xğ‘™= vec(xğ‘™) âˆˆR1Ã—ğ‘›ğ¶
to preserve full context information. Then, we follow the original HC formulation to get the
dynamic mappings and the static mappings as follows:
ï£±ï£´ï£´ï£´ï£´ï£´ï£´ï£²
ï£´ï£´ï£´ï£´ï£´ï£´ï£³
Â®xâ€²
ğ‘™= RMSNorm(Â®xğ‘™)
ËœHpre
ğ‘™
= ğ›¼pre
ğ‘™
Â· (Â®xâ€²
ğ‘™ğœ‘pre
ğ‘™
) + bpre
ğ‘™
ËœHpost
ğ‘™
= ğ›¼post
ğ‘™
Â· (Â®xâ€²
ğ‘™ğœ‘post
ğ‘™
) + bpost
ğ‘™
ËœHres
ğ‘™
= ğ›¼res
ğ‘™
Â· mat(Â®xâ€²
ğ‘™ğœ‘res
ğ‘™) + bres
ğ‘™,
(7)
where ğœ‘pre
ğ‘™
, ğœ‘post
ğ‘™
âˆˆRğ‘›ğ¶Ã—ğ‘›and ğœ‘res
ğ‘™
âˆˆRğ‘›ğ¶Ã—ğ‘›2 are linear projections for dynamic mappings and
mat(Â·) is a reshape function from R1Ã—ğ‘›2 to Rğ‘›Ã—ğ‘›.
Then, the final constrained mappings are obtained via:
ï£±ï£´ï£´ï£´ï£²
ï£´ï£´ï£´ï£³
Hpre
ğ‘™
= ğœ( ËœHpre
ğ‘™
)
Hpost
ğ‘™
= 2ğœ( ËœHpost
ğ‘™
)
Hres
ğ‘™
= Sinkhorn-Knopp( ËœHres
ğ‘™
),
(8)
where ğœ(Â·) denotes the Sigmoid function. The Sinkhorn-Knopp(Â·) operator firstly makes all
elements to be positive via an exponent operator and then conducts iterative normalization
process that alternately rescales rows and columns to sum to 1. Specifically, given a positive
matrix M(0) = exp( ËœHres
ğ‘™
) as the start point, the normalization iteration proceeds as:
M(ğ‘¡) = Tğ‘Ÿ

Tğ‘(M(ğ‘¡âˆ’1))

,
(9)
where Tğ‘Ÿand Tğ‘denote row and column normalization, respectively. This process converges to a
doubly stochastic matrix Hres
ğ‘™
= M(ğ‘¡max) as ğ‘¡max â†’âˆ. We choose ğ‘¡max = 20 as a practical value in
our experiments.
4.3. Efficient Infrastructure Design
In this section, we detail the infrastructure design tailored for mHC. Through rigorous optimiza-
tion, we implement mHC (with ğ‘›= 4) in large-scale models with a marginal training overhead
of only 6.7%.
4.3.1. Kernel Fusion
Observing that RMSNorm in mHC imposes significant latency when operating on the high-
dimensional hidden state Â®xğ‘™âˆˆR1Ã—ğ‘›ğ¶, we reorder the dividing-by-norm operation to follow the
9


matrix multiplication. This optimization maintains mathematical equivalence while improving
efficiency. Furthermore, we employ mixed-precision strategies to maximize numerical accuracy
without compromising speed, and fuse multiple operations with shared memory access into
unified compute kernels to reduce memory bandwidth bottlenecks. Based on the inputs and
parameters detailed in Eq. (10) to (13), we implement three specialized mHC kernels to compute
Hpre
ğ‘™
, Hpost
ğ‘™
, and Hres
ğ‘™
. In these kernels, the biases and linear projections are consolidated into bğ‘™
and ğœ‘ğ‘™, and the RMSNorm weight is also absorbed in ğœ‘ğ‘™.
â€¢ Eq. (14) to (15): We develop a unified kernel that fuses two scans on Â®xğ‘™, leveraging ma-
trix multiplication units to maximize memory bandwidth utilization. The backward
passâ€”comprising two matrix multiplicationsâ€”is similarly consolidated into a single ker-
nel, eliminating redundant reloading of Â®xğ‘™. Both kernels feature a finely tuned pipeline
(load, cast, compute, store) to efficiently handle mixed-precision processing.
â€¢ Eq. (16) to (18): These lightweight operations on small coefficients are opportunistically
fused into a single kernel, significantly reducing kernel launch overhead.
â€¢ Eq. (19): We implement the Sinkhorn-Knopp iteration within a single kernel. For the
backward pass, we derive a custom backward kernel that recomputes the intermediate
results on-chip and traverses the entire iteration.
ğœ‘ğ‘™: tfloat32
[ğ‘›ğ¶, ğ‘›2 + 2ğ‘›]
(10)
Â®xğ‘™: bfloat16
[1, ğ‘›ğ¶]
(11)
ğ›¼pre
ğ‘™
, ğ›¼post
ğ‘™
, ğ›¼res
ğ‘™
: float32
Scalars
(12)
bğ‘™: float32
[1, ğ‘›2 + 2ğ‘›]
(13)
h ËœËœHpre
ğ‘™
, ËœËœHpost
ğ‘™
, ËœËœHres
ğ‘™
i
: float32
= Â®xğ‘™ğœ‘ğ‘™
(14)
ğ‘Ÿ: float32
=
Â®xğ‘™

2 /
âˆš
ğ‘›ğ¶
(15)
h
ËœHpre
ğ‘™
, ËœHpost
ğ‘™
, ËœHres
ğ‘™
i
: float32
= 1/ğ‘Ÿ
h
ğ›¼pre
ğ‘™
ËœËœHpre
ğ‘™
, ğ›¼post
ğ‘™
ËœËœHpost
ğ‘™
, ğ›¼res
ğ‘™
ËœËœHres
ğ‘™
i
+ bğ‘™
(16)
Hpre
ğ‘™
: float32
= ğœ

ËœHpre
ğ‘™

(17)
Hpost
ğ‘™
: float32
= 2ğœ

ËœHpost
ğ‘™

(18)
Hres
ğ‘™
: float32
= Sinkhorn-Knopp  ËœHres
ğ‘™

(19)
Using the coefficients derived from the aforementioned kernels, we introduce two addi-
tional kernels to apply these mappings: one for Fpre â‰”Hpre
ğ‘™
xğ‘™and another for Fpost,res â‰”
Hres
ğ‘™
xğ‘™+ Hpost âŠ¤
ğ‘™
F (Â·, Â·). Through fusing the application of Hpost
ğ‘™
and Hres
ğ‘™
with residual merging,
we reduce the number of elements read from (3ğ‘›+ 1)ğ¶to (ğ‘›+ 1)ğ¶and the number of elements
written from 3ğ‘›ğ¶to ğ‘›ğ¶for this kernel. We efficiently implement the majority of kernels (ex-
cluding Eq. (14) to (15)) using TileLang (Wang et al., 2025). This framework streamlines the
implementation of kernels with complex calculation process and allows us to fully utilize the
memory bandwidth with minimal engineering effort.
4.3.2. Recomputing
The ğ‘›-stream residual design introduces substantial memory overhead during training. To
mitigate this, we discard the intermediate activations of the mHC kernels after the forward pass
and recompute them on-the-fly in the backward pass, through re-executing the mHC kernels
10


without the heavy layer function F . Consequently, for a block of ğ¿ğ‘Ÿconsecutive layers, we need
only store the input xğ‘™0 to the first layer. Excluding lightweight coefficients while accounting
for the pre-norm with in F , Tab. 3 summarizes the intermediate activations preserved for the
backward pass.
Table 3 | Stored and Recomputed Intermediate Activations We list per token activation pre-
served for the backward pass and the transient activation recomputed in ğ¿ğ‘Ÿconsecutive layers.
Layer ğ‘™0 represents the first layer in ğ¿ğ‘Ÿlayers and layer ğ‘™is in [ğ‘™0, ğ‘™0 + ğ¿ğ‘Ÿâˆ’1].
Activations
xğ‘™0
F (Hpre
ğ‘™
xğ‘™, Wğ‘™)
xğ‘™
Hpre
ğ‘™
xğ‘™
RMSNorm(Hpre
ğ‘™
xğ‘™)
Size (Elements)
ğ‘›ğ¶
ğ¶
ğ‘›ğ¶
ğ¶
ğ¶
Stored Method
Every ğ¿ğ‘Ÿlayers
Every layer
Transient inside ğ¿ğ‘Ÿlayers
Since mHC kernels recomputation is performed for blocks of ğ¿ğ‘Ÿconsecutive layers, given
a total of ğ¿layers, we must persistently store the first layer input xğ‘™0 for all âŒˆğ¿
ğ¿ğ‘ŸâŒ‰blocks for the
backward pass. In addition to this resident memory, the recomputation process introduces a
transient memory overhead of (ğ‘›+ 2)ğ¶Ã— ğ¿ğ‘Ÿelements for the active block, which determines the
peak memory usage during backpropagation. Consequently, we determine the optimal block
size ğ¿âˆ—
ğ‘Ÿby minimizing the total memory footprint corresponded to ğ¿ğ‘Ÿ:
ğ¿âˆ—
ğ‘Ÿ= arg min
ğ¿ğ‘Ÿ

ğ‘›ğ¶Ã—
 ğ¿
ğ¿ğ‘Ÿ

+ (ğ‘›+ 2)ğ¶Ã— ğ¿ğ‘Ÿ

â‰ˆ
âˆšï¸‚
ğ‘›ğ¿
ğ‘›+ 2.
(20)
Furthermore, pipeline parallelism in large-scale training imposes a constraint: recomputation
blocks must not cross pipeline stage boundaries. Observing that the theoretical optimum ğ¿âˆ—
ğ‘Ÿ
typically aligns with the number of layers per pipeline stage, we choose to synchronize the
recomputation boundaries with the pipeline stages.
4.3.3. Overlapping Communication in DualPipe
In large-scale training, pipeline parallelism is the standard practice for mitigating parameter and
gradient memory footprints. Specifically, we adopt the DualPipe schedule (Liu et al., 2024b),
which effectively overlaps scale-out interconnected communication traffic, such as those in
expert and pipeline parallelism. However, compared to the single-stream design, the proposed
ğ‘›-stream residual in mHC incurs substantial communication latency across pipeline stages.
Furthermore, at stage boundaries, the recomputation of mHC kernels for all ğ¿ğ‘Ÿlayers introduces
non-negligible computational overhead. To address these bottlenecks, we extend the DualPipe
schedule (see Fig. 4) to facilitate improved overlapping of communication and computation at
pipeline stage boundaries.
Notably, to prevent blocking the communication stream, we execute the Fpost,res kernels
of MLP (i.e. FFN) layers on a dedicated high-priority compute stream. We further refrain
from employing persistent kernels for long-running operations in attention layers, thereby
preventing extended stalls. This design enables the preemption of overlapped attention com-
putations, allowing for flexible scheduling while maintaining high utilization of the compute
deviceâ€™s processing units. Furthermore, the recomputation process is decoupled from pipeline
communication dependencies, as the initial activation of each stage xğ‘™0 is already cached locally.
11


MLP (B)
DISPATCH (B)
DISPATCH (F)
MLP (W)
MLP (F)
ATTN (B)
COMBINE (F)
COMBINE (B)
PP Send Recv (F)
PP Send Recv (B)
â„±!"#$, '(#
)
(F)
â„±!"#$, '(#
)
(B)
â„±!"#$, '(#
*
(B)
â„±!'(
) (B)
ATTN (W)
ATTN (F)
Whole Stage 
Recompute (B)
â„±!'(
* (B)
â„±!'(
* (F)
â„±!"#$, '(#
*
(F)
â„±!'(
) (F)
Normal Compute Stream
Communication Stream
High Priority Compute Stream
Figure 4 | Communication-Computation Overlapping for mHC. We extend the DualPipe
schedule to handle the overhead introduced by mHC. Lengths of each block are illustrative only
and do not represent actual duration. (F), (B), (W) refers to forward pass, backward pass, weight
gradient computation, respectively. F A and F M represents kernels corresponded to Attention
and MLP, respectively.
5. Experiments
5.1. Experimental Setup
We validate the proposed method via language model pre-training, conducting a comparative
analysis between the baseline, HC, and our proposed mHC. Utilizing MoE architectures inspired
by DeepSeek-V3 (Liu et al., 2024b), we train four distinct model variants to cover different
evaluation regimes. Specifically, the expansion rate ğ‘›for both HC and mHC is set to 4. Our
primary focus is a 27B model trained with a dataset size proportional to its parameters, which
serves as the subject for our system-level main results. Expanding on this, we analyze the
compute scaling behavior by incorporating smaller 3B and 9B models trained with proportional
data, which allows us to observe performance trends across varying compute. Additionally,
to specifically investigate the token scaling behavior, we train a separate 3B model on a fixed
corpus of 1 trillion tokens. Detailed model configurations and training hyper-parameters are
provided in Appendix A.1.
5.2. Main Results
10000
20000
30000
40000
50000
Steps
-0.06
-0.04
-0.02
0.00
Absolute Loss Gap
(a) Absolute Training Loss Gap vs.  Training Steps
Baseline
HC
mHC
10000
20000
30000
40000
50000
Steps
0.00
0.05
0.10
0.15
0.20
Grad Norm
(b) Gradient Norm vs.  Training Steps
Baseline
HC
mHC
Figure 5 | Training Stability of Manifold-Constrained Hyper-Connections (mHC). This figure
illustrates (a) the absolute training loss gap of mHC and HC relative to the baseline, and (b)
the gradient norm of the three methods. All experiments utilize the 27B model. The results
demonstrate that mHC exhibits improved stability in terms of both loss and gradient norm.
We begin by examining the training stability and convergence of the 27B models. As
illustrated in Fig. 5 (a), mHC effectively mitigates the training instability observed in HC,
achieving a final loss reduction of 0.021 compared to the baseline. This improved stability is
further corroborated by the gradient norm analysis in Fig. 5 (b), where mHC exhibits significantly
better behavior than HC, maintaining a stable profile comparable to the baseline.
12


Table 4 | System-level Benchmark Results for 27B Models. This table compares the zero-
shot and few-shot performance of the Baseline, HC, and mHC across 8 diverse downstream
benchmarks. mHC consistently outperforms the Baseline and surpasses HC on the majority of
benchmarks, demonstrating its effectiveness in large-scale pre-training.
Benchmark
BBH
DROP
GSM8K
HellaSwag
MATH
MMLU
PIQA
TriviaQA
(Metric)
(EM)
(F1)
(EM)
(Acc.)
(EM)
(Acc.)
(Acc.)
(EM)
# Shots
3-shot
3-shot
8-shot
10-shot
4-shot
5-shot
0-shot
5-shot
27B Baseline
43.8
47.0
46.7
73.7
22.0
59.0
78.5
54.3
27B w/ HC
48.9
51.6
53.2
74.3
26.4
63.0
79.9
56.3
27B w/ mHC
51.0
53.9
53.8
74.7
26.0
63.4
80.5
57.6
Tab. 4 presents the downstream performance across a diverse set of benchmarks (Bisk et al.,
2020; Cobbe et al., 2021; Hendrycks et al., 2020, 2021; Joshi et al., 2017; Zellers et al., 2019). mHC
yields comprehensive improvements, consistently outperforming the baseline and surpassing
HC on the majority of tasks. Notably, compared to HC, mHC further enhances the modelâ€™s
reasoning capabilities, delivering performance gains of 2.1% on BBH (Suzgun et al., 2022) and
2.3% on DROP (Dua et al., 2019).
5.3. Scaling Experiments
1021
1022
FLOPs
-0.04
-0.03
-0.02
-0.01
0.00
0.01
0.02
Absolute Loss Gap
Baseline
mHC
1021
1022
FLOPs
98.0%
99.0%
100.0%
101.0%
Relative Loss Ratio
Baseline
mHC
2
4
FLOPs
Ã—1021
-0.03
-0.02
-0.01
0.00
0.01
Absolute Loss Gap
Baseline
mHC
2
4
FLOPs
Ã—1021
98.0%
99.0%
100.0%
101.0%
Relative Loss Ratio
Baseline
mHC
(a) Compute Scaling Curve
(b) Token Scaling Curve
Figure 6 | Scaling properties of mHC compared to the Baseline. (a) Compute Scaling Curve.
Solid lines depict the performance gap across different compute budgets. Each point represents
a specific compute-optimal configuration of model size and dataset size, scaling from 3B and 9B
to 27B parameters. (b) Token Scaling Curve. Trajectory of the 3B model during training. Each
point represents the modelâ€™s performance at different training tokens. Detailed architectures
and training configurations are provided in Appendix A.1.
To assess the scalability of our approach, we report the relative loss improvement of mHC
against the baseline across different scales. In Fig. 6 (a), we plot the compute scaling curve
spanning 3B, 9B, and 27B parameters. The trajectory indicates that the performance advantage is
robustly maintained even at higher computational budgets, showing only marginal attenuation.
Furthermore, we examine the within-run dynamics in Fig. 6 (b), which presents the token
scaling curve for the 3B model. Collectively, these findings validate the effectiveness of mHC
in large-scale scenarios. This conclusion is further corroborated by our in-house large-scale
training experiments.
13


0
10
20
30
40
50
60
Layer Index l
0.0
0.5
1.0
1.5
2.0
Amax Gain Magnitude
(a) Single-Layer Mapping
PMres(Hres
l ) Forward Signal Gain
PMres(Hres
l ) Backward Gradient Gain
0
10
20
30
40
50
60
Layer Index l
0.0
0.5
1.0
1.5
2.0
Amax Gain Magnitude
(b) Composite Mapping
Y
l
i = 1PMres(Hres
l + 1 âˆ’i) Forward Signal Gain
Y
61 âˆ’l
i = 1 PMres(Hres
61 âˆ’i) Backward Gradient Gain
Figure 7 | Propagation Stability of Manifold-Constrained Hyper-Connections (mHC). This
figure illustrates the propagation dynamics of (a) the single-layer mapping PMres(Hres
ğ‘™
) and (b)
the composite mapping Ãğ¿âˆ’ğ‘™
ğ‘–=1 PMres(Hres
ğ¿âˆ’ğ‘–) within the 27B model. The results demonstrate that
mHC significantly enhances propagation stability compared to HC.
-6.81 -6.81 -6.81 -6.81
18.73
-15.29
-14.79
-15.88
5.43
4.43
4.43
4.43
-4.07 -3.07 -4.07 -4.07
-3.95 -3.95 -2.95 -3.95
-4.22 -4.22 -4.22 -3.22
HC
Hres
1
0.83
0.73
0.66
0.75
0.84
0.67
0.49
0.96
0.94 -0.07 -0.05 0.02
-0.08 0.89 -0.07 -0.07
-0.10 -0.14 0.81 -0.07
0.06
0.05 -0.03 0.87
Hres
30
-11.97 -6.86 -10.23-11.89
-21.64
-20.22
22.50
-21.59
-5.58 -3.74 -5.71 -6.60
-6.06 -2.27 -5.33 -6.57
6.08
3.12
6.53
6.77
-6.41 -3.97 -5.72 -5.49
Hres
60
1.22
1.04
1.06
1.02
-1.35
6.47
0.03
-0.81
-0.38 -0.33 -0.34 -0.31
1.81
1.56
1.58
1.51
0.01 -0.00 0.01
0.01
-0.23 -0.19 -0.20 -0.19
Y
30
i = 1Hres
31 âˆ’i
-135.4-133.4-489.0273.3
-251.4
-243.0
264.6
-254.8
-69.9 -68.3 -255.3142.1
-69.1 -66.1 -247.4139.6
74.8
72.7 268.9-151.8
-71.2 -71.8 -255.2143.3
Y
30
i = 1Hres
61 âˆ’i
-259.2-219.1-228.2-221.0
-475.3
-462.8
509.1
-498.5
-132.8-112.2-117.0-113.3
-129.3-109.3-113.9-110.3
142.3 120.2 125.3 121.3
-139.3-117.8-122.6-118.7
Y
60
i = 1Hres
61 âˆ’i
0.98
1.00
0.98
1.04
1.00
1.00
1.00
1.00
0.67
0.09
0.03
0.22
0.26
0.48
0.26
0.00
0.03
0.24
0.00
0.73
0.03
0.20
0.69
0.09
mHC
PMres(Hres
1 )
0.96
1.02
1.04
0.99
1.00
1.00
1.00
1.00
0.96
0.01
0.00
0.04
0.00
0.97
0.03
0.00
0.00
0.00
1.00
0.00
0.00
0.04
0.01
0.95
PMres(Hres
30 )
1.00
1.01
0.99
1.00
1.00
1.00
1.00
1.00
0.92
0.06
0.01
0.01
0.05
0.81
0.01
0.13
0.00
0.01
0.97
0.02
0.03
0.13
0.00
0.84
PMres(Hres
60 )
0.90
1.06
0.93
1.11
1.00
1.00
1.00
1.00
0.30
0.25
0.22
0.24
0.24
0.25
0.25
0.26
0.20
0.24
0.28
0.28
0.17
0.32
0.18
0.34
Y
30
i = 1PMres(Hres
31 âˆ’i)
0.41
1.50
1.50
0.60
1.00
1.00
1.00
1.00
0.35
0.28
0.17
0.20
0.03
0.62
0.29
0.07
0.01
0.17
0.80
0.02
0.02
0.42
0.25
0.31
Y
30
i = 1PMres(Hres
61 âˆ’i)
0.88
1.03
1.00
1.11
1.00
1.01
1.01
1.00
0.24
0.26
0.23
0.27
0.23
0.25
0.26
0.27
0.21
0.25
0.27
0.28
0.21
0.27
0.24
0.29
Y
60
i = 1PMres(Hres
61 âˆ’i)
Figure 8 | Visualizations of Learnable Mappings. This figure displays representative single-
layer and composite mappings for HC (first row) and mHC (second row). Each matrix is
computed by averaging over all tokens within a selected sequence. The labels annotated along
the y-axis and x-axis indicate the forward signal gain (row sum) and the backward gradient gain
(column sum), respectively.
5.4. Stability Analysis
Similar to Fig. 3, Fig. 7 illustrates the propagation stability of mHC. Ideally, the single-layer
mapping satisfies the doubly stochastic constraint, implying that both the forward signal gain
and the backward gradient gain should equal to 1. However, practice implementations utilizing
the Sinkhorn-Knopp algorithm must limit the number of iterations to achieve computational
efficiency. In our settings, we use 20 iterations to obtain an approximate solution. Consequently,
as shown in Fig. 7(a), the backward gradient gain deviates slightly from 1. In the composite case
shown in Fig. 7(b), the deviation increases but remains bounded, reaching a maximum value
of approximately 1.6. Notably, compared to the maximum gain magnitude of nearly 3000 in
HC, mHC significantly reduces it by three orders of magnitude. These results demonstrate that
mHC significantly enhances propagation stability compared to HC, ensuring stable forward
signal and backward gradient flows. Additionally, Fig. 8 displays representative mappings. We
observe that for HC, when the maximum gain is large, other values also tend to be significant,
which indicates general instability across all propagation paths. In contrast, mHC consistently
yields stable results.
14


6. Conclusion and Outlook
In this paper, we identify that while expanding the width of residual stream and diversifying
connections yields performance gains as proposed in Hyper-Connections (HC), the uncon-
strained nature of these connections leads to signal divergence. This disruption compromises
the conservation of signal energy across layers, inducing training instability and hindering the
scalability of deep networks. To address these challenges, we introduce Manifold-Constrained
Hyper-Connections (mHC), a generalized framework that projects the residual connection space
onto a specific manifold. By employing the Sinkhorn-Knopp algorithm to enforce a doubly
stochastic constraint on residual mappings, mHC transforms signal propagation into a convex
combination of features. Empirical results confirm that mHC effectively restores the identity
mapping property, enabling stable large-scale training with superior scalability compared to
conventional HC. Crucially, through efficient infrastructure-level optimizations, mHC delivers
these improvements with negligible computational overhead.
As a generalized extension of the HC paradigm, mHC opens several promising avenues for
future research. Although this work utilizes doubly stochastic matrices to ensure stability, the
framework accommodates the exploration of diverse manifold constraints tailored to specific
learning objectives. We anticipate that further investigation into distinct geometric constraints
could yield novel methods that better optimize the trade-off between plasticity and stability.
Furthermore, we hope mHC rejuvenates community interest in macro-architecture design.
By deepening the understanding of how topological structures influence optimization and
representation learning, mHC will help address current limitations and potentially illuminate
new pathways for the evolution of next-generation foundational architectures.
References
J. Ainslie, J. Lee-Thorp, M. De Jong, Y. Zemlyanskiy, F. LebrÃ³n, and S. Sanghai. Gqa: Training
generalized multi-query transformer models from multi-head checkpoints. arXiv preprint
arXiv:2305.13245, 2023.
Y. Bisk, R. Zellers, R. L. Bras, J. Gao, and Y. Choi. PIQA: reasoning about physical commonsense
in natural language. In The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI
2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI
2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI
2020, New York, NY, USA, February 7-12, 2020, pages 7432â€“7439. AAAI Press, 2020. doi:
10.1609/aaai.v34i05.6239. URL https://doi.org/10.1609/aaai.v34i05.6239.
T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam,
G. Sastry, A. Askell, et al. Language models are few-shot learners. Advances in neural
information processing systems, 33:1877â€“1901, 2020.
Y. Chai, S. Jin, and X. Hou. Highway transformer: Self-gating enhanced self-attentive networks.
In D. Jurafsky, J. Chai, N. Schluter, and J. Tetreault, editors, Proceedings of the 58th Annual
Meeting of the Association for Computational Linguistics, pages 6887â€“6900, Online, July
2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.616. URL
https://aclanthology.org/2020.acl-main.616/.
F. Chollet. Xception: Deep learning with depthwise separable convolutions. In Proceedings of
the IEEE conference on computer vision and pattern recognition, pages 1251â€“1258, 2017.
15


K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek,
J. Hilton, R. Nakano, et al. Training verifiers to solve math word problems. arXiv preprint
arXiv:2110.14168, 2021.
T. Dao, D. Y. Fu, S. Ermon, A. Rudra, and C. RÃ©. FlashAttention: Fast and memory-efficient
exact attention with IO-awareness. In Advances in Neural Information Processing Systems
(NeurIPS), 2022.
D. Dua, Y. Wang, P. Dasigi, G. Stanovsky, S. Singh, and M. Gardner. DROP: A reading compre-
hension benchmark requiring discrete reasoning over paragraphs. In J. Burstein, C. Doran, and
T. Solorio, editors, Proceedings of the 2019 Conference of the North American Chapter of the
Association for Computational Linguistics: Human Language Technologies, NAACL-HLT
2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pages 2368â€“
2378. Association for Computational Linguistics, 2019. doi: 10.18653/V1/N19-1246. URL
https://doi.org/10.18653/v1/n19-1246.
Y. Fang, Y. CAI, J. Chen, J. Zhao, G. Tian, and G. Li. Cross-layer retrospective retrieving via layer
attention. In The Eleventh International Conference on Learning Representations, 2023. URL
https://openreview.net/forum?id=pvgEL1yS3Ql.
W. Fedus, B. Zoph, and N. Shazeer. Switch transformers: Scaling to trillion parameter models
with simple and efficient sparsity. Journal of Machine Learning Research, 23(120):1â€“39, 2022.
K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proceedings
of the IEEE conference on computer vision and pattern recognition, pages 770â€“778, 2016a.
K. He, X. Zhang, S. Ren, and J. Sun. Identity mappings in deep residual networks. In European
conference on computer vision, pages 630â€“645. Springer, 2016b.
M. Heddes, A. Javanmard, K. Axiotis, G. Fu, M. Bateni, and V. Mirrokni. Deepcrossattention:
Supercharging transformer residual connections. In Forty-second International Conference
on Machine Learning, 2025. URL https://openreview.net/forum?id=j3JBfFnGYh.
D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Measuring
massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020.
D. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang, D. Song, and J. Steinhardt. Mea-
suring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874,
2021.
J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. de Las Casas,
L. A. Hendricks, J. Welbl, A. Clark, T. Hennigan, E. Noland, K. Millican, G. van den Driessche,
B. Damoc, A. Guy, S. Osindero, K. Simonyan, E. Elsen, O. Vinyals, J. Rae, and L. Sifre.
An empirical analysis of compute-optimal large language model training. In S. Koyejo,
S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural
Information Processing Systems, volume 35, pages 30016â€“30030. Curran Associates, Inc., 2022.
URL https://proceedings.neurips.cc/paper_files/paper/2022/file/c1e2faf
f6f588870935f114ebe04a3e5-Paper-Conference.pdf.
G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger. Densely connected convolutional
networks. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pages 4700â€“4708, 2017.
16


M. Joshi, E. Choi, D. Weld, and L. Zettlemoyer. TriviaQA: A large scale distantly supervised chal-
lenge dataset for reading comprehension. In R. Barzilay and M.-Y. Kan, editors, Proceedings of
the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long
Papers), pages 1601â€“1611, Vancouver, Canada, July 2017. Association for Computational
Linguistics. doi: 10.18653/v1/P17-1147. URL https://aclanthology.org/P17-1147.
G. Larsson, M. Maire, and G. Shakhnarovich. Fractalnet: Ultra-deep neural networks without
residuals. arXiv preprint arXiv:1605.07648, 2016.
D. Lepikhin, H. Lee, Y. Xu, D. Chen, O. Firat, Y. Huang, M. Krikun, N. Shazeer, and Z. Chen.
Gshard: Scaling giant models with conditional computation and automatic sharding. arXiv
preprint arXiv:2006.16668, 2020.
A. Liu, B. Feng, B. Wang, B. Wang, B. Liu, C. Zhao, C. Dengr, C. Ruan, D. Dai, D. Guo, et al.
Deepseek-v2: A strong, economical, and efficient mixture-of-experts language model. arXiv
preprint arXiv:2405.04434, 2024a.
A. Liu, B. Feng, B. Xue, B. Wang, B. Wu, C. Lu, C. Zhao, C. Deng, C. Zhang, C. Ruan, et al.
Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024b.
I. Loshchilov and F. Hutter.
Decoupled weight decay regularization.
arXiv preprint
arXiv:1711.05101, 2017.
B. Mak and J. Flanigan. Residual matrix transformers: Scaling the size of the residual stream.
arXiv preprint arXiv:2506.22696, 2025.
G. Menghani, R. Kumar, and S. Kumar.
LAurel: Learned augmented residual layer.
In
Forty-second International Conference on Machine Learning, 2025. URL https://open
review.net/forum?id=rUDRWP9WvZ.
M. Pagliardini, A. Mohtashami, F. Fleuret, and M. Jaggi. Denseformer: Enhancing information
flow in transformers via depth weighted averaging. In The Thirty-eighth Annual Conference
on Neural Information Processing Systems, 2024. URL https://openreview.net/forum
?id=kMnoh7CXrq.
P. Qi, X. Wan, G. Huang, and M. Lin. Zero bubble (almost) pipeline parallelism. In The Twelfth
International Conference on Learning Representations, 2024. URL https://openreview
.net/forum?id=tuzTN0eIO5.
N. Shazeer.
Fast transformer decoding: One write-head is all you need.
arXiv preprint
arXiv:1911.02150, 2019.
N. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. Le, G. Hinton, and J. Dean.
Outra-
geously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint
arXiv:1701.06538, 2017.
R. Sinkhorn and P. Knopp. Concerning nonnegative matrices and doubly stochastic matrices.
Pacific Journal of Mathematics, 21(2):343â€“348, 1967.
R. K. Srivastava, K. Greff, and J. Schmidhuber. Training very deep networks. In C. Cortes,
N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information
Processing Systems, volume 28. Curran Associates, Inc., 2015. URL https://proceedings.
neurips.cc/paper_files/paper/2015/file/215a71a12769b056c3c32e7299f1c5e
d-Paper.pdf.
17


J. Su, M. Ahmed, Y. Lu, S. Pan, W. Bo, and Y. Liu. Roformer: Enhanced transformer with rotary
position embedding. Neurocomputing, 568:127063, 2024.
M. Suzgun, N. Scales, N. SchÃ¤rli, S. Gehrmann, Y. Tay, H. W. Chung, A. Chowdhery, Q. V. Le,
E. H. Chi, D. Zhou, et al. Challenging big-bench tasks and whether chain-of-thought can solve
them. arXiv preprint arXiv:2210.09261, 2022.
H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. RoziÃ¨re, N. Goyal,
E. Hambro, F. Azhar, et al. Llama: Open and efficient foundation language models. arXiv
preprint arXiv:2302.13971, 2023.
A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Å. Kaiser, and I. Polo-
sukhin. Attention is all you need. Advances in neural information processing systems, 30,
2017.
L. Wang, H. Gao, C. Zhao, X. Sun, and D. Dai. Auxiliary-loss-free load balancing strategy for
mixture-of-experts. arXiv preprint arXiv:2408.15664, 2024.
L. Wang, Y. Cheng, Y. Shi, Z. Tang, Z. Mo, W. Xie, L. Ma, Y. Xia, J. Xue, F. Yang, et al. Tilelang: A
composable tiled programming model for ai systems. arXiv preprint arXiv:2504.17577, 2025.
D. Xiao, Q. Meng, S. Li, and X. Yuan. Muddformer: Breaking residual bottlenecks in transformers
via multiway dynamic dense connections. arXiv preprint arXiv:2502.12170, 2025.
S. Xie, R. Girshick, P. DollÃ¡r, Z. Tu, and K. He. Aggregated residual transformations for deep
neural networks. In Proceedings of the IEEE conference on computer vision and pattern
recognition, pages 1492â€“1500, 2017.
S. Xie, H. Zhang, J. Guo, X. Tan, J. Bian, H. H. Awadalla, A. Menezes, T. Qin, and R. Yan. Residual:
Transformer with dual residual connections, 2023. URL https://arxiv.org/abs/2304.1
4802.
F. Yu, D. Wang, E. Shelhamer, and T. Darrell. Deep layer aggregation. In Proceedings of the
IEEE conference on computer vision and pattern recognition, pages 2403â€“2412, 2018.
R. Zellers, A. Holtzman, Y. Bisk, A. Farhadi, and Y. Choi. HellaSwag: Can a machine really finish
your sentence? In A. Korhonen, D. R. Traum, and L. MÃ rquez, editors, Proceedings of the 57th
Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July
28- August 2, 2019, Volume 1: Long Papers, pages 4791â€“4800. Association for Computational
Linguistics, 2019. doi: 10.18653/v1/p19-1472. URL https://doi.org/10.18653/v1/p1
9-1472.
B. Zhang and R. Sennrich.
Root mean square layer normalization.
Advances in neural
information processing systems, 32, 2019.
D. Zhu, H. Huang, Z. Huang, Y. Zeng, Y. Mao, B. Wu, Q. Min, and X. Zhou. Hyper-connections.
arXiv preprint arXiv:2409.19606, 2024.
18


A. Appendix
A.1. Detailed Model Specifications and Hyper-parameters.
Table 5 | Detailed Model Specifications and Hyper-parameters. This table presents the architec-
tural configurations for the 3B, 9B, and 27B models based on the DeepSeek-V3 (Liu et al., 2024b)
architecture. It outlines the specific hyper-parameters for mHC and HC, including the residual
stream expansion and Sinkhorn-Knopp settings, alongside the optimization and training proto-
cols used in the experiments.
Attribute
3B
9B
27B
3B
1T Tokens
Vocab Params
331M
496M
662M
331M
Active Params
612M
1.66B
4.14B
612M
Total Params
2.97B
9.18B
27.0B
2.97B
Layers
12
18
30
12
Leading Dense Layers
1
1
Routed Experts
64
64
72
64
Active Experts
6
6
Shared Experts
2
2
Dimension
1280
1920
2560
1280
FFN Dimension
896
1280
1536
896
Load Balancing Method
Loss-Free (Wang et al., 2024)
Loss-Free
Attention Heads
16
24
32
16
Attention Dimension
128
128
Attention Variant
MLA (Liu et al., 2024a)
MLA
KV Rank
512
512
Position Embedding
RoPE (Su et al., 2024)
RoPE
RoPE Dimension
64
64
RoPE ğœƒ
10000
10000
Layer Norm Type
RMSNorm (Zhang and Sennrich, 2019)
RMSNorm
Layer Norm ğœ€
1e-20
1e-20
mHC/HC Expansion Rate ğ‘›
4
4
mHC/HC Gating Factor Init ğ›¼
0.01
0.01
mHC Sinkhorn-Knopp ğ‘¡max
20
20
Sequence Length
4096
4096
Vocab Size
129280
129280
Batch Size
320
512
1280
2560
Training Steps
30000
50000
50000
100000
Training Tokens
39.3B
105B
262B
1.05T
Warmup Steps
2000
2000
Optimizer
AdamW (Loshchilov and Hutter, 2017)
AdamW
AdamW Betas
(0.9, 0.95)
(0.9, 0.95)
AdamW ğœ€
1e-20
1e-20
Base Learning Rate
8.6e-4
5.9e-4
4.0e-4
9.0e-4
Lr Scheduler
Step
Step
Lr Decay Step Ratio
[0.8 Ã—, 0.9 Ã—]
[0.8 Ã—, 0.9 Ã—]
Lr Decay Rate
[0.316, 0.1]
[0.316, 0.1]
Weight Decay
0.1
0.1
19
