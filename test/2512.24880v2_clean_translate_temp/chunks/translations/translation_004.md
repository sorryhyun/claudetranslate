## Translation Output

### Translated Text

**# Shots** 3-shot 3-shot 8-shot 10-shot 4-shot 5-shot 0-shot 5-shot

í‘œ 4ëŠ” ë‹¤ì–‘í•œ ë²¤ì¹˜ë§ˆí¬ ì„¸íŠ¸ì—ì„œì˜ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ì„±ëŠ¥ì„ ì œì‹œí•œë‹¤ (Bisk et al., 2020; Cobbe et al., 2021; Hendrycks et al., 2020, 2021; Joshi et al., 2017; Zellers et al., 2019). _m_ HCëŠ” í¬ê´„ì ì¸ ê°œì„ ì„ ë‹¬ì„±í•˜ì—¬ ì¼ê´€ë˜ê²Œ ê¸°ì¤€ì„ ì„ ëŠ¥ê°€í•˜ê³  ëŒ€ë¶€ë¶„ì˜ ì‘ì—…ì—ì„œ HCë¥¼ ì´ˆê³¼í•œë‹¤. íŠ¹íˆ, HCì™€ ë¹„êµí•˜ì—¬ _m_ HCëŠ” ëª¨ë¸ì˜ ì¶”ë¡  ëŠ¥ë ¥ì„ ë”ìš± í–¥ìƒì‹œì¼œ BBH (Suzgun et al., 2022)ì—ì„œ 2.1%, DROP (Dua et al., 2019)ì—ì„œ 2.3%ì˜ ì„±ëŠ¥ í–¥ìƒì„ ì œê³µí•œë‹¤.

**5.3. í™•ì¥ì„± ì‹¤í—˜**

ê·¸ë¦¼ 6 | **ê¸°ì¤€ì„ ê³¼ ë¹„êµí•œ** _**m**_ **HCì˜ í™•ì¥ íŠ¹ì„±. (a) ì—°ì‚° í™•ì¥ì„± ê³¡ì„ .** ì‹¤ì„ ì€ ë‹¤ì–‘í•œ ì—°ì‚° ì˜ˆì‚°ì— ê±¸ì¹œ ì„±ëŠ¥ ì°¨ì´ë¥¼ ë‚˜íƒ€ë‚¸ë‹¤. ê° ì ì€ 3Bì™€ 9Bì—ì„œ 27B íŒŒë¼ë¯¸í„°ê¹Œì§€ í™•ì¥ë˜ëŠ” ëª¨ë¸ í¬ê¸°ì™€ ë°ì´í„°ì…‹ í¬ê¸°ì˜ íŠ¹ì • ì—°ì‚° ìµœì  êµ¬ì„±ì„ ë‚˜íƒ€ë‚¸ë‹¤. **(b) í† í° í™•ì¥ì„± ê³¡ì„ .** í•™ìŠµ ì¤‘ 3B ëª¨ë¸ì˜ ê¶¤ì . ê° ì ì€ ì„œë¡œ ë‹¤ë¥¸ í•™ìŠµ í† í°ì—ì„œ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ë‚˜íƒ€ë‚¸ë‹¤. ìì„¸í•œ ì•„í‚¤í…ì²˜ ë° í•™ìŠµ êµ¬ì„±ì€ ë¶€ë¡ A.1ì— ì œê³µë˜ì–´ ìˆë‹¤.

ìš°ë¦¬ì˜ ì ‘ê·¼ë²•ì˜ í™•ì¥ì„±ì„ í‰ê°€í•˜ê¸° ìœ„í•´, ì„œë¡œ ë‹¤ë¥¸ ê·œëª¨ì— ê±¸ì³ ê¸°ì¤€ì„  ëŒ€ë¹„ _m_ HCì˜ ìƒëŒ€ì  ì†ì‹¤ ê°œì„ ì„ ë³´ê³ í•œë‹¤. ê·¸ë¦¼ 6 (a)ì—ì„œëŠ” 3B, 9B, 27B íŒŒë¼ë¯¸í„°ì— ê±¸ì¹œ ì—°ì‚° í™•ì¥ì„± ê³¡ì„ ì„ ê·¸ë¦°ë‹¤. ê¶¤ì ì€ ì„±ëŠ¥ ì´ì ì´ ë” ë†’ì€ ì—°ì‚° ì˜ˆì‚°ì—ì„œë„ ê²¬ê³ í•˜ê²Œ ìœ ì§€ë˜ë©°, ë¯¸ë¯¸í•œ ê°ì‡ ë§Œ ë³´ì¸ë‹¤ëŠ” ê²ƒì„ ë‚˜íƒ€ë‚¸ë‹¤. ë˜í•œ, ê·¸ë¦¼ 6 (b)ì—ì„œ ì‹¤í–‰ ë‚´ ë™ì—­í•™ì„ ì¡°ì‚¬í•˜ë©°, ì´ëŠ” 3B ëª¨ë¸ì— ëŒ€í•œ í† í° í™•ì¥ì„± ê³¡ì„ ì„ ì œì‹œí•œë‹¤. ì¢…í•©ì ìœ¼ë¡œ, ì´ëŸ¬í•œ ë°œê²¬ì€ ëŒ€ê·œëª¨ ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ _m_ HCì˜ íš¨ê³¼ë¥¼ ê²€ì¦í•œë‹¤. ì´ ê²°ë¡ ì€ ìš°ë¦¬ì˜ ì‚¬ë‚´ ëŒ€ê·œëª¨ í•™ìŠµ ì‹¤í—˜ì— ì˜í•´ ë”ìš± ì…ì¦ëœë‹¤.

13

ê·¸ë¦¼ 7 | **ë§¤ë‹ˆí´ë“œ ì œì•½ í•˜ì´í¼ ì—°ê²° (** _**m**_ **HC)ì˜ ì „íŒŒ ì•ˆì •ì„±.** ì´ ê·¸ë¦¼ì€ (a) ë‹¨ì¼ ê³„ì¸µ ë§¤í•‘ PMres (H _ğ‘™_ [res] )ì™€ (b) 27B ëª¨ë¸ ë‚´ì—ì„œ ë³µí•© ë§¤í•‘ [ï¿½] _ğ‘–_ _[ğ¿]_ = [âˆ’] 1 _[ğ‘™]_ [P][M][res] [(H] _ğ¿_ [res] - _ğ‘–_ [)]ì˜ ì „íŒŒ ë™ì—­í•™ì„ ë³´ì—¬ì¤€ë‹¤. ê²°ê³¼ëŠ” _m_ HCê°€ HCì™€ ë¹„êµí•˜ì—¬ ì „íŒŒ ì•ˆì •ì„±ì„ í¬ê²Œ í–¥ìƒì‹œí‚¨ë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì¤€ë‹¤.

ê·¸ë¦¼ 8 | **í•™ìŠµ ê°€ëŠ¥í•œ ë§¤í•‘ì˜ ì‹œê°í™”.** ì´ ê·¸ë¦¼ì€ HC (ì²« ë²ˆì§¸ í–‰)ì™€ _m_ HC (ë‘ ë²ˆì§¸ í–‰)ì— ëŒ€í•œ ëŒ€í‘œì ì¸ ë‹¨ì¼ ê³„ì¸µ ë° ë³µí•© ë§¤í•‘ì„ í‘œì‹œí•œë‹¤. ê° í–‰ë ¬ì€ ì„ íƒëœ ì‹œí€€ìŠ¤ ë‚´ì˜ ëª¨ë“  í† í°ì— ëŒ€í•´ í‰ê· ì„ êµ¬í•˜ì—¬ ê³„ì‚°ëœë‹¤. yì¶•ê³¼ xì¶•ì„ ë”°ë¼ í‘œì‹œëœ ë ˆì´ë¸”ì€ ê°ê° ìˆœì „íŒŒ ì‹ í˜¸ ì¦í­ (í–‰ í•©)ê³¼ ì—­ì „íŒŒ ê¸°ìš¸ê¸° ì¦í­ (ì—´ í•©)ì„ ë‚˜íƒ€ë‚¸ë‹¤.

**5.4. ì•ˆì •ì„± ë¶„ì„**

ê·¸ë¦¼ 3ê³¼ ìœ ì‚¬í•˜ê²Œ, ê·¸ë¦¼ 7ì€ _m_ HCì˜ ì „íŒŒ ì•ˆì •ì„±ì„ ë³´ì—¬ì¤€ë‹¤. ì´ìƒì ìœ¼ë¡œ, ë‹¨ì¼ ê³„ì¸µ ë§¤í•‘ì€ ì´ì¤‘ í™•ë¥  ì œì•½ì„ ë§Œì¡±í•˜ë©°, ì´ëŠ” ìˆœì „íŒŒ ì‹ í˜¸ ì¦í­ê³¼ ì—­ì „íŒŒ ê¸°ìš¸ê¸° ì¦í­ì´ ëª¨ë‘ 1ì´ì–´ì•¼ í•¨ì„ ì˜ë¯¸í•œë‹¤. ê·¸ëŸ¬ë‚˜ ì‹±í¬í˜¼-í¬ë†‰ ì•Œê³ ë¦¬ì¦˜ì„ í™œìš©í•˜ëŠ” ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ì—°ì‚° íš¨ìœ¨ì„±ì„ ë‹¬ì„±í•˜ê¸° ìœ„í•´ ë°˜ë³µ íšŸìˆ˜ë¥¼ ì œí•œí•´ì•¼ í•œë‹¤. ìš°ë¦¬ì˜ ì„¤ì •ì—ì„œëŠ” 20íšŒ ë°˜ë³µì„ ì‚¬ìš©í•˜ì—¬ ê·¼ì‚¬ ì†”ë£¨ì…˜ì„ ì–»ëŠ”ë‹¤. ê²°ê³¼ì ìœ¼ë¡œ, ê·¸ë¦¼ 7(a)ì—ì„œ ë³´ì—¬ì§€ë“¯ì´ ì—­ì „íŒŒ ê¸°ìš¸ê¸° ì¦í­ì´ 1ì—ì„œ ì•½ê°„ ë²—ì–´ë‚œë‹¤. ê·¸ë¦¼ 7(b)ì— í‘œì‹œëœ ë³µí•© ê²½ìš°ì—ì„œëŠ” í¸ì°¨ê°€ ì¦ê°€í•˜ì§€ë§Œ ì œí•œë˜ì–´ ìˆìœ¼ë©°, ìµœëŒ€ê°’ì´ ì•½ 1.6ì— ë„ë‹¬í•œë‹¤. íŠ¹íˆ, HCì˜ ê±°ì˜ 3000ì— ë‹¬í•˜ëŠ” ìµœëŒ€ ì´ë“ í¬ê¸°ì™€ ë¹„êµí•˜ì—¬ _m_ HCëŠ” ì´ë¥¼ 3ì°¨ìˆ˜ í¬ê¸°ë¡œ ê°ì†Œì‹œí‚¨ë‹¤. ì´ëŸ¬í•œ ê²°ê³¼ëŠ” _m_ HCê°€ HCì™€ ë¹„êµí•˜ì—¬ ì „íŒŒ ì•ˆì •ì„±ì„ í¬ê²Œ í–¥ìƒì‹œì¼œ ì•ˆì •ì ì¸ ìˆœì „íŒŒ ì‹ í˜¸ ë° ì—­ì „íŒŒ ê¸°ìš¸ê¸° íë¦„ì„ ë³´ì¥í•œë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì¤€ë‹¤. ì¶”ê°€ë¡œ, ê·¸ë¦¼ 8ì€ ëŒ€í‘œì ì¸ ë§¤í•‘ì„ í‘œì‹œí•œë‹¤. ìš°ë¦¬ëŠ” HCì˜ ê²½ìš° ìµœëŒ€ ì´ë“ì´ í´ ë•Œ ë‹¤ë¥¸ ê°’ë“¤ë„ í¬ê²Œ ë‚˜íƒ€ë‚˜ëŠ” ê²½í–¥ì´ ìˆìœ¼ë©°, ì´ëŠ” ëª¨ë“  ì „íŒŒ ê²½ë¡œì— ê±¸ì¹œ ì¼ë°˜ì ì¸ ë¶ˆì•ˆì •ì„±ì„ ë‚˜íƒ€ë‚¸ë‹¤ëŠ” ê²ƒì„ ê´€ì°°í•œë‹¤. ëŒ€ì¡°ì ìœ¼ë¡œ, _m_ HCëŠ” ì¼ê´€ë˜ê²Œ ì•ˆì •ì ì¸ ê²°ê³¼ë¥¼ ì‚°ì¶œí•œë‹¤.

14

#### **6. ê²°ë¡  ë° ì „ë§**

ë³¸ ë…¼ë¬¸ì—ì„œëŠ” í•˜ì´í¼ ì—°ê²° (HC)ì—ì„œ ì œì•ˆëœ ë°”ì™€ ê°™ì´ ì”ì°¨ ìŠ¤íŠ¸ë¦¼ì˜ í­ì„ í™•ì¥í•˜ê³  ì—°ê²°ì„ ë‹¤ì–‘í™”í•˜ëŠ” ê²ƒì´ ì„±ëŠ¥ í–¥ìƒì„ ê°€ì ¸ì˜¤ëŠ” ë™ì‹œì—, ì´ëŸ¬í•œ ì—°ê²°ì˜ ë¬´ì œì•½ì  íŠ¹ì„±ì´ ì‹ í˜¸ ë°œì‚°ì„ ìœ ë°œí•œë‹¤ëŠ” ê²ƒì„ ë°íŒë‹¤. ì´ëŸ¬í•œ êµë€ì€ ê³„ì¸µ ê°„ ì‹ í˜¸ ì—ë„ˆì§€ ë³´ì¡´ì„ ì†ìƒì‹œì¼œ í•™ìŠµ ë¶ˆì•ˆì •ì„±ì„ ìœ ë„í•˜ê³  ì‹¬ì¸µ ë„¤íŠ¸ì›Œí¬ì˜ í™•ì¥ì„±ì„ ì €í•´í•œë‹¤. ì´ëŸ¬í•œ ê³¼ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ìš°ë¦¬ëŠ” ì”ì°¨ ì—°ê²° ê³µê°„ì„ íŠ¹ì • ë§¤ë‹ˆí´ë“œì— íˆ¬ì˜í•˜ëŠ” ì¼ë°˜í™”ëœ í”„ë ˆì„ì›Œí¬ì¸ **ë§¤ë‹ˆí´ë“œ ì œì•½ í•˜ì´í¼ ì—°ê²°** ( _**m**_ **HC** )ì„ ì†Œê°œí•œë‹¤. ì‹±í¬í˜¼-í¬ë†‰ ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•˜ì—¬ ì”ì°¨ ë§¤í•‘ì— ì´ì¤‘ í™•ë¥  ì œì•½ì„ ì ìš©í•¨ìœ¼ë¡œì¨, _m_ HCëŠ” ì‹ í˜¸ ì „íŒŒë¥¼ íŠ¹ì§•ì˜ ë³¼ë¡ ì¡°í•©ìœ¼ë¡œ ë³€í™˜í•œë‹¤. ì‹¤í—˜ ê²°ê³¼ëŠ” _m_ HCê°€ íš¨ê³¼ì ìœ¼ë¡œ í•­ë“± ì‚¬ìƒ íŠ¹ì„±ì„ ë³µì›í•˜ì—¬ ê¸°ì¡´ HCì™€ ë¹„êµí•˜ì—¬ ìš°ìˆ˜í•œ í™•ì¥ì„±ìœ¼ë¡œ ì•ˆì •ì ì¸ ëŒ€ê·œëª¨ í•™ìŠµì„ ê°€ëŠ¥í•˜ê²Œ í•œë‹¤ëŠ” ê²ƒì„ í™•ì¸í•œë‹¤. ì¤‘ìš”í•˜ê²Œë„, íš¨ìœ¨ì ì¸ ì¸í”„ë¼ ìˆ˜ì¤€ ìµœì í™”ë¥¼ í†µí•´ _m_ HCëŠ” ë¬´ì‹œí•  ìˆ˜ ìˆëŠ” ì—°ì‚° ì˜¤ë²„í—¤ë“œë¡œ ì´ëŸ¬í•œ ê°œì„ ì„ ì œê³µí•œë‹¤.

HC íŒ¨ëŸ¬ë‹¤ì„ì˜ ì¼ë°˜í™”ëœ í™•ì¥ìœ¼ë¡œì„œ, _m_ HCëŠ” í–¥í›„ ì—°êµ¬ë¥¼ ìœ„í•œ ì—¬ëŸ¬ ìœ ë§í•œ ë°©í–¥ì„ ì œì‹œí•œë‹¤. ë³¸ ì—°êµ¬ëŠ” ì•ˆì •ì„±ì„ ë³´ì¥í•˜ê¸° ìœ„í•´ ì´ì¤‘ í™•ë¥  í–‰ë ¬ì„ í™œìš©í•˜ì§€ë§Œ, í”„ë ˆì„ì›Œí¬ëŠ” íŠ¹ì • í•™ìŠµ ëª©í‘œì— ë§ì¶¤í™”ëœ ë‹¤ì–‘í•œ ë§¤ë‹ˆí´ë“œ ì œì•½ì˜ íƒìƒ‰ì„ ìˆ˜ìš©í•œë‹¤. ìš°ë¦¬ëŠ” ì„œë¡œ ë‹¤ë¥¸ ê¸°í•˜í•™ì  ì œì•½ì— ëŒ€í•œ ì¶”ê°€ ì¡°ì‚¬ê°€ ê°€ì†Œì„±ê³¼ ì•ˆì •ì„± ê°„ì˜ ê· í˜•ì„ ë” ì˜ ìµœì í™”í•˜ëŠ” ìƒˆë¡œìš´ ë°©ë²•ì„ ì‚°ì¶œí•  ìˆ˜ ìˆì„ ê²ƒìœ¼ë¡œ ê¸°ëŒ€í•œë‹¤. ë˜í•œ, _m_ HCê°€ ê±°ì‹œì  ì•„í‚¤í…ì²˜ ì„¤ê³„ì— ëŒ€í•œ ì»¤ë®¤ë‹ˆí‹°ì˜ ê´€ì‹¬ì„ ë‹¤ì‹œ ë¶ˆëŸ¬ì¼ìœ¼í‚¤ê¸°ë¥¼ ë°”ë€ë‹¤. í† í´ë¡œì§€ êµ¬ì¡°ê°€ ìµœì í™” ë° í‘œí˜„ í•™ìŠµì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì— ëŒ€í•œ ì´í•´ë¥¼ ì‹¬í™”í•¨ìœ¼ë¡œì¨, _m_ HCëŠ” í˜„ì¬ì˜ í•œê³„ë¥¼ í•´ê²°í•˜ê³  ì°¨ì„¸ëŒ€ ê¸°ì´ˆ ì•„í‚¤í…ì²˜ì˜ ì§„í™”ë¥¼ ìœ„í•œ ìƒˆë¡œìš´ ê²½ë¡œë¥¼ ì ì¬ì ìœ¼ë¡œ ì¡°ëª…í•  ê²ƒì´ë‹¤.

#### **ì°¸ê³ ë¬¸í—Œ**

J. Ainslie, J. Lee-Thorp, M. De Jong, Y. Zemlyanskiy, F. LebrÃ³n, and S. Sanghai. Gqa: Training generalized multi-query transformer models from multi-head checkpoints. arXiv preprint arXiv:2305.13245, 2023.

Y. Bisk, R. Zellers, R. L. Bras, J. Gao, and Y. Choi. PIQA: reasoning about physical commonsense in natural language. In The Thirty-Fourth AAAI Conference on Artifcial Intelligence, AAAI
2020, The Thirty-Second Innovative Applications of Artifcial Intelligence Conference, IAAI
2020, The Tenth AAAI Symposium on Educational Advances in Artifcial Intelligence, EAAI
2020, New York, NY, USA, February 7-12, 2020, pages 7432â€“7439. AAAI Press, 2020. doi:
10.1609/aaai.v34i05.6239. URL `https://doi.org/10.1609/aaai.v34i05.6239` .

T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877â€“1901, 2020.

Y. Chai, S. Jin, and X. Hou. Highway transformer: Self-gating enhanced self-attentive networks.
In D. Jurafsky, J. Chai, N. Schluter, and J. Tetreault, editors, Proceedings of the 58th Annual
Meeting of the Association for Computational Linguistics, pages 6887â€“6900, Online, July
2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.616. URL
`https://aclanthology.org/2020.acl-main.616/` .

F. Chollet. Xception: Deep learning with depthwise separable convolutions. In Proceedings of

the IEEE conference on computer vision and pattern recognition, pages 1251â€“1258, 2017.

15

K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek, J. Hilton, R. Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.

T. Dao, D. Y. Fu, S. Ermon, A. Rudra, and C. RÃ©. FlashAttention: Fast and memory-efficient exact attention with IO-awareness. In Advances in Neural Information Processing Systems (NeurIPS), 2022.

D. Dua, Y. Wang, P. Dasigi, G. Stanovsky, S. Singh, and M. Gardner. DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs. In J. Burstein, C. Doran, and
T. Solorio, editors, Proceedings of the 2019 Conference of the North American Chapter of the
Association for Computational Linguistics: Human Language Technologies, NAACL-HLT
2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pages 2368â€“
2378. Association for Computational Linguistics, 2019. doi: 10.18653/V1/N19-1246. URL
`https://doi.org/10.18653/v1/n19-1246` .

Y. Fang, Y. CAI, J. Chen, J. Zhao, G. Tian, and G. Li. Cross-layer retrospective retrieving via layer attention. In The Eleventh International Conference on Learning Representations, 2023. URL
`https://openreview.net/forum?id=pvgEL1yS3Ql` .

W. Fedus, B. Zoph, and N. Shazeer. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. Journal of Machine Learning Research, 23(120):1â€“39, 2022.

K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proceedings

of the IEEE conference on computer vision and pattern recognition, pages 770â€“778, 2016a.

K. He, X. Zhang, S. Ren, and J. Sun. Identity mappings in deep residual networks. In European

conference on computer vision, pages 630â€“645. Springer, 2016b.

M. Heddes, A. Javanmard, K. Axiotis, G. Fu, M. Bateni, and V. Mirrokni. Deepcrossattention:
Supercharging transformer residual connections. In Forty-second International Conference on Machine Learning, 2025. URL `https://openreview.net/forum?id=j3JBfFnGYh` .

D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020.

D. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang, D. Song, and J. Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021.

J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. de Las Casas, L. A. Hendricks, J. Welbl, A. Clark, T. Hennigan, E. Noland, K. Millican, G. van den Driessche, B. Damoc, A. Guy, S. Osindero, K. Simonyan, E. Elsen, O. Vinyals, J. Rae, and L. Sifre.
An empirical analysis of compute-optimal large language model training. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural
Information Processing Systems, volume 35, pages 30016â€“30030. Curran Associates, Inc., 2022.
URL `https://proceedings.neurips.cc/paper_files/paper/2022/file/c1e2faff6f588870935f114ebe04a3e5-Paper-Conference.pdf` .

G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger. Densely connected convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4700â€“4708, 2017.

16

M. Joshi, E. Choi, D. Weld, and L. Zettlemoyer. TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension. In R. Barzilay and M.-Y. Kan, editors, Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long
Papers), pages 1601â€“1611, Vancouver, Canada, July 2017. Association for Computational
Linguistics. doi: 10.18653/v1/P17-1147. URL `https://aclanthology.org/P17-1147` .

G. Larsson, M. Maire, and G. Shakhnarovich. Fractalnet: Ultra-deep neural networks without residuals. arXiv preprint arXiv:1605.07648, 2016.

D. Lepikhin, H. Lee, Y. Xu, D. Chen, O. Firat, Y. Huang, M. Krikun, N. Shazeer, and Z. Chen.
Gshard: Scaling giant models with conditional computation and automatic sharding. arXiv preprint arXiv:2006.16668, 2020.

A. Liu, B. Feng, B. Wang, B. Wang, B. Liu, C. Zhao, C. Dengr, C. Ruan, D. Dai, D. Guo, et al.
Deepseek-v2: A strong, economical, and efficient mixture-of-experts language model. arXiv preprint arXiv:2405.04434, 2024a.

A. Liu, B. Feng, B. Xue, B. Wang, B. Wu, C. Lu, C. Zhao, C. Deng, C. Zhang, C. Ruan, et al.
Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024b.

I. Loshchilov and F. Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017.

B. Mak and J. Flanigan. Residual matrix transformers: Scaling the size of the residual stream.

arXiv preprint arXiv:2506.22696, 2025.

G. Menghani, R. Kumar, and S. Kumar. LAurel: Learned augmented residual layer. In
Forty-second International Conference on Machine Learning, 2025. URL `https://openreview.net/forum?id=rUDRWP9WvZ` .

M. Pagliardini, A. Mohtashami, F. Fleuret, and M. Jaggi. Denseformer: Enhancing information flow in transformers via depth weighted averaging. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. URL `https://openreview.net/forum?id=kMnoh7CXrq` .

P. Qi, X. Wan, G. Huang, and M. Lin. Zero bubble (almost) pipeline parallelism. In The Twelfth

International Conference on Learning Representations, 2024. URL `https://openreview.net/forum?id=tuzTN0eIO5` .

N. Shazeer. Fast transformer decoding: One write-head is all you need. arXiv preprint arXiv:1911.02150, 2019.

N. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. Le, G. Hinton, and J. Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538, 2017.

R. Sinkhorn and P. Knopp. Concerning nonnegative matrices and doubly stochastic matrices.

Pacifc Journal of Mathematics, 21(2):343â€“348, 1967.

R. K. Srivastava, K. Greff, and J. Schmidhuber. Training very deep networks. In C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information
Processing Systems, volume 28. Curran Associates, Inc., 2015. URL `https://proceedings.neurips.cc/paper_files/paper/2015/file/215a71a12769b056c3c32e7299f1c5ed-Paper.pdf` .

17

J. Su, M. Ahmed, Y. Lu, S. Pan, W. Bo, and Y. Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024.

M. Suzgun, N. Scales, N. SchÃ¤rli, S. Gehrmann, Y. Tay, H. W. Chung, A. Chowdhery, Q. V. Le, E. H. Chi, D. Zhou, et al. Challenging big-bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261, 2022.

H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. RoziÃ¨re, N. Goyal, E. Hambro, F. Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.

A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Å. Kaiser, and I. Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.

### Translation Notes

- ì°¸ê³ ë¬¸í—Œ ì„¹ì…˜ì€ í•™ìˆ  ê´€ë¡€ì— ë”°ë¼ ì›ë¬¸ í˜•ì‹ì„ ìœ ì§€í–ˆìŠµë‹ˆë‹¤ (ì €ìëª…, ì œëª©, ì¶œíŒ ì •ë³´, URL ë“± ëª¨ë‘ ì˜ë¬¸ ì›ë¬¸ ê·¸ëŒ€ë¡œ ìœ ì§€)
- ë²¤ì¹˜ë§ˆí¬ ë°ì´í„°ì…‹ ì´ë¦„ë“¤ (BBH, DROP, PIQA, TriviaQA, MMLU, MATH, HellaSwag, GSM8K)ì€ ê³ ìœ ëª…ì‚¬ë¡œ ì›ë¬¸ ìœ ì§€
- ëª¨ë¸ëª… ë° í”„ë ˆì„ì›Œí¬ ì´ë¦„ (DeepSeek-V3, HC, mHC)ì€ ì¼ê´€ì„± ìˆê²Œ ì²˜ë¦¬
- ìˆ˜ì¹˜ ë°ì´í„° (2.1%, 2.3%, 3B, 9B, 27B ë“±)ëŠ” ì›ë¬¸ ê·¸ëŒ€ë¡œ ìœ ì§€
- ê·¸ë¦¼ ë° í‘œ ì°¸ì¡°ëŠ” í•œêµ­ì–´ ì¡°ì‚¬ë¥¼ ì ì ˆíˆ ì‚¬ìš© (ê·¸ë¦¼ 6, ê·¸ë¦¼ 7, ê·¸ë¦¼ 8, í‘œ 4)
- ê¸°ìˆ  ìš©ì–´ëŠ” glossaryì— ë”°ë¼ ì¼ê´€ë˜ê²Œ ë²ˆì—­ (ì „íŒŒ ì•ˆì •ì„±, ìˆœì „íŒŒ ì‹ í˜¸ ì¦í­, ì—­ì „íŒŒ ê¸°ìš¸ê¸° ì¦í­ ë“±)
- ê²°ë¡  ë¶€ë¶„ì€ ë…¼ë¬¸ì˜ í•µì‹¬ ê¸°ì—¬ë¥¼ ìš”ì•½í•˜ëŠ” ì¤‘ìš”í•œ ì„¹ì…˜ìœ¼ë¡œ, ì´ì „ ì„¹ì…˜ì—ì„œ ì‚¬ìš©ëœ ìš©ì–´ì™€ì˜ ì¼ê´€ì„±ì„ ìœ ì§€í•˜ë©° ë²ˆì—­
- ìˆ˜í•™ì  í‘œê¸° (PMres, H_l, ë“±)ëŠ” ì›ë¬¸ ê·¸ëŒ€ë¡œ ìœ ì§€

### Confidence Level

High - ì´ ì²­í¬ëŠ” ì‹¤í—˜ ê²°ê³¼, í™•ì¥ì„± ë¶„ì„, ì•ˆì •ì„± ê²€ì¦, ê²°ë¡  ë° ì°¸ê³ ë¬¸í—Œì„ í¬í•¨í•˜ê³  ìˆìŠµë‹ˆë‹¤. ê¸°ìˆ  ìš©ì–´ëŠ” glossaryë¥¼ ì¼ê´€ë˜ê²Œ ì ìš©í–ˆìœ¼ë©°, í•™ìˆ  ë…¼ë¬¸ì˜ ê²°ë¡  ë¶€ë¶„ì— ì í•©í•œ í˜•ì‹ì ì¸ ì–´ì¡°ë¥¼ ìœ ì§€í–ˆìŠµë‹ˆë‹¤. ì°¸ê³ ë¬¸í—Œì€ í•™ìˆ  ê´€ë¡€ì— ë”°ë¼ ì›ë¬¸ì„ ê·¸ëŒ€ë¡œ ìœ ì§€í–ˆìŠµë‹ˆë‹¤.

### Transition Notes

- Ending: ì´ ì²­í¬ëŠ” ì°¸ê³ ë¬¸í—Œ ì„¹ì…˜ì˜ ì¼ë¶€ë¡œ ì¢…ë£Œë©ë‹ˆë‹¤ (ì•„ì§ ì „ì²´ ì°¸ê³ ë¬¸í—Œ ëª©ë¡ì´ ì™„ë£Œë˜ì§€ ì•ŠìŒ). ë‹¤ìŒ ì²­í¬(chunk 5)ëŠ” ë‚˜ë¨¸ì§€ ì°¸ê³ ë¬¸í—Œê³¼ ë¶€ë¡ A.1ì˜ ëª¨ë¸ ì‚¬ì–‘ì„ í¬í•¨í•  ê²ƒìœ¼ë¡œ ì˜ˆìƒë©ë‹ˆë‹¤.
