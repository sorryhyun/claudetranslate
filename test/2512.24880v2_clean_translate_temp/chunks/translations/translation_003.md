## Translation Output

### Translated Text

8

스트림 간 정보 혼합을 단조적으로 유지하여 강건한 특징 융합 메커니즘으로 효과적으로 기능합니다.

또한, 입력 매핑 H _𝑙_ [pre]와 출력 매핑 H _𝑙_ [post]에 비음수 제약을 부과합니다. 이 제약은 양수 계수와 음수 계수의 합성으로 발생하는 신호 상쇄를 방지하며, 이는 특수한 매니폴드 투영으로도 간주될 수 있습니다.

**4.2. 매개변수화와 매니폴드 투영**

이 절에서는 _m_ HC에서 H _𝑙_ [pre], H _𝑙_ [post], H _𝑙_ [res]의 계산 과정을 자세히 설명합니다. _𝑙_ 번째 계층의 입력 은닉 행렬 **x** _𝑙_ ∈ **R** _[𝑛]_ [×] _[𝐶]_가 주어지면, 먼저 이를 벡터 � **x** _𝑙_ = vec( **x** _𝑙_ ) ∈ **R** [1][×] _[𝑛𝐶]_로

평탄화하여 전체 맥락 정보를 보존합니다. 그런 다음 원래의 HC 공식을 따라 동적 매핑과 정적 매핑을 다음과 같이 얻습니다:

**x**  - [′] _𝑙_ [=][ RMSNorm][(�] **[x]** _[𝑙]_ [)]

H˜ _𝑙_ [pre] = _𝛼_ [pre] _𝑙_ - (� **x** [′] _𝑙_ _[𝜑]_ [pre] _𝑙_ ) + **b** [pre] _𝑙_

H˜ _𝑙_ [post] = _𝛼_ [post] _𝑙_  - (� **x** [′] _𝑙_ _[𝜑]_ [post] _𝑙_ ) + **b** [post] _𝑙_
H˜ _𝑙_ [res] = _𝛼_ [res] _𝑙_  - mat(� **x** [′] _𝑙_ _[𝜑]_ [res] _𝑙_ [) +] **[ b]** [res] _𝑙_ [,]



(7)

여기서 _𝜑_ [pre] _𝑙_, _𝜑_ [post] _𝑙_ ∈ **R** _[𝑛𝐶]_ [×] _[𝑛]_와 _𝜑_ [res] _𝑙_ ∈ **R** _[𝑛𝐶]_ [×] _[𝑛]_ [2]는 동적 매핑을 위한 선형 투영이며, mat(·)는 **R** [1][×] _[𝑛]_ [2]에서 **R** _[𝑛]_ [×] _[𝑛]_으로의 재구성 함수입니다.

그런 다음, 최종 제약 매핑은 다음과 같이 얻어집니다:

H _𝑙_ [pre] = _𝜎_ (H [˜] _𝑙_ [pre] )



H _𝑙_ [post] = 2 _𝜎_ (H [˜] _𝑙_ [post] )
H _𝑙_ [res] = Sinkhorn-Knopp(H [˜] _𝑙_ [res] ),



(8)

여기서 _𝜎_ (·)는 시그모이드 함수를 나타냅니다. 싱크혼-크놉 연산자(Sinkhorn-Knopp(·))는 먼저 지수 연산자를 통해 모든 원소를 양수로 만든 다음, 행과 열을 번갈아 재조정하여 합이 1이 되도록 하는 반복 정규화 과정을 수행합니다. 구체적으로, 시작점으로 양수 행렬 **M** [(][0][)] = exp(H [˜] _𝑙_ [res] )가 주어지면, 정규화 반복은 다음과 같이 진행됩니다:

-                             **M** [(] _[𝑡]_ [)] = T _𝑟_ T _𝑐_ ( **M** [(] _[𝑡]_ [−][1][)] ), (9)

여기서 T _𝑟_과 T _𝑐_는 각각 행 정규화와 열 정규화를 나타냅니다. 이 과정은 _𝑡_ max →∞일 때 이중 확률 행렬 H _𝑙_ [res] = **M** [(] _[𝑡]_ [max] [)]로 수렴합니다. 실험에서는 _𝑡_ max = 20을 실용적인 값으로 선택합니다.

**4.3. 효율적인 인프라 설계**

이 절에서는 _m_ HC를 위해 맞춤화된 인프라 설계를 자세히 설명합니다. 엄격한 최적화를 통해, 대규모 모델에서 _m_ HC (_𝑛_ = 4)를 단 6.7%의 미미한 학습 오버헤드로 구현합니다.

_**4.3.1. 커널 융합**_

_m_ HC의 RMSNorm이 고차원 은닉 상태 � **x** _𝑙_ ∈ **R** [1][×] _[𝑛𝐶]_에서 작동할 때 상당한 지연을 발생시킨다는 것을 관찰하여, 노름으로 나누는 연산을 행렬 곱셈 이후로 재배치합니다.

9

이 최적화는 효율성을 향상시키면서 수학적 동등성을 유지합니다. 또한, 속도를 저해하지 않으면서 수치 정확도를 최대화하기 위해 혼합 정밀도 전략을 사용하고, 공유 메모리 접근을 가진 여러 연산을 통합된 계산 커널로 융합하여 메모리 대역폭 병목을 줄입니다. 식 (10)~(13)에 자세히 설명된 입력과 매개변수를 기반으로, H _𝑙_ [pre], H _𝑙_ [post], H _𝑙_ [res]를 계산하기 위한 세 가지 특수화된 _m_ HC 커널을 구현합니다. 이러한 커널에서 편향과 선형 투영은 **b** _𝑙_과 _𝜑𝑙_로 통합되며, RMSNorm 가중치도 _𝜑𝑙_에 흡수됩니다.

- 식 (14)~(15): � **x** _𝑙_에 대한 두 번의 스캔을 융합하는 통합 커널을 개발하여, 행렬 곱셈 유닛을 활용하여 메모리 대역폭 활용을 최대화합니다. 역전파 과정—두 번의 행렬 곱셈으로 구성됨—도 유사하게 단일 커널로 통합되어 � **x** _𝑙_의 중복 재로딩을 제거합니다. 두 커널 모두 혼합 정밀도 처리를 효율적으로 다루기 위해 세밀하게 조정된 파이프라인(로드, 캐스트, 계산, 저장)을 특징으로 합니다.

- 식 (16)~(18): 작은 계수에 대한 이러한 경량 연산은 단일 커널로 기회주의적으로 융합되어 커널 실행 오버헤드를 크게 줄입니다.

- 식 (19): 싱크혼-크놉 반복을 단일 커널 내에서 구현합니다. 역전파 과정에서는 중간 결과를 온칩에서 재계산하고 전체 반복을 순회하는 맞춤형 역전파 커널을 유도합니다.

_𝜑𝑙_ : tfloat32 [ _𝑛𝐶_, _𝑛_ [2] + 2 _𝑛_ ] (10)
**x**            - _𝑙_ : bfloat16 [1, _𝑛𝐶_ ] (11)

_𝛼_ [pre] _𝑙_, _𝛼_ [post] _𝑙_, _𝛼_ [res] _𝑙_ : float32 스칼라 (12)

**b** _𝑙_ : float32 [1, _𝑛_ [2] + 2 _𝑛_ ] (13)

- pre      H˜˜ _𝑙_, H [˜˜] _𝑙_ [post], H [˜˜] _𝑙_ [res] : float32 = � **x** _𝑙_ _𝜑𝑙_ (14)

~~√~~
_𝑟_ : float32 = �� **x**                - _𝑙_ ��2 [/] _𝑛𝐶_ (15)

�H˜ _𝑙_ [pre], H [˜] _𝑙_ [post], H [˜] _𝑙_ [res]     - : float32 = 1/ _𝑟_     - _𝛼_ [pre] _𝑙_ H˜˜ _𝑙_ [pre], _𝛼_ [post] _𝑙_ H˜˜ _𝑙_ [post], _𝛼_ [res] _𝑙_ H˜˜ _𝑙_ res� + **b** _𝑙_ (16)

-                               H _𝑙_ [pre] : float32 = _𝜎_ H˜ _𝑙_ [pre] (17)

-                                H _𝑙_ [post] : float32 = 2 _𝜎_ H˜ _𝑙_ [post] (18)

H _𝑙_ [res] : float32 = Sinkhorn-Knopp H _𝑙_ [res]      - (19)

[�] [˜]

앞서 언급한 커널에서 유도된 계수를 사용하여, 이러한 매핑을 적용하기 위한 두 개의 추가 커널을 도입합니다: 하나는 Fpre ≔ H _𝑙_ [pre] **x** _𝑙_를 위한 것이고, 다른 하나는 Fpost,res ≔
H _𝑙_ [res] **x** _𝑙_ + H _𝑙_ [post][ ⊤] F (·, ·)를 위한 것입니다. H _𝑙_ [post]와 H _𝑙_ [res]의 적용을 잔차 병합과 융합함으로써, 이 커널에서 읽는 원소의 수를 (3 _𝑛_ + 1) _𝐶_에서 ( _𝑛_ + 1) _𝐶_로, 쓰는 원소의 수를 3 _𝑛𝐶_에서 _𝑛𝐶_로 줄입니다. 대부분의 커널(식 (14)~(15) 제외)을 TileLang (Wang et al., 2025)을 사용하여 효율적으로 구현합니다. 이 프레임워크는 복잡한 계산 과정을 가진 커널의 구현을 간소화하고 최소한의 엔지니어링 노력으로 메모리 대역폭을 완전히 활용할 수 있게 합니다.

_**4.3.2. 재계산**_

_𝑛_ 개의 스트림 잔차 설계는 학습 중에 상당한 메모리 오버헤드를 도입합니다. 이를 완화하기 위해, 순전파 이후 _m_ HC 커널의 중간 활성화를 폐기하고 역전파 과정에서 즉석에서 재계산합니다. 이는 무거운 계층 함수 F 없이 _m_ HC 커널을 재실행함으로써 이루어집니다.

10

결과적으로, _𝐿𝑟_개의 연속 계층 블록에 대해, 첫 번째 계층에 대한 입력 **x** _𝑙_ 0만 저장하면 됩니다. F 내의 사전 정규화를 고려하여 경량 계수를 제외하면, 표 3은 역전파 과정을 위해 보존되는 중간 활성화를 요약합니다.

표 3 | **저장 및 재계산되는 중간 활성화** _𝐿𝑟_개의 연속 계층에서 역전파를 위해 보존된 토큰당 활성화와 재계산되는 임시 활성화를 나열합니다.
계층 _𝑙_ 0은 _𝐿𝑟_ 계층의 첫 번째 계층을 나타내며 계층 _𝑙_은 [ _𝑙_ 0, _𝑙_ 0 + _𝐿𝑟_ - 1]에 있습니다.

활성화 **x** _𝑙_ 0 F (H _𝑙_ [pre] **x** _𝑙_, W _𝑙_ ) **x** _𝑙_ H _𝑙_ [pre] **x** _𝑙_ RMSNorm(H _𝑙_ [pre] **x** _𝑙_ )

크기 (원소) _𝑛𝐶_ _𝐶_ _𝑛𝐶_ _𝐶_ _𝐶_
저장 방법 _𝐿𝑟_ 계층마다 모든 계층 _𝐿𝑟_ 계층 내 임시

_m_ HC 커널 재계산은 _𝐿𝑟_개의 연속 계층 블록에 대해 수행되므로, 총 _𝐿_개의 계층이 주어지면, 역전파 과정을 위해 모든 ⌈ _𝐿_ _[𝐿]_ _𝑟_ [⌉] [블록에 대한 첫 번째 계층 입력] **[x]** _𝑙_ 0을 지속적으로 저장해야 합니다.

이러한 상주 메모리 외에도, 재계산 과정은 활성 블록에 대해 ( _𝑛_ + 2) _𝐶_ × _𝐿𝑟_ 원소의 일시적 메모리 오버헤드를 도입하며, 이는 역전파 중 최대 메모리 사용량을 결정합니다. 결과적으로, 다음과 같이 _𝐿_ [𝑟][에 대응하는 총 메모리 공간을 최소화함으로써 최적 블록 크기] _𝐿_ [∗] _𝑟_[을 결정합니다:]

_𝑛𝐿_ (20)
_𝑛_ + 2 [.]

~~√~~

≈

- _𝐿_
_𝑛𝐶_ ×

_𝐿𝑟_

+ ( _𝑛_ + 2) _𝐶_ × _𝐿𝑟_

_𝐿_ [∗] _𝑟_ [=][ arg min]
_𝐿𝑟_

또한, 대규모 학습에서 파이프라인 병렬화는 재계산 블록이 파이프라인 스테이지 경계를 넘지 않아야 한다는 제약을 부과합니다. 이론적 최적값 _𝐿_ [∗] _𝑟_이 일반적으로 파이프라인 스테이지당 계층 수와 일치한다는 것을 관찰하여, 재계산 경계를 파이프라인 스테이지와 동기화하도록 선택합니다.

_**4.3.3. 듀얼파이프에서 통신 중첩**_

대규모 학습에서 파이프라인 병렬화는 매개변수와 기울기 메모리 공간을 완화하기 위한 표준 관행입니다. 구체적으로, 우리는 전문가 및 파이프라인 병렬화와 같은 스케일아웃 상호 연결 통신 트래픽을 효과적으로 중첩시키는 듀얼파이프 스케줄 (Liu et al., 2024b)을 채택합니다. 그러나 단일 스트림 설계에 비해, _m_ HC에서 제안된
_𝑛_ 개의 스트림 잔차는 파이프라인 스테이지 간 상당한 통신 지연을 발생시킵니다.
또한, 스테이지 경계에서 모든 _𝐿𝑟_ 계층에 대한 _m_ HC 커널의 재계산은 무시할 수 없는 계산 오버헤드를 도입합니다. 이러한 병목을 해결하기 위해, 파이프라인 스테이지 경계에서 통신과 계산의 개선된 중첩을 용이하게 하도록 듀얼파이프 스케줄을 확장합니다(그림 4 참조).

특히, 통신 스트림의 차단을 방지하기 위해, MLP (즉, FFN) 계층의 Fpost,res 커널을 전용 고우선순위 계산 스트림에서 실행합니다. 또한 주목 계층에서 장시간 실행되는 연산에 대해 지속 커널을 사용하는 것을 자제하여 장시간 정지를 방지합니다. 이 설계는 중첩된 주목 계산의 선점을 가능하게 하여, 계산 장치의 처리 유닛의 높은 활용률을 유지하면서 유연한 스케줄링을 가능하게 합니다. 또한, 재계산 과정은 각 스테이지의 초기 활성화 **x** _𝑙_ 0이 이미 로컬에 캐시되어 있으므로 파이프라인 통신 종속성으로부터 분리됩니다.

11

정상 계산 스트림

통신 스트림

고우선순위 계산 스트림

!"#$, '(#) (F) ℱ!"#$, '(#)

 ℱ!"#$, '(#)

) (B)

그림 4 | **_m_ HC를 위한 통신-계산 중첩.** _m_ HC에 의해 도입된 오버헤드를 처리하기 위해 듀얼파이프
스케줄을 확장합니다. 각 블록의 길이는 설명 목적일 뿐이며 실제 지속 시간을 나타내지 않습니다. (F), (B), (W)는 각각 순전파, 역전파, 가중치 기울기 계산을 나타냅니다. F [A]와 F [M]은 각각 주목과 MLP에 대응하는 커널을 나타냅니다.

#### **5. 실험**

**5.1. 실험 설정**

제안된 방법을 언어 모델 사전 학습을 통해 검증하고, 기준선, HC, 그리고 제안된 _m_ HC 간의 비교 분석을 수행합니다. DeepSeek-V3 (Liu et al., 2024b)에서 영감을 받은 MoE 아키텍처를 활용하여 서로 다른 평가 체제를 포괄하는 네 가지 고유한 모델 변형을 학습합니다. 구체적으로, HC와 _m_ HC 모두의 확장 비율 _𝑛_은 4로 설정됩니다. 우리의 주된 초점은 매개변수에 비례하는 데이터셋 크기로 학습된 27B 모델이며, 이는 시스템 수준 주요 결과의 대상이 됩니다. 이를 확장하여, 비례 데이터로 학습된 더 작은 3B 및 9B 모델을 통합하여 연산 확장 동작을 분석하며, 이를 통해 다양한 연산에 걸친 성능 추세를 관찰할 수 있습니다. 또한, 토큰 확장 동작을 구체적으로 조사하기 위해, 1조 토큰의 고정 코퍼스로 학습된 별도의 3B 모델을 학습합니다. 자세한 모델 구성 및 학습 하이퍼파라미터는 부록 A.1에 제공됩니다.

**5.2. 주요 결과**

그림 5 | **매니폴드 제약 하이퍼 연결 (** _**m**_ **HC)의 학습 안정성.** 이 그림은
(a) 기준선에 대한 _m_ HC와 HC의 절대 학습 손실 차이, 그리고 (b) 세 가지 방법의 기울기 노름을 보여줍니다. 모든 실험은 27B 모델을 활용합니다. 결과는 _m_ HC가 손실과 기울기 노름 모두에서 개선된 안정성을 보인다는 것을 보여줍니다.

27B 모델의 학습 안정성과 수렴을 검토하는 것으로 시작합니다. 그림 5 (a)에서 보듯이, _m_ HC는 HC에서 관찰된 학습 불안정성을 효과적으로 완화하여 기준선에 비해 0.021의 최종 손실 감소를 달성합니다. 이러한 개선된 안정성은 그림 5 (b)의 기울기 노름 분석에서 더욱 확증되며, _m_ HC는 HC보다 현저히 나은 동작을 보이며, 기준선과 유사한 안정적인 프로파일을 유지합니다.

12

표 4 | **27B 모델에 대한 시스템 수준 벤치마크 결과.** 이 표는 8개의 다양한 다운스트림
벤치마크에 걸친 기준선, HC, _m_ HC의 제로샷 및 퓨샷 성능을 비교합니다. _m_ HC는 기준선을 일관되게 능가하고 대부분의 벤치마크에서 HC를 초과하여 대규모 사전 학습에서의 효과를 입증합니다.

**벤치마크** BBH DROP GSM8K HellaSwag MATH MMLU PIQA TriviaQA
**(지표)** (EM) (F1) (EM) (Acc.) (EM) (Acc.) (Acc.) (EM)

### Translation Notes
- 모든 수학 기호, 변수, 행렬 표기법은 원문 그대로 보존했습니다 (예: **x** _𝑙_, H _𝑙_ [pre], _𝜑𝑙_).
- 식 번호 (7), (8), (9), (10)~(20)은 원문 그대로 유지했습니다.
- 기술 용어는 용어집에 따라 일관성 있게 번역했습니다:
  - "Sinkhorn-Knopp" → "싱크혼-크놉"
  - "doubly stochastic matrix" → "이중 확률 행렬"
  - "kernel fusion" → "커널 융합"
  - "recomputing" → "재계산"
  - "DualPipe" → "듀얼파이프"
  - "pipeline parallelism" → "파이프라인 병렬화"
- 표 3과 그림 4의 캡션을 한국어로 번역하되, 변수명과 수식은 보존했습니다.
- 벤치마크 이름(BBH, DROP, GSM8K 등)은 고유명사로 유지했습니다.
- 논문의 학술적 어조와 형식적 문체를 유지했습니다.
- 기술적 정확성을 위해 "활성화", "스칼라", "순전파", "역전파" 등 표준 한국어 머신러닝 용어를 사용했습니다.

### Confidence Level
High - 이 청크는 수학적 공식화와 시스템 최적화 기술에 대한 고도로 기술적인 내용을 포함하고 있습니다. 용어집의 사전 번역된 용어를 일관성 있게 사용했으며, 수학 표기법과 기술적 정확성을 완전히 보존했습니다. 한국어 머신러닝 학계의 표준 학술 용어를 따랐으며, 원문의 논리적 흐름과 기술적 깊이를 유지했습니다.

### Transition Notes
- Ending: 이 청크는 실험 섹션(섹션 5)의 시작 부분으로 끝나며, 27B 모델의 학습 안정성 결과(그림 5)와 시스템 수준 벤치마크 성능(표 4)을 제시합니다. 표 4는 청크 끝에서 헤더만 보이고 데이터 행은 다음 청크로 계속됩니다. 다음 번역자는 표 4의 데이터 행과 벤치마크별 성능 수치를 기대할 수 있습니다.
