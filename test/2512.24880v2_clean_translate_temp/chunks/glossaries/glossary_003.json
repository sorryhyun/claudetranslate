{
  "version": "1.0",
  "chunk_index": 3,
  "translated": true,
  "target_language": "korean",
  "terms": [
    {
      "source": "RMSNorm",
      "translation": "RMSNorm",
      "context": "Used in Section 4.2 to normalize flattened hidden state before computing dynamic mappings; reordered in kernel fusion optimization to follow matrix multiplication",
      "category": "technical",
      "notes": "Acronym (Root Mean Square Normalization) is standard in Korean ML literature; kept as-is following context analysis guidance"
    },
    {
      "source": "Sinkhorn-Knopp operator",
      "translation": "싱크혼-크놉 연산자",
      "context": "Iterative normalization algorithm applied to H_l^res that makes all elements positive and conducts alternating row/column rescaling to create doubly stochastic matrix with t_max=20 iterations",
      "category": "technical",
      "notes": "Transliteration of algorithm name (Sinkhorn-Knopp) + Korean translation of 'operator' (연산자). Standard mathematical terminology."
    },
    {
      "source": "doubly stochastic matrix",
      "translation": "이중 확률 행렬",
      "context": "Result of Sinkhorn-Knopp algorithm where rows and columns sum to 1; used for constrained residual mappings",
      "category": "technical",
      "notes": "Established Korean term in linear algebra and mathematics literature. 이중(doubly) + 확률(stochastic) + 행렬(matrix)"
    },
    {
      "source": "kernel fusion",
      "translation": "커널 융합",
      "context": "Infrastructure optimization technique that combines multiple operations (RMSNorm, matrix multiplication, biases) into unified compute kernels to reduce memory bandwidth bottlenecks",
      "category": "technical",
      "notes": "Following context analysis recommendation. 커널(kernel) + 융합(fusion)"
    },
    {
      "source": "recomputing",
      "translation": "재계산",
      "context": "Memory optimization strategy in Section 4.3.2 that discards intermediate activations after forward pass and recomputes them during backward pass to reduce training memory overhead",
      "category": "technical",
      "notes": "Context analysis recommends '재계산' for recomputing. Standard Korean term for this memory optimization technique."
    },
    {
      "source": "DualPipe schedule",
      "translation": "듀얼파이프 스케줄",
      "context": "Pipeline parallelism framework extended in Section 4.3.3 to handle overlapping of communication and computation at pipeline stage boundaries for n-stream residual design",
      "category": "technical",
      "notes": "DualPipe is a proper name (framework name) - transliterated as 듀얼파이프; schedule translated as 스케줄"
    },
    {
      "source": "TileLang",
      "translation": "TileLang",
      "context": "Framework mentioned in Section 4.3.1 used to implement complex GPU kernels with efficient memory bandwidth utilization",
      "category": "proper_noun",
      "notes": "Proper noun - framework name kept as-is following context analysis guidance and standard practice for proprietary framework names"
    },
    {
      "source": "pipeline parallelism",
      "translation": "파이프라인 병렬화",
      "context": "Standard practice in large-scale training mentioned in Section 4.3.3 for distributing model layers across multiple stages; requires recomputation blocks to respect stage boundaries",
      "category": "technical",
      "notes": "Following context analysis recommendation. 파이프라인(pipeline) + 병렬화(parallelism/parallelization)"
    },
    {
      "source": "mixed-precision processing",
      "translation": "혼합 정밀도 처리",
      "context": "Strategy employed in kernel implementation to maximize numerical accuracy without compromising computational speed; uses tfloat32 and bfloat16 data types",
      "category": "technical",
      "notes": "혼합(mixed) + 정밀도(precision) + 처리(processing). Standard ML terminology in Korean for mixed-precision computation techniques."
    }
  ]
}
