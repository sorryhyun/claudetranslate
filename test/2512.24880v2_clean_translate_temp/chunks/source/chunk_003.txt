8

the mixing of information across streams monotonically, effectively functioning as a robust feature fusion mechanism.

Additionally, we impose non-negativity constraints on the input mappings H _ğ‘™_ [pre] and output mappings H _ğ‘™_ [post] . This constrain prevents signal cancellation arising from the composition of positive and negative coefficients, which can also be considered as a special manifold projection.

**4.2. Parameterization and Manifold Projection**

In this section, we detail the calculation process of H _ğ‘™_ [pre], H _ğ‘™_ [post], and H _ğ‘™_ [res] in _m_ HC. Given the input hidden matrix **x** _ğ‘™_ âˆˆ **R** _[ğ‘›]_ [Ã—] _[ğ¶]_ at the _ğ‘™_ -th layer, we first flatten it into a vector ï¿½ **x** _ğ‘™_ = vec( **x** _ğ‘™_ ) âˆˆ **R** [1][Ã—] _[ğ‘›ğ¶]_

to preserve full context information. Then, we follow the original HC formulation to get the dynamic mappings and the static mappings as follows:

**x**  - [â€²] _ğ‘™_ [=][ RMSNorm][(ï¿½] **[x]** _[ğ‘™]_ [)]

ï£±ï£´ï£´ï£´ï£´ï£´ï£´ï£²HËœ _ğ‘™_ [pre] = _ğ›¼_ [pre] _ğ‘™_ - (ï¿½ **x** [â€²] _ğ‘™_ _[ğœ‘]_ [pre] _ğ‘™_ ) + **b** [pre] _ğ‘™_

HËœ _ğ‘™_ [post] = _ğ›¼_ [post] _ğ‘™_  - (ï¿½ **x** [â€²] _ğ‘™_ _[ğœ‘]_ [post] _ğ‘™_ ) + **b** [post] _ğ‘™_
HËœ _ğ‘™_ [res] = _ğ›¼_ [res] _ğ‘™_  - mat(ï¿½ **x** [â€²] _ğ‘™_ _[ğœ‘]_ [res] _ğ‘™_ [) +] **[ b]** [res] _ğ‘™_ [,]

ï£´ï£´ï£´ï£´ï£´ï£´ï£³

(7)

where _ğœ‘_ [pre] _ğ‘™_, _ğœ‘_ [post] _ğ‘™_ âˆˆ **R** _[ğ‘›ğ¶]_ [Ã—] _[ğ‘›]_ and _ğœ‘_ [res] _ğ‘™_ âˆˆ **R** _[ğ‘›ğ¶]_ [Ã—] _[ğ‘›]_ [2] are linear projections for dynamic mappings and mat(Â·) is a reshape function from **R** [1][Ã—] _[ğ‘›]_ [2] to **R** _[ğ‘›]_ [Ã—] _[ğ‘›]_ .

Then, the final constrained mappings are obtained via:

H _ğ‘™_ [pre] = _ğœ_ (H [Ëœ] _ğ‘™_ [pre] )

ï£±ï£´ï£´ï£´ï£²

H _ğ‘™_ [post] = 2 _ğœ_ (H [Ëœ] _ğ‘™_ [post] )
H _ğ‘™_ [res] = Sinkhorn-Knopp(H [Ëœ] _ğ‘™_ [res] ),

ï£´ï£´ï£´ï£³

(8)

where _ğœ_ (Â·) denotes the Sigmoid function. The Sinkhorn-Knopp(Â·) operator firstly makes all elements to be positive via an exponent operator and then conducts iterative normalization process that alternately rescales rows and columns to sum to 1. Specifically, given a positive matrix **M** [(][0][)] = exp(H [Ëœ] _ğ‘™_ [res] ) as the start point, the normalization iteration proceeds as:

-                             **M** [(] _[ğ‘¡]_ [)] = T _ğ‘Ÿ_ T _ğ‘_ ( **M** [(] _[ğ‘¡]_ [âˆ’][1][)] ), (9)

where T _ğ‘Ÿ_ and T _ğ‘_ denote row and column normalization, respectively. This process converges to a doubly stochastic matrix H _ğ‘™_ [res] = **M** [(] _[ğ‘¡]_ [max] [)] as _ğ‘¡_ max â†’âˆ. We choose _ğ‘¡_ max = 20 as a practical value in our experiments.

**4.3. Efficient Infrastructure Design**

In this section, we detail the infrastructure design tailored for _m_ HC. Through rigorous optimization, we implement _m_ HC (with _ğ‘›_ = 4) in large-scale models with a marginal training overhead of only 6.7%.

_**4.3.1. Kernel Fusion**_

Observing that RMSNorm in _m_ HC imposes significant latency when operating on the highdimensional hidden state ï¿½ **x** _ğ‘™_ âˆˆ **R** [1][Ã—] _[ğ‘›ğ¶]_, we reorder the dividing-by-norm operation to follow the

9

matrix multiplication. This optimization maintains mathematical equivalence while improving efficiency. Furthermore, we employ mixed-precision strategies to maximize numerical accuracy without compromising speed, and fuse multiple operations with shared memory access into unified compute kernels to reduce memory bandwidth bottlenecks. Based on the inputs and parameters detailed in Eq. (10) to (13), we implement three specialized _m_ HC kernels to compute
H _ğ‘™_ [pre], H _ğ‘™_ [post], and H _ğ‘™_ [res] . In these kernels, the biases and linear projections are consolidated into **b** _ğ‘™_ and _ğœ‘ğ‘™_, and the RMSNorm weight is also absorbed in _ğœ‘ğ‘™_ .

- Eq. (14) to (15): We develop a unified kernel that fuses two scans on ï¿½ **x** _ğ‘™_, leveraging matrix multiplication units to maximize memory bandwidth utilization. The backward passâ€”comprising two matrix multiplicationsâ€”is similarly consolidated into a single kernel, eliminating redundant reloading of ï¿½ **x** _ğ‘™_ . Both kernels feature a finely tuned pipeline (load, cast, compute, store) to efficiently handle mixed-precision processing.

- Eq. (16) to (18): These lightweight operations on small coefficients are opportunistically fused into a single kernel, significantly reducing kernel launch overhead.

- Eq. (19): We implement the Sinkhorn-Knopp iteration within a single kernel. For the backward pass, we derive a custom backward kernel that recomputes the intermediate results on-chip and traverses the entire iteration.

_ğœ‘ğ‘™_ : tfloat32 [ _ğ‘›ğ¶_, _ğ‘›_ [2] + 2 _ğ‘›_ ] (10)
**x**            - _ğ‘™_ : bfloat16 [1, _ğ‘›ğ¶_ ] (11)

_ğ›¼_ [pre] _ğ‘™_, _ğ›¼_ [post] _ğ‘™_, _ğ›¼_ [res] _ğ‘™_ : float32 Scalars (12)

**b** _ğ‘™_ : float32 [1, _ğ‘›_ [2] + 2 _ğ‘›_ ] (13)

- pre      HËœËœ _ğ‘™_, H [ËœËœ] _ğ‘™_ [post], H [ËœËœ] _ğ‘™_ [res] : float32 = ï¿½ **x** _ğ‘™_ _ğœ‘ğ‘™_ (14)

~~âˆš~~
_ğ‘Ÿ_ : float32 = ï¿½ï¿½ **x**                - _ğ‘™_ ï¿½ï¿½2 [/] _ğ‘›ğ¶_ (15)

ï¿½HËœ _ğ‘™_ [pre], H [Ëœ] _ğ‘™_ [post], H [Ëœ] _ğ‘™_ [res]     - : float32 = 1/ _ğ‘Ÿ_     - _ğ›¼_ [pre] _ğ‘™_ HËœËœ _ğ‘™_ [pre], _ğ›¼_ [post] _ğ‘™_ HËœËœ _ğ‘™_ [post], _ğ›¼_ [res] _ğ‘™_ HËœËœ _ğ‘™_ resï¿½ + **b** _ğ‘™_ (16)

-                               H _ğ‘™_ [pre] : float32 = _ğœ_ HËœ _ğ‘™_ [pre] (17)

-                                H _ğ‘™_ [post] : float32 = 2 _ğœ_ HËœ _ğ‘™_ [post] (18)

H _ğ‘™_ [res] : float32 = Sinkhorn-Knopp H _ğ‘™_ [res]      - (19)

[ï¿½] [Ëœ]

Using the coefficients derived from the aforementioned kernels, we introduce two additional kernels to apply these mappings: one for Fpre â‰” H _ğ‘™_ [pre] **x** _ğ‘™_ and another for Fpost,res â‰”
H _ğ‘™_ [res] **x** _ğ‘™_ + H _ğ‘™_ [post][ âŠ¤] F (Â·, Â·). Through fusing the application of H _ğ‘™_ [post] and H _ğ‘™_ [res] with residual merging, we reduce the number of elements read from (3 _ğ‘›_ + 1) _ğ¶_ to ( _ğ‘›_ + 1) _ğ¶_ and the number of elements written from 3 _ğ‘›ğ¶_ to _ğ‘›ğ¶_ for this kernel. We efficiently implement the majority of kernels (excluding Eq. (14) to (15)) using TileLang (Wang et al., 2025). This framework streamlines the implementation of kernels with complex calculation process and allows us to fully utilize the memory bandwidth with minimal engineering effort.

_**4.3.2. Recomputing**_

The _ğ‘›_ -stream residual design introduces substantial memory overhead during training. To mitigate this, we discard the intermediate activations of the _m_ HC kernels after the forward pass and recompute them on-the-fly in the backward pass, through re-executing the _m_ HC kernels

10

without the heavy layer function F . Consequently, for a block of _ğ¿ğ‘Ÿ_ consecutive layers, we need only store the input **x** _ğ‘™_ 0 to the first layer. Excluding lightweight coefficients while accounting for the pre-norm with in F, Tab. 3 summarizes the intermediate activations preserved for the backward pass.

Table 3 | **Stored and Recomputed Intermediate Activations** We list per token activation preserved for the backward pass and the transient activation recomputed in _ğ¿ğ‘Ÿ_ consecutive layers.
Layer _ğ‘™_ 0 represents the first layer in _ğ¿ğ‘Ÿ_ layers and layer _ğ‘™_ is in [ _ğ‘™_ 0, _ğ‘™_ 0 + _ğ¿ğ‘Ÿ_ - 1].

Activations **x** _ğ‘™_ 0 F (H _ğ‘™_ [pre] **x** _ğ‘™_, W _ğ‘™_ ) **x** _ğ‘™_ H _ğ‘™_ [pre] **x** _ğ‘™_ RMSNorm(H _ğ‘™_ [pre] **x** _ğ‘™_ )

Size (Elements) _ğ‘›ğ¶_ _ğ¶_ _ğ‘›ğ¶_ _ğ¶_ _ğ¶_
Stored Method Every _ğ¿ğ‘Ÿ_ layers Every layer Transient inside _ğ¿ğ‘Ÿ_ layers

Since _m_ HC kernels recomputation is performed for blocks of _ğ¿ğ‘Ÿ_ consecutive layers, given a total of _ğ¿_ layers, we must persistently store the first layer input **x** _ğ‘™_ 0 for all âŒˆ _ğ¿_ _[ğ¿]_ _ğ‘Ÿ_ [âŒ‰] [blocks for the]

backward pass. In addition to this resident memory, the recomputation process introduces a transient memory overhead of ( _ğ‘›_ + 2) _ğ¶_ Ã— _ğ¿ğ‘Ÿ_ elements for the active block, which determines the peak memory usage during backpropagation. Consequently, we determine the optimal block size _ğ¿_ [âˆ—] _ğ‘Ÿ_ [by minimizing the total memory footprint corresponded to] _[ ğ¿][ğ‘Ÿ]_ [:]

_ğ‘›ğ¿_ (20)
_ğ‘›_ + 2 [.]

~~âˆš~~

â‰ˆ

- _ğ¿_
_ğ‘›ğ¶_ Ã—

_ğ¿ğ‘Ÿ_

+ ( _ğ‘›_ + 2) _ğ¶_ Ã— _ğ¿ğ‘Ÿ_

_ğ¿_ [âˆ—] _ğ‘Ÿ_ [=][ arg min]
_ğ¿ğ‘Ÿ_

Furthermore, pipeline parallelism in large-scale training imposes a constraint: recomputation blocks must not cross pipeline stage boundaries. Observing that the theoretical optimum _ğ¿_ [âˆ—] _ğ‘Ÿ_ typically aligns with the number of layers per pipeline stage, we choose to synchronize the recomputation boundaries with the pipeline stages.

_**4.3.3. Overlapping Communication in DualPipe**_

In large-scale training, pipeline parallelism is the standard practice for mitigating parameter and gradient memory footprints. Specifically, we adopt the DualPipe schedule (Liu et al., 2024b), which effectively overlaps scale-out interconnected communication traffic, such as those in expert and pipeline parallelism. However, compared to the single-stream design, the proposed
_ğ‘›_ -stream residual in _m_ HC incurs substantial communication latency across pipeline stages.
Furthermore, at stage boundaries, the recomputation of _m_ HC kernels for all _ğ¿ğ‘Ÿ_ layers introduces non-negligible computational overhead. To address these bottlenecks, we extend the DualPipe schedule (see Fig. 4) to facilitate improved overlapping of communication and computation at pipeline stage boundaries.

Notably, to prevent blocking the communication stream, we execute the Fpost,res kernels of MLP (i.e. FFN) layers on a dedicated high-priority compute stream. We further refrain from employing persistent kernels for long-running operations in attention layers, thereby preventing extended stalls. This design enables the preemption of overlapped attention computations, allowing for flexible scheduling while maintaining high utilization of the compute deviceâ€™s processing units. Furthermore, the recomputation process is decoupled from pipeline communication dependencies, as the initial activation of each stage **x** _ğ‘™_ 0 is already cached locally.

11

Normal Compute Stream

Communication Stream

High Priority Compute Stream

!"#$, '(#) (F) â„±!"#$, '(#)

â„±!"#$, '(#)

) (B)

Figure 4 | **Communication-Computation Overlapping for** _**m**_ **HC.** We extend the DualPipe
schedule to handle the overhead introduced by _m_ HC. Lengths of each block are illustrative only and do not represent actual duration. (F), (B), (W) refers to forward pass, backward pass, weight gradient computation, respectively. F [A] and F [M] represents kernels corresponded to Attention and MLP, respectively.

#### **5. Experiments**

**5.1. Experimental Setup**

We validate the proposed method via language model pre-training, conducting a comparative analysis between the baseline, HC, and our proposed _m_ HC. Utilizing MoE architectures inspired by DeepSeek-V3 (Liu et al., 2024b), we train four distinct model variants to cover different evaluation regimes. Specifically, the expansion rate _ğ‘›_ for both HC and _m_ HC is set to 4. Our primary focus is a 27B model trained with a dataset size proportional to its parameters, which serves as the subject for our system-level main results. Expanding on this, we analyze the compute scaling behavior by incorporating smaller 3B and 9B models trained with proportional data, which allows us to observe performance trends across varying compute. Additionally, to specifically investigate the token scaling behavior, we train a separate 3B model on a fixed corpus of 1 trillion tokens. Detailed model configurations and training hyper-parameters are provided in Appendix A.1.

**5.2. Main Results**

Figure 5 | **Training Stability of Manifold-Constrained Hyper-Connections (** _**m**_ **HC).** This figure
illustrates (a) the absolute training loss gap of _m_ HC and HC relative to the baseline, and (b) the gradient norm of the three methods. All experiments utilize the 27B model. The results demonstrate that _m_ HC exhibits improved stability in terms of both loss and gradient norm.

We begin by examining the training stability and convergence of the 27B models. As illustrated in Fig. 5 (a), _m_ HC effectively mitigates the training instability observed in HC, achieving a final loss reduction of 0.021 compared to the baseline. This improved stability is further corroborated by the gradient norm analysis in Fig. 5 (b), where _m_ HC exhibits significantly better behavior than HC, maintaining a stable profile comparable to the baseline.

12

Table 4 | **System-level Benchmark Results for 27B Models.** This table compares the zeroshot and few-shot performance of the Baseline, HC, and _m_ HC across 8 diverse downstream
benchmarks. _m_ HC consistently outperforms the Baseline and surpasses HC on the majority of benchmarks, demonstrating its effectiveness in large-scale pre-training.

**Benchmark** BBH DROP GSM8K HellaSwag MATH MMLU PIQA TriviaQA
**(Metric)** (EM) (F1) (EM) (Acc.) (EM) (Acc.) (Acc.) (EM)