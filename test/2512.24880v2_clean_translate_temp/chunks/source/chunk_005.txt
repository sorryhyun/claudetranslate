L. Wang, H. Gao, C. Zhao, X. Sun, and D. Dai. Auxiliary-loss-free load balancing strategy for mixture-of-experts. arXiv preprint arXiv:2408.15664, 2024.

L. Wang, Y. Cheng, Y. Shi, Z. Tang, Z. Mo, W. Xie, L. Ma, Y. Xia, J. Xue, F. Yang, et al. Tilelang: A composable tiled programming model for ai systems. arXiv preprint arXiv:2504.17577, 2025.

D. Xiao, Q. Meng, S. Li, and X. Yuan. Muddformer: Breaking residual bottlenecks in transformers via multiway dynamic dense connections. arXiv preprint arXiv:2502.12170, 2025.

S. Xie, R. Girshick, P. Doll√°r, Z. Tu, and K. He. Aggregated residual transformations for deep neural networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1492‚Äì1500, 2017.

S. Xie, H. Zhang, J. Guo, X. Tan, J. Bian, H. H. Awadalla, A. Menezes, T. Qin, and R. Yan. Residual:
Transformer with dual residual connections, 2023. URL `[https://arxiv.org/abs/2304.1](https://arxiv.org/abs/2304.14802)`
`[4802](https://arxiv.org/abs/2304.14802)` .

F. Yu, D. Wang, E. Shelhamer, and T. Darrell. Deep layer aggregation. In Proceedings of the

IEEE conference on computer vision and pattern recognition, pages 2403‚Äì2412, 2018.

R. Zellers, A. Holtzman, Y. Bisk, A. Farhadi, and Y. Choi. HellaSwag: Can a machine really finish your sentence? In A. Korhonen, D. R. Traum, and L. M√†rquez, editors, Proceedings of the 57th
Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July
28- August 2, 2019, Volume 1: Long Papers, pages 4791‚Äì4800. Association for Computational
Linguistics, 2019. doi: 10.18653/v1/p19-1472. URL `[https://doi.org/10.18653/v1/p1](https://doi.org/10.18653/v1/p19-1472)`
`[9-1472](https://doi.org/10.18653/v1/p19-1472)` .

B. Zhang and R. Sennrich. Root mean square layer normalization. Advances in neural information processing systems, 32, 2019.

D. Zhu, H. Huang, Z. Huang, Y. Zeng, Y. Mao, B. Wu, Q. Min, and X. Zhou. Hyper-connections.

arXiv preprint arXiv:2409.19606, 2024.

18

#### **A. Appendix**

**A.1. Detailed Model Specifications and Hyper-parameters.**

Table 5 | **Detailed Model Specifications and Hyper-parameters.** This table presents the architectural configurations for the 3B, 9B, and 27B models based on the DeepSeek-V3 (Liu et al., 2024b)
architecture. It outlines the specific hyper-parameters for _m_ HC and HC, including the residual stream expansion and Sinkhorn-Knopp settings, alongside the optimization and training protocols used in the experiments.

3B
**Attribute** 3B 9B 27B
1T Tokens

Vocab Params 331M 496M 662M 331M
Active Params 612M 1.66B 4.14B 612M
Total Params 2.97B 9.18B 27.0B 2.97B

Layers 12 18 30 12
Leading Dense Layers 1 1
Routed Experts 64 64 72 64
Active Experts 6 6
Shared Experts 2 2
Dimension 1280 1920 2560 1280
FFN Dimension 896 1280 1536 896
Load Balancing Method Loss-Free (Wang et al., 2024) Loss-Free
Attention Heads 16 24 32 16
Attention Dimension 128 128
Attention Variant MLA (Liu et al., 2024a) MLA
KV Rank 512 512
Position Embedding RoPE (Su et al., 2024) RoPE
RoPE Dimension 64 64
RoPE _ùúÉ_ 10000 10000
Layer Norm Type RMSNorm (Zhang and Sennrich, 2019) RMSNorm
Layer Norm _ùúÄ_ 1e-20 1e-20

_m_ HC/HC Expansion Rate _ùëõ_ 4 4
_m_ HC/HC Gating Factor Init _ùõº_ 0.01 0.01
_m_ HC Sinkhorn-Knopp _ùë°_ max 20 20

Sequence Length 4096 4096
Vocab Size 129280 129280
Batch Size 320 512 1280 2560
Training Steps 30000 50000 50000 100000
Training Tokens 39.3B 105B 262B 1.05T
Warmup Steps 2000 2000
Optimizer AdamW (Loshchilov and Hutter, 2017) AdamW
AdamW Betas (0.9, 0.95) (0.9, 0.95)
AdamW _ùúÄ_ 1e-20 1e-20
Base Learning Rate 8.6e-4 5.9e-4 4.0e-4 9.0e-4
Lr Scheduler Step Step
Lr Decay Step Ratio [0.8 √ó, 0.9 √ó] [0.8 √ó, 0.9 √ó]
Lr Decay Rate [0.316, 0.1] [0.316, 0.1]
Weight Decay 0.1 0.1

19

<!-- Extraction Warnings:
  - Detected potential broken math subscripts/superscripts. PDF may use custom fonts for mathematical notation.
  - Found 39 characters from Unicode Private Use Area. These may be custom glyphs that didn't convert properly.
  - Document appears to contain mathematical equations. Consider using OCR-based tools for better equation extraction.
-->