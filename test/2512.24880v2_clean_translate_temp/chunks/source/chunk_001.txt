## **m HC: Manifold-Constrained Hyper-Connections**

Zhenda Xie* [â€ ], Yixuan Wei*, Huanqi Cao*, Chenggang Zhao, Chengqi Deng, Jiashi Li, Damai Dai, Huazuo Gao, Jiang Chang, Kuai Yu, Liang Zhao, Shangyan Zhou, Zhean Xu, Zhengyan Zhang, Wangding Zeng, Shengding Hu, Yuqing Wang, Jingyang Yuan, Lean Wang, Wenfeng Liang

#### **DeepSeek-AI**

### **Abstract**

Recently, studies exemplified by Hyper-Connections (HC) have extended the ubiquitous residual connection paradigm established over the past decade by expanding the residual stream width and diversifying connectivity patterns. While yielding substantial performance gains, this diversification fundamentally compromises the identity mapping property intrinsic to the residual connection, which causes severe training instability and restricted scalability, and additionally incurs notable memory access overhead. To address these challenges, we propose **Manifold-Constrained Hyper-Connections** ( _**m**_ **HC** ), a general framework that projects the residual connection space of HC onto a specific manifold to restore the identity mapping property, while incorporating rigorous infrastructure optimization to ensure efficiency. Empirical experiments demonstrate that _m_ HC is effective for training at scale, offering tangible performance improvements and superior scalability. We anticipate that _m_ HC, as a flexible and practical extension of HC, will contribute to a deeper understanding of topological architecture design and suggest promising directions for the evolution of foundational models.

(a) Residual Connection (b) Hyper-Connections (HC) (c) Manifold-Constrained HC ( _m_ HC)

Figure 1 | **Illustrations of Residual Connection Paradigms.** This figure compares the structural
design of (a) standard Residual Connection, (b) Hyper-Connections (HC), and (c) our proposed
**Manifold-Constrained Hyper-Connections** ( _**m**_ **HC** ). Unlike the unconstrained HC, _m_ HC focuses on optimizing the residual connection space by projecting the matrices onto a constrained manifold to ensure stability.

*Core contributors. [â€ ] [Corresponding author: xie.zhenda@deepseek.com](mailto:xie.zhenda@deepseek.com)

#### **Contents**

**1** **Introduction** **3**

**2** **Related Works** **4**

2.1 Micro Design . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4

2.2 Macro Design . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5

**3** **Preliminary** **5**

3.1 Numerical Instability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6

3.2 System Overhead . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7

**4** **Method** **8**

4.1 Manifold-Constrained Hyper-Connections . . . . . . . . . . . . . . . . . . . . . . 8

4.2 Parameterization and Manifold Projection . . . . . . . . . . . . . . . . . . . . . . . 9

4.3 Efficient Infrastructure Design . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9

4.3.1 Kernel Fusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9

4.3.2 Recomputing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10

4.3.3 Overlapping Communication in DualPipe . . . . . . . . . . . . . . . . . . 11

**5** **Experiments** **12**

5.1 Experimental Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12

5.2 Main Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12

5.3 Scaling Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13

5.4 Stability Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14

**6** **Conclusion and Outlook** **15**

**A Appendix** **19**

A.1 Detailed Model Specifications and Hyper-parameters. . . . . . . . . . . . . . . . . 19

2

#### **1. Introduction**

Deep neural network architectures have undergone rapid evolution since the introduction of
ResNets (He et al., 2016a). As illustrated in Fig. 1(a), the structure of a single-layer can be formulated as follows:
**x** _ğ‘™_ +1 = **x** _ğ‘™_ + F ( **x** _ğ‘™_, W _ğ‘™_ ), (1)

where **x** _ğ‘™_ and **x** _ğ‘™_ +1 denote the _ğ¶_ -dimensional input and output of the _ğ‘™_ -th layer, respectively, and F represents the residual function. Although the residual function F has evolved over the past decade to include various operations such as convolution, attention mechanisms, and feed forward networks, the paradigm of the residual connection has maintained its original form. Accompanying the progression of Transformer (Vaswani et al., 2017) architecture, this paradigm has currently established itself as a fundamental design element in large language models (LLMs) (Brown et al., 2020; Liu et al., 2024b; Touvron et al., 2023).

This success is primarily attributed to the concise form of the residual connection. More importantly, early research (He et al., 2016b) revealed that the identity mapping property of the residual connection maintains stability and efficiency during large-scale training. By recursively extending the residual connection across multiple layers, Eq. (1) yields:

**x** _ğ¿_ = **x** _ğ‘™_ +

_ğ¿_ âˆ’1
âˆ‘ï¸

F ( **x** _ğ‘–_, W _ğ‘–_ ), (2)

_ğ‘–_ = _ğ‘™_

where _ğ¿_ and _ğ‘™_ correspond to deeper and shallower layers, respectively. The term identity mapping refers to the component **x** _ğ‘™_ itself, which emphasizes the property that the signal from the shallower layer maps directly to the deeper layer without any modification.

Recently, studies exemplified by Hyper-Connections (HC) (Zhu et al., 2024) have introduced a new dimension to the residual connection and empirically demonstrated its performance potential. The single-layer architecture of HC is illustrated in Fig. 1(b). By expanding the width of the residual stream and enhancing connection complexity, HC significantly increases topological complexity without altering the computational overhead of individual units regarding FLOPs.
Formally, single-layer propagation in HC is defined as:

**x** _ğ‘™_ +1 = H _ğ‘™_ [res] **x** _ğ‘™_ + H _ğ‘™_ [post][ âŠ¤] F (H _ğ‘™_ [pre] **x** _ğ‘™_, W _ğ‘™_ ), (3)

where **x** _ğ‘™_ and **x** _ğ‘™_ +1 denote the input and output of the _ğ‘™_ -th layer, respectively. Unlike the formulation in Eq. (1), the feature dimension of **x** _ğ‘™_ and **x** _ğ‘™_ +1 is expanded from _ğ¶_ to _ğ‘›_ Ã— _ğ¶_, where _ğ‘›_ is the expansion rate. The term H _ğ‘™_ [res] âˆˆ **R** _[ğ‘›]_ [Ã—] _[ğ‘›]_ represents a learnable mapping that mixes features within the residual stream. Also as a learnable mapping, H _ğ‘™_ [pre] âˆˆ **R** [1][Ã—] _[ğ‘›]_ aggregates features from the _ğ‘›ğ¶_ -dim stream into a _ğ¶_ -dim layer input, and conversely, H _ğ‘™_ [post] âˆˆ **R** [1][Ã—] _[ğ‘›]_ maps the layer output back onto the stream.

However, as the training scale increases, HC introduces potential risks of instability. The primary concern is that the unconstrained nature of HC compromises the identity mapping property when the architecture extends across multiple layers. In architectures comprising multiple parallel streams, an ideal identity mapping serves as a conservation mechanism. It ensures that the average signal intensity across streams remains invariant during both forward and backward propagation. Recursively extending HC to multiple layers via Eq. (3) yields:

ï¿½ï¿½

**x** _ğ¿_ =

- _ğ¿_ - _ğ‘™_

H _ğ¿_ [res]  - _ğ‘–_
_ğ‘–_ =1

**x** _ğ‘™_ +

_ğ¿_ âˆ’1
âˆ‘ï¸

_ğ‘–_ = _ğ‘™_

_ğ¿_ âˆ’1âˆ’ _ğ‘–_

H _ğ¿_ [res]  - _ğ‘—_ ï¿½ï¿½ H _ğ‘–_ [post][ âŠ¤] F (H _ğ‘–_ [pre] **x** _ğ‘–_, W _ğ‘–_ ), (4)
_ğ‘—_ =1

3

where _ğ¿_ and _ğ‘™_ represent a deeper layer and a shallower layer, respectively. In contrast to Eq. (2), the composite mapping [ï¿½] _ğ‘–_ _[ğ¿]_ = [âˆ’] 1 _[ğ‘™]_ [H] _ğ¿_ [res] - _ğ‘–_ [in HC fails to preserve the global mean of the features. This] discrepancy leads to unbounded signal amplification or attenuation, resulting in instability during large-scale training. A further consideration is that, while HC preserves computational efficiency in terms of FLOPs, the hardware efficiency concerning memory access costs for the widened residual stream remains unaddressed in the original design. These factors collectively restrict the practical scalability of HC and hinder its application in large-scale training.

To address these challenges, we propose **Manifold-Constrained Hyper-Connections** ( _**m**_ **HC** ), as shown in Fig. 1(c), a general framework that projects the residual connection space of HC onto a specific manifold to restore the identity mapping property, while incorporating rigorous infrastructure optimization to ensure efficiency. Specifically, _m_ HC utilizes the Sinkhorn-Knopp algorithm (Sinkhorn and Knopp, 1967) to entropically project H _ğ‘™_ [res] onto the Birkhoff polytope.
This operation effectively constrains the residual connection matrices within the manifold that is constituted by doubly stochastic matrices. Since the row and column sums of these matrices equal to 1, the operation H _ğ‘™_ [res] **x** _ğ‘™_ functions as a convex combination of the input features.
This characteristic facilitates a well-conditioned signal propagation where the feature mean is conserved, and the signal norm is strictly regularized, effectively mitigating the risk of vanishing or exploding signals. Furthermore, due to the closure of matrix multiplication for doubly stochastic matrices, the composite mapping [ï¿½] _ğ‘–_ _[ğ¿]_ = [âˆ’] 1 _[ğ‘™]_ [H] _ğ¿_ [res] - _ğ‘–_ [retains this conservation property.]
Consequently, _m_ HC effectively maintains the stability of identity mappings between arbitrary depths. To ensure efficiency, we employ kernel fusion and develop mixed precision kernels utilizing TileLang (Wang et al., 2025). Furthermore, we mitigate the memory footprint through selective recomputing and carefully overlap communication within the DualPipe schedule (Liu et al., 2024b).

Extensive experiments on language model pretraining demonstrate that _m_ HC exhibits exceptional stability and scalability while maintaining the performance advantages of HC. Inhouse large-scale training indicates that _m_ HC supports training at scale and introduces only a
6.7% additional time overhead when expansion rate _ğ‘›_ = 4.

#### **2. Related Works**

Architectural advancements in deep learning can be primarily classified into _micro-design_ and
_macro-design_ . Micro-design concerns the internal architecture of computational blocks, specifying how features are processed across spatial, temporal, and channel dimensions. In contrast, macro-design establishes the inter-block topological structure, thereby dictating how feature representations are propagated, routed, and merged across distinct layers.

**2.1. Micro Design**

Driven by parameter sharing and translation invariance, convolution initially dominated the processing of structured signals. While subsequent variations such as depthwise separable (Chollet, 2017) and grouped convolutions (Xie et al., 2017) optimized efficiency, the advent of Transformers (Vaswani et al., 2017) established Attention and Feed-Forward Networks (FFNs) as the fundamental building blocks of modern architecture. Attention mechanisms facilitate global information propagation, while FFNs enhance the representational capacity of individual features. To balance performance with the computational demands of LLMs, attention mechanisms have evolved towards efficient variants such as Multi-Query Attention (MQA) (Shazeer, 2019), Grouped-Query Attention (GQA) (Ainslie et al., 2023), and Multi-Head Latent Attention

4

(MLA) (Liu et al., 2024a). Simultaneously, FFNs have been generalized into sparse computing paradigms via Mixture-of-Experts (MoE) (Fedus et al., 2022; Lepikhin et al., 2020; Shazeer et al., 2017), allowing for massive parameter scaling without proportional computational costs.