**2.2. Macro Design**

Macro-design governs the global topology of the network (Srivastava et al., 2015). Following
ResNet (He et al., 2016a), architectures such as DenseNet (Huang et al., 2017) and FractalNet (Larsson et al., 2016) aimed to enhance performance by increasing topological complexity through dense connectivity and multi-path structures, respectively. Deep Layer Aggregation (DLA) (Yu et al., 2018) further extended this paradigm by recursively aggregating features across various depths and resolutions.

More recently, the focus of macro-design has shifted toward expanding the width of the residual stream (Chai et al., 2020; Fang et al., 2023; Heddes et al., 2025; Mak and Flanigan, 2025; Menghani et al., 2025; Pagliardini et al., 2024; Xiao et al., 2025; Xie et al., 2023; Zhu et al., 2024). Hyper-Connections (HC) (Zhu et al., 2024) introduced learnable matrices to modulate connection strengths among features at varying depths, while the Residual Matrix Transformer (RMT) (Mak and Flanigan, 2025) replaced the standard residual stream with an outer-product memory matrix to facilitate feature storage. Similarly, MUDDFormer (Xiao et al., 2025) employs multiway dynamic dense connections to optimize cross-layer information flow. Despite their potential, these approaches compromise the inherent identity mapping property of the residual connection, thereby introducing instability and hindering scalability. Furthermore, they incur significant memory access overhead due to expanded feature widths. Building upon HC, the proposed _m_ HC restricts the residual connection space onto a specific manifold to restore the identity mapping property, while also incorporating rigorous infrastructure optimizations to ensure efficiency. This approach enhances stability and scalability while maintaining the topological benefits of expanded connections.

#### **3. Preliminary**

We first establish the notation used in this work. In the HC formulation, the input to the _ğ‘™_ -th layer, **x** _ğ‘™_ âˆˆ **R** [1][Ã—] _[ğ¶]_, is expanded by a factor of _ğ‘›_ to construct a hidden matrix **x** _ğ‘™_ = ( **x** [âŠ¤] _ğ‘™_,0 [,] _[ . . .]_ [,] **[ x]** [âŠ¤] _ğ‘™_, _ğ‘›_ âˆ’1 [)][âŠ¤] [âˆˆ] **[R]** _[ğ‘›]_ [Ã—] _[ğ¶]_

which can be viewed as _ğ‘›_ -stream residual. This operation effectively broadens the width of the residual stream. To govern the read-out, write-in, and updating processes of this stream, HC introduces three learnable linear mappingsâ€”H _ğ‘™_ [pre], H _ğ‘™_ [post] âˆˆ **R** [1][Ã—] _[ğ‘›]_, and H _ğ‘™_ [res] âˆˆ **R** _[ğ‘›]_ [Ã—] _[ğ‘›]_ . These mappings modify the standard residual connection shown in Eq. (1), resulting in the formulation given in Eq. (3).

In the HC formulation, learnable mappings are composed of two parts of coefficients: the input-dependent one and the global one, referred to as dynamic mappings and static mappings, respectively. Formally, HC computes the coefficients as follows:

ï£±ï£´ï£´ï£´ï£´ï£´ï£´ï£²
ï£´ï£´ï£´ï£´ï£´ï£´ï£³

**x** Ëœ _ğ‘™_ = RMSNorm( **x** _ğ‘™_ )
H _ğ‘™_ [pre] = _ğ›¼_ [pre] _ğ‘™_ - tanh( _ğœƒğ‘™_ [pre] **x** Ëœ [âŠ¤] _ğ‘™_ [) +] **[ b]** [pre] _ğ‘™_
H _ğ‘™_ [post] = _ğ›¼_ [post] _ğ‘™_ - tanh( _ğœƒğ‘™_ [post] **x** Ëœ [âŠ¤] _ğ‘™_ [) +] **[ b]** [post] _ğ‘™_
H _ğ‘™_ [res] = _ğ›¼_ [res] _ğ‘™_ - tanh( _ğœƒğ‘™_ [res] **x** Ëœ [âŠ¤] _ğ‘™_ [) +] **[ b]** [res] _ğ‘™_ [,]

(5)

where RMSNorm(Â·) (Zhang and Sennrich, 2019) is applied to the last dimension, and the scalars

_ğ›¼_ [pre] _ğ‘™_, _ğ›¼_ [post] _ğ‘™_ and _ğ›¼_ [res] _ğ‘™_ âˆˆ **R** are learnable gating factors initialized to small values. The dynamic

5

mappings are derived via linear projections parameterized by _ğœƒğ‘™_ [pre], _ğœƒğ‘™_ [post] âˆˆ **R** [1][Ã—] _[ğ¶]_ and _ğœƒğ‘™_ [res] âˆˆ **R** _[ğ‘›]_ [Ã—] _[ğ¶]_, while the static mappings are represented by learnable biases **b** [pre] _ğ‘™_, **b** [post] _ğ‘™_ âˆˆ **R** [1][Ã—] _[ğ‘›]_ and **b** [res] _ğ‘™_ âˆˆ **R** _[ğ‘›]_ [Ã—] _[ğ‘›]_ .

It is worth noting that the introduction of these mappingsâ€”H _ğ‘™_ [pre], H _ğ‘™_ [post], and H _ğ‘™_ [res] â€”incurs negligible computational overhead, as the typical expansion rate _ğ‘›_, e.g. 4, is much smaller than the input dimension _ğ¶_ . With this design, HC effectively decouples the information capacity of the residual stream from the layerâ€™s input dimension, which is strongly correlated with the modelâ€™s computational complexity (FLOPs). Consequently, HC offers a new avenue for scaling by adjusting the residual stream width, complementing the traditional scaling dimensions of model FLOPs and training data size discussed in pre-training scaling laws (Hoffmann et al., 2022).

Although HC necessitates three mappings to manage the dimensional mismatch between the residual stream and the layer input, preliminary experiments presented in Tab. 1 indicate that the residual mapping H _ğ‘™_ [res] yields the most significant performance gain. This finding underscores the critical importance of effective information exchange within the residual stream.

Table 1 | **Ablation Study of HC Components.** When a specific mapping (H _ğ‘™_ [pre], H _ğ‘™_ [post], or H _ğ‘™_ [res] ) is
disabled, we employ a fixed mapping to maintain dimensional consistency: uniform weights of
1/ _ğ‘›_ for H _ğ‘™_ [pre], uniform weights of ones for H _ğ‘™_ [post], and the identity matrix for H _ğ‘™_ [res] .

H _ğ‘™_ [res] H _ğ‘™_ [pre] H _ğ‘™_ [post] Absolute Loss Gap

0.0
âœ“         - 0.022
âœ“ âœ“         - 0.025
âœ“ âœ“ âœ“         - 0.027

**3.1. Numerical Instability**

While the residual mapping H _ğ‘™_ [res] is instrumental for performance, its sequential application poses a significant risk to numerical stability. As detailed in Eq. (4), when HC is extended across multiple layers, the effective signal propagation from layer _ğ‘™_ to _ğ¿_ is governed by the composite mapping [ï¿½] _ğ‘–_ _[ğ¿]_ = [âˆ’] 1 _[ğ‘™]_ [H] _ğ¿_ [res] - _ğ‘–_ [. Since the learnable mapping][ H] _ğ‘™_ [res] is unconstrained, this composite mapping inevitably deviates from the identity mapping. Consequently, the signal magnitude is prone to explosion or vanishing during both the forward pass and backpropagation. This phenomenon undermines the fundamental premise of residual learning, which relies on unimpeded signal flow, thereby destabilizing the training process in deeper or larger-scale models.

Empirical evidence supports this analysis. We observe unstable loss behavior in large-scale experiments, as illustrated in Fig. 2. Taking _m_ HC as the baseline, HC exhibits an unexpected loss surge around the 12k step, which is highly correlated with the instability in the gradient norm. Furthermore, the analysis on H _ğ‘™_ [res] validates the mechanism of this instability. To quantify how the composite mapping [ï¿½] _ğ‘–_ _[ğ¿]_ = [âˆ’] 1 _[ğ‘™]_ [H] _ğ¿_ [res] - _ğ‘–_ [amplifies signals along the residual stream, we utilize] two metrics. The first, based on the maximum absolute value of the row sums of the composite mapping, captures the worst-case expansion in the forward pass. The second, based on the maximum absolute column sum, corresponds to the backward pass. We refer to these metrics as the _Amax Gain Magnitude_ of the composite mapping. As shown in Fig. 3 (b), the Amax Gain
Magnitude yields extreme values with peaks of 3000, a stark divergence from 1 that confirms the presence of exploding residual streams.

6

Figure 2 | **Training Instability of Hyper-Connections (HC).** This figure illustrates (a) the absolute
loss gap of HC relative to _m_ HC, and (b) the comparisons of gradient norms. All results are based on 27B models.

|Hr es Forward Signal Gain<br>l<br>Hr es Backward Gradient<br>l|Gain|
|---|---|
|||
|||
|0<br>10<br>20<br><br>Laye|0<br>10<br>20<br><br>Laye|

|5|Col2|Col3|
|---|---|---|
||Y<br>l<br>i = 1H<br>Y<br>~~61 âˆ’l~~<br>i = 1|res<br>l + 1 âˆ’i Forward Signal Gain<br>~~res~~<br>61 i~~ Backward Gradient Gain~~|
||Y<br>l<br>i = 1H<br>Y<br>~~61 âˆ’l~~<br>i = 1|res<br>l + 1 âˆ’i Forward Signal Gain<br>~~res~~<br>61 i~~ Backward Gradient Gain~~|
||<br>|âˆ’|
||||
||||
||||
|0<br>10<br>20<br>30<br>Layer In|0<br>10<br>20<br>30<br>Layer In|40<br>50<br>60<br> dexl|

Figure 3 | **Propagation Instability of Hyper-Connections (HC).** This figure illustrates the
propagation dynamics of (a) the single-layer mapping H _ğ‘™_ [res] and (b) the composite mapping

- _ğ¿_ - _ğ‘™_
_ğ‘–_ =1 [H] _ğ¿_ [res]   - _ğ‘–_ [within the 27B model. The layer index] _[ ğ‘™]_ [(x-axis) unrolls each standard Transformer] block into two independent layers (Attention and FFN). The Amax Gain Magnitude (y-axis) is calculated as the maximum absolute row sum (for the forward signal) and column sum (for the backward gradient), averaged over all tokens in a selected sequence.

**3.2. System Overhead**

While the computational complexity of HC remains manageable due to the linearity of the additional mappings, the system-level overhead prevents a non-negligible challenge. Specifically, memory access (I/O) costs often constitute one of the primary bottlenecks in modern model architectures, which is widely referred to as the â€œmemory wallâ€ (Dao et al., 2022). This bottleneck is frequently overlooked in architectural design, yet it decisively impacts runtime efficiency.

Focusing on the widely adopted pre-norm Transformer (Vaswani et al., 2017) architecture, we analyze the I/O patterns inherent to HC. Tab. 2 summarizes the per token memory access overhead in a single residual layer introduced by the _ğ‘›_ -stream residual design. The analysis reveals that HC increases the memory access cost by a factor approximately proportional to _ğ‘›_ .
This excessive I/O demand significantly degrades training throughput without the mitigation of fused kernels. Besides, since H _ğ‘™_ [pre], H _ğ‘™_ [post], and H _ğ‘™_ [res] involve learnable parameters, their intermediate activations are required for backpropagation. This results in a substantial increase in the
GPU memory footprint, often necessitating gradient checkpointing to maintain feasible memory usage. Furthermore, HC requires _ğ‘›_ -fold more communication cost in pipeline parallelism (Qi et al., 2024), leading to larger bubbles and decreasing the training throughput.

7

Table 2 | **Comparison of Memory Access Costs Per Token.** This analysis accounts for the
overhead introduced by the residual stream maintenance in the forward pass, excluding the internal I/O of the layer function F .

Method Operation Read (Elements) Write (Elements)

Residual
Connection

HyperConnections

#### **4. Method**

Residual Merge 2 _ğ¶_ _ğ¶_

**Total I/O** **2C** **C**

Calculate H _ğ‘™_ [pre], H _ğ‘™_ [post], H _ğ‘™_ [res] _ğ‘›ğ¶_ _ğ‘›_ [2] + 2 _ğ‘›_
H _ğ‘™_ [pre] _ğ‘›ğ¶_ + _ğ‘›_ _ğ¶_
H _ğ‘™_ [post] _ğ¶_ + _ğ‘›_ _ğ‘›ğ¶_
H _ğ‘™_ [res] _ğ‘›ğ¶_ + _ğ‘›_ [2] _ğ‘›ğ¶_
Residual Merge 2 _ğ‘›ğ¶_ _ğ‘›ğ¶_

**Total I/O** ( **5n** + **1** ) **C** + **n** **[2]** + **2n** ( **3n** + **1** ) **C** + **n** **[2]** + **2n**

**4.1. Manifold-Constrained Hyper-Connections**

Drawing inspiration from the identity mapping principle (He et al., 2016b), the core premise of _m_ HC is to constrain the residual mapping H _ğ‘™_ [res] onto a specific manifold. While the original identity mapping ensures stability by enforcing H _ğ‘™_ [res] = **I**, it fundamentally precludes information exchange within the residual stream, which is critical for maximizing the potential of multistream architectures. Therefore, we propose projecting the residual mapping onto a manifold that simultaneously maintains the stability of signal propagation across layers and facilitates mutual interaction among residual streams to preserve the modelâ€™s expressivity. To this end, we restrict H _ğ‘™_ [res] to be a doubly stochastic matrix, which has non-negative entries where both the rows and columns sum to 1. Formally, let M [res] denote the manifold of doubly stochastic matrices (also known as the Birkhoff polytope). We constrain H _ğ‘™_ [res] to PMres (H _ğ‘™_ [res] ), defined as:

PMres (H _ğ‘™_ [res] ) â‰” ï¿½H _ğ‘™_ [res] âˆˆ **R** _[ğ‘›]_ [Ã—] _[ğ‘›]_ | H _ğ‘™_ [res] **1** _ğ‘›_ = **1** _ğ‘›_, **1** [âŠ¤] _ğ‘›_ [H] _ğ‘™_ [res] = **1** [âŠ¤] _ğ‘›_ [,][ H] _ğ‘™_ [res] â©¾ 0ï¿½, (6)

where **1** _ğ‘›_ represents the _ğ‘›_ -dimensional vector of all ones.

It is worth noting that when _ğ‘›_ = 1, the doubly stochastic condition degenerates to the scalar
1, thereby recovering the original identity mapping. The choice of double stochasticity confers several rigorous theoretical properties beneficial for large-scale model training:

1. **Norm Preservation:** The spectral norm of a doubly stochastic matrix is bounded by 1
(i.e., âˆ¥H _ğ‘™_ [res] âˆ¥2 â‰¤ 1). This implies that the learnable mapping is non-expansive, effectively mitigating the gradient explosion problem.
2. **Compositional Closure:** The set of doubly stochastic matrices is closed under matrix
multiplication. This ensures that the composite residual mapping across multiple layers,

- _ğ¿_  - _ğ‘™_
_ğ‘–_ =1 [H] _ğ¿_ [res]          - _ğ‘–_ [, remains doubly stochastic, thereby preserving stability throughout the entire] depth of the model.
3. **Geometric Interpretation via the Birkhoff Polytope:** The set M [res] forms the Birkhoff
polytope, which is the convex hull of the set of permutation matrices. This provides a clear geometric interpretation: the residual mapping acts as a convex combination of permutations. Mathematically, the repeated application of such matrices tends to increase